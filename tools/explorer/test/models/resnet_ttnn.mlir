
#device = #tt.device<workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1)[s0, s1] -> (0, d0 floordiv s0, d1 floordiv s1, (d0 mod s0) * s1 + d1 mod s1), dramMap = (d0, d1)[s0, s1] -> (0, 0, ((((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 8192) mod 12, (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 98304 + (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) mod 8192), meshShape = , chipIds = [0]>
#dram = #tt.memory_space<dram>
#loc = loc("ResNet":0:0)
#system = #tt.memory_space<system>
#system_desc = #tt.system_desc<[{arch = <wormhole_b0>, grid = 8x8, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 1024, erisc_l1_unreserved_base = 1024, dram_unreserved_base = 1024, dram_unreserved_end = 1073741824, physical_cores = {worker = [ 0x0,  0x1,  0x2,  0x3,  0x4,  0x5,  0x6,  0x7,  1x0,  1x1,  1x2,  1x3,  1x4,  1x5,  1x6,  1x7,  2x0,  2x1,  2x2,  2x3,  2x4,  2x5,  2x6,  2x7,  3x0,  3x1,  3x2,  3x3,  3x4,  3x5,  3x6,  3x7,  4x0,  4x1,  4x2,  4x3,  4x4,  4x5,  4x6,  4x7,  5x0,  5x1,  5x2,  5x3,  5x4,  5x5,  5x6,  5x7,  6x0,  6x1,  6x2,  6x3,  6x4,  6x5,  6x6,  6x7,  7x0,  7x1,  7x2,  7x3,  7x4,  7x5,  7x6,  7x7] dram = [ 8x0,  9x0,  10x0,  8x1,  9x1,  10x1,  8x2,  9x2,  10x2,  8x3,  9x3,  10x3]}, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32]}], [0], [3 : i32], [ 0x0x0x0]>
#layout = #tt.layout<(d0, d1, d2, d3) -> (d0 * 672 + d1 * 224 + d2, d3), undef, <1x1>, memref<672x224xf32, #system>>
#layout1 = #tt.layout<(d0, d1, d2) -> (d0 + d1, d2), undef, <1x1>, memref<64x1xf32, #system>>
#layout2 = #tt.layout<(d0, d1, d2) -> (d0 + d1, d2), undef, <1x1>, memref<256x1xf32, #system>>
#layout3 = #tt.layout<(d0, d1, d2) -> (d0 + d1, d2), undef, <1x1>, memref<128x1xf32, #system>>
#layout4 = #tt.layout<(d0, d1, d2) -> (d0 + d1, d2), undef, <1x1>, memref<512x1xf32, #system>>
#layout5 = #tt.layout<(d0, d1, d2) -> (d0 + d1, d2), undef, <1x1>, memref<1024x1xf32, #system>>
#layout6 = #tt.layout<(d0, d1, d2) -> (d0 + d1, d2), undef, <1x1>, memref<2048x1xf32, #system>>
#layout7 = #tt.layout<(d0, d1, d2, d3) -> (d0 * 21 + d1 * 7 + d2, d3), undef, <1x1>, memref<1344x7xf32, #system>>
#layout8 = #tt.layout<(d0, d1, d2, d3) -> (d0 * 64 + d1 + d2, d3), undef, <1x1>, memref<4096x1xf32, #system>>
#layout9 = #tt.layout<(d0, d1, d2, d3) -> (d0 * 192 + d1 * 3 + d2, d3), undef, <1x1>, memref<12288x3xf32, #system>>
#layout10 = #tt.layout<(d0, d1, d2, d3) -> (d0 * 64 + d1 + d2, d3), undef, <1x1>, memref<16384x1xf32, #system>>
#layout11 = #tt.layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 + d2, d3), undef, <1x1>, memref<16384x1xf32, #system>>
#layout12 = #tt.layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 + d2, d3), undef, <1x1>, memref<32768x1xf32, #system>>
#layout13 = #tt.layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 3 + d2, d3), undef, <1x1>, memref<49152x3xf32, #system>>
#layout14 = #tt.layout<(d0, d1, d2, d3) -> (d0 * 128 + d1 + d2, d3), undef, <1x1>, memref<65536x1xf32, #system>>
#layout15 = #tt.layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 + d2, d3), undef, <1x1>, memref<131072x1xf32, #system>>
#layout16 = #tt.layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 + d2, d3), undef, <1x1>, memref<65536x1xf32, #system>>
#layout17 = #tt.layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 + d2, d3), undef, <1x1>, memref<131072x1xf32, #system>>
#layout18 = #tt.layout<(d0, d1, d2, d3) -> (d0 * 768 + d1 * 3 + d2, d3), undef, <1x1>, memref<196608x3xf32, #system>>
#layout19 = #tt.layout<(d0, d1, d2, d3) -> (d0 * 256 + d1 + d2, d3), undef, <1x1>, memref<262144x1xf32, #system>>
#layout20 = #tt.layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 + d2, d3), undef, <1x1>, memref<524288x1xf32, #system>>
#layout21 = #tt.layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 + d2, d3), undef, <1x1>, memref<262144x1xf32, #system>>
#layout22 = #tt.layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 + d2, d3), undef, <1x1>, memref<524288x1xf32, #system>>
#layout23 = #tt.layout<(d0, d1, d2, d3) -> (d0 * 1536 + d1 * 3 + d2, d3), undef, <1x1>, memref<786432x3xf32, #system>>
#layout24 = #tt.layout<(d0, d1, d2, d3) -> (d0 * 512 + d1 + d2, d3), undef, <1x1>, memref<1048576x1xf32, #system>>
#layout25 = #tt.layout<(d0, d1, d2, d3) -> (d0 * 1024 + d1 + d2, d3), undef, <1x1>, memref<2097152x1xf32, #system>>
#layout26 = #tt.layout<(d0, d1, d2, d3) -> (d0 * 2048 + d1 + d2, d3), undef, <1x1>, memref<1048576x1xf32, #system>>
#layout27 = #tt.layout<(d0, d1) -> (d0, d1), undef, <1x1>, memref<2048x1000xf32, #system>>
#layout28 = #tt.layout<(d0) -> (0, d0), undef, <1x1>, memref<1x1000xf32, #system>>
#layout29 = #tt.layout<(d0, d1) -> (d0, d1), undef, <1x1>, memref<1x1000xf32, #system>>
#layout30 = #tt.layout<(d0, d1, d2, d3) -> (d0 * 672 + d1 * 224 + d2, d3), undef, <1x1>, memref<672x224xf32, #dram>, interleaved>
#layout31 = #tt.layout<(d0, d1, d2, d3) -> (d0 * 672 + d1 * 3 + d2, d3), undef, <1x1>, memref<672x224xf32, #dram>, interleaved>
#layout32 = #tt.layout<(d0, d1, d2, d3) -> (d0 * 50176 + d1 * 224 + d2, d3), undef, <1x1>, memref<50176x3xf32, #dram>, interleaved>
#layout33 = #tt.layout<(d0, d1, d2, d3) -> (d0 * 12544 + d1 * 112 + d2, d3), undef, <1x1>, memref<12544x64xf32, #dram>, interleaved>
#layout34 = #tt.layout<(d0, d1, d2, d3) -> (d0 * 7168 + d1 * 64 + d2, d3), undef, <1x1>, memref<7168x112xf32, #dram>, interleaved>
#layout35 = #tt.layout<(d0, d1, d2, d3) -> (d0 * 7168 + d1 * 112 + d2, d3), undef, <1x1>, memref<7168x112xf32, #dram>, interleaved>
#layout36 = #tt.layout<(d0, d1, d2) -> (d0 + d1, d2), undef, <1x1>, memref<64x1xf32, #dram>, interleaved>
#layout37 = #tt.layout<(d0, d1, d2, d3) -> (d0 * 7168 + d1 * 7168 + d2, d3), undef, <1x1>, memref<7168x112xf32, #dram>, interleaved>
#layout38 = #tt.layout<(d0, d1, d2, d3) -> (d0 * 3584 + d1 * 3584 + d2, d3), undef, <1x1>, memref<3584x56xf32, #dram>, interleaved>
#layout39 = #tt.layout<(d0, d1, d2, d3) -> (d0 * 3584 + d1 * 56 + d2, d3), undef, <1x1>, memref<3584x56xf32, #dram>, interleaved>
#layout40 = #tt.layout<(d0, d1, d2, d3) -> (d0 * 3584 + d1 * 64 + d2, d3), undef, <1x1>, memref<3584x56xf32, #dram>, interleaved>
#layout41 = #tt.layout<(d0, d1, d2, d3) -> (d0 * 3136 + d1 * 56 + d2, d3), undef, <1x1>, memref<3136x64xf32, #dram>, interleaved>
#layout42 = #tt.layout<(d0, d1, d2, d3) -> (d0 * 3136 + d1 * 56 + d2, d3), undef, <1x1>, memref<3136x256xf32, #dram>, interleaved>
#layout43 = #tt.layout<(d0, d1, d2, d3) -> (d0 * 14336 + d1 * 256 + d2, d3), undef, <1x1>, memref<14336x56xf32, #dram>, interleaved>
#layout44 = #tt.layout<(d0, d1, d2, d3) -> (d0 * 14336 + d1 * 56 + d2, d3), undef, <1x1>, memref<14336x56xf32, #dram>, interleaved>
#layout45 = #tt.layout<(d0, d1, d2) -> (d0 + d1, d2), undef, <1x1>, memref<256x1xf32, #dram>, interleaved>
#layout46 = #tt.layout<(d0, d1, d2, d3) -> (d0 * 3136 + d1 * 56 + d2, d3), undef, <1x1>, memref<3136x128xf32, #dram>, interleaved>
#layout47 = #tt.layout<(d0, d1, d2, d3) -> (d0 * 7168 + d1 * 128 + d2, d3), undef, <1x1>, memref<7168x56xf32, #dram>, interleaved>
#layout48 = #tt.layout<(d0, d1, d2, d3) -> (d0 * 7168 + d1 * 56 + d2, d3), undef, <1x1>, memref<7168x56xf32, #dram>, interleaved>
#layout49 = #tt.layout<(d0, d1, d2) -> (d0 + d1, d2), undef, <1x1>, memref<128x1xf32, #dram>, interleaved>
#layout50 = #tt.layout<(d0, d1, d2, d3) -> (d0 * 784 + d1 * 28 + d2, d3), undef, <1x1>, memref<784x128xf32, #dram>, interleaved>
#layout51 = #tt.layout<(d0, d1, d2, d3) -> (d0 * 3584 + d1 * 128 + d2, d3), undef, <1x1>, memref<3584x28xf32, #dram>, interleaved>
#layout52 = #tt.layout<(d0, d1, d2, d3) -> (d0 * 3584 + d1 * 28 + d2, d3), undef, <1x1>, memref<3584x28xf32, #dram>, interleaved>
#layout53 = #tt.layout<(d0, d1, d2, d3) -> (d0 * 784 + d1 * 28 + d2, d3), undef, <1x1>, memref<784x512xf32, #dram>, interleaved>
#layout54 = #tt.layout<(d0, d1, d2, d3) -> (d0 * 14336 + d1 * 512 + d2, d3), undef, <1x1>, memref<14336x28xf32, #dram>, interleaved>
#layout55 = #tt.layout<(d0, d1, d2, d3) -> (d0 * 14336 + d1 * 28 + d2, d3), undef, <1x1>, memref<14336x28xf32, #dram>, interleaved>
#layout56 = #tt.layout<(d0, d1, d2) -> (d0 + d1, d2), undef, <1x1>, memref<512x1xf32, #dram>, interleaved>
#layout57 = #tt.layout<(d0, d1, d2, d3) -> (d0 * 784 + d1 * 28 + d2, d3), undef, <1x1>, memref<784x256xf32, #dram>, interleaved>
#layout58 = #tt.layout<(d0, d1, d2, d3) -> (d0 * 7168 + d1 * 256 + d2, d3), undef, <1x1>, memref<7168x28xf32, #dram>, interleaved>
#layout59 = #tt.layout<(d0, d1, d2, d3) -> (d0 * 7168 + d1 * 28 + d2, d3), undef, <1x1>, memref<7168x28xf32, #dram>, interleaved>
#layout60 = #tt.layout<(d0, d1, d2, d3) -> (d0 * 196 + d1 * 14 + d2, d3), undef, <1x1>, memref<196x256xf32, #dram>, interleaved>
#layout61 = #tt.layout<(d0, d1, d2, d3) -> (d0 * 3584 + d1 * 256 + d2, d3), undef, <1x1>, memref<3584x14xf32, #dram>, interleaved>
#layout62 = #tt.layout<(d0, d1, d2, d3) -> (d0 * 3584 + d1 * 14 + d2, d3), undef, <1x1>, memref<3584x14xf32, #dram>, interleaved>
#layout63 = #tt.layout<(d0, d1, d2, d3) -> (d0 * 196 + d1 * 14 + d2, d3), undef, <1x1>, memref<196x1024xf32, #dram>, interleaved>
#layout64 = #tt.layout<(d0, d1, d2, d3) -> (d0 * 14336 + d1 * 1024 + d2, d3), undef, <1x1>, memref<14336x14xf32, #dram>, interleaved>
#layout65 = #tt.layout<(d0, d1, d2, d3) -> (d0 * 14336 + d1 * 14 + d2, d3), undef, <1x1>, memref<14336x14xf32, #dram>, interleaved>
#layout66 = #tt.layout<(d0, d1, d2) -> (d0 + d1, d2), undef, <1x1>, memref<1024x1xf32, #dram>, interleaved>
#layout67 = #tt.layout<(d0, d1, d2, d3) -> (d0 * 196 + d1 * 14 + d2, d3), undef, <1x1>, memref<196x512xf32, #dram>, interleaved>
#layout68 = #tt.layout<(d0, d1, d2, d3) -> (d0 * 7168 + d1 * 512 + d2, d3), undef, <1x1>, memref<7168x14xf32, #dram>, interleaved>
#layout69 = #tt.layout<(d0, d1, d2, d3) -> (d0 * 7168 + d1 * 14 + d2, d3), undef, <1x1>, memref<7168x14xf32, #dram>, interleaved>
#layout70 = #tt.layout<(d0, d1, d2, d3) -> (d0 * 49 + d1 * 7 + d2, d3), undef, <1x1>, memref<49x512xf32, #dram>, interleaved>
#layout71 = #tt.layout<(d0, d1, d2, d3) -> (d0 * 3584 + d1 * 512 + d2, d3), undef, <1x1>, memref<3584x7xf32, #dram>, interleaved>
#layout72 = #tt.layout<(d0, d1, d2, d3) -> (d0 * 3584 + d1 * 7 + d2, d3), undef, <1x1>, memref<3584x7xf32, #dram>, interleaved>
#layout73 = #tt.layout<(d0, d1, d2, d3) -> (d0 * 49 + d1 * 7 + d2, d3), undef, <1x1>, memref<49x2048xf32, #dram>, interleaved>
#layout74 = #tt.layout<(d0, d1, d2, d3) -> (d0 * 14336 + d1 * 2048 + d2, d3), undef, <1x1>, memref<14336x7xf32, #dram>, interleaved>
#layout75 = #tt.layout<(d0, d1, d2, d3) -> (d0 * 14336 + d1 * 7 + d2, d3), undef, <1x1>, memref<14336x7xf32, #dram>, interleaved>
#layout76 = #tt.layout<(d0, d1, d2) -> (d0 + d1, d2), undef, <1x1>, memref<2048x1xf32, #dram>, interleaved>
#layout77 = #tt.layout<(d0, d1, d2, d3) -> (d0 * 2048 + d1 * 2048 + d2, d3), undef, <1x1>, memref<2048x49xf32, #dram>, interleaved>
#layout78 = #tt.layout<(d0, d1, d2, d3) -> (d0 * 49 + d1 * 49 + d2, d3), undef, <1x1>, memref<49x2048xf32, #dram>, interleaved>
#layout79 = #tt.layout<(d0, d1, d2, d3) -> (d0 + d1 + d2, d3), undef, <1x1>, memref<1x2048xf32, #dram>, interleaved>
#layout80 = #tt.layout<(d0, d1, d2, d3) -> (d0 * 2048 + d1 + d2, d3), undef, <1x1>, memref<2048x1xf32, #dram>, interleaved>
#layout81 = #tt.layout<(d0, d1, d2) -> (d0 * 2048 + d1, d2), undef, <1x1>, memref<2048x1xf32, #dram>, interleaved>
#layout82 = #tt.layout<(d0, d1) -> (d0, d1), undef, <1x1>, memref<1x2048xf32, #dram>, interleaved>
#layout83 = #tt.layout<(d0, d1) -> (d0, d1), undef, <1x1>, memref<2048x1000xf32, #dram>, interleaved>
#layout84 = #tt.layout<(d0, d1) -> (d0, d1), undef, <1x1>, memref<1x1000xf32, #dram>, interleaved>
#layout85 = #tt.layout<(d0) -> (0, d0), undef, <1x1>, memref<1x1000xf32, #dram>, interleaved>
module @ResNet attributes {tt.device = #device, tt.system_desc = #system_desc} {
  func.func @forward(%arg0: tensor<1x3x224x224xf32, #layout> {ttir.name = "input_1"} loc("ResNet":0:0), %arg1: tensor<64x1x1xf32, #layout1> {ttir.name = "input_1_add_1"} loc("ResNet":0:0), %arg2: tensor<64x1x1xf32, #layout1> {ttir.name = "input_1_add_1_fork_clone1229"} loc("ResNet":0:0), %arg3: tensor<64x1x1xf32, #layout1> {ttir.name = "input_1_add_18"} loc("ResNet":0:0), %arg4: tensor<64x1x1xf32, #layout1> {ttir.name = "input_1_add_18_fork_clone1271"} loc("ResNet":0:0), %arg5: tensor<64x1x1xf32, #layout1> {ttir.name = "input_1_add_34"} loc("ResNet":0:0), %arg6: tensor<64x1x1xf32, #layout1> {ttir.name = "input_1_add_34_fork_clone1204"} loc("ResNet":0:0), %arg7: tensor<256x1x1xf32, #layout2> {ttir.name = "input_1_add_50"} loc("ResNet":0:0), %arg8: tensor<256x1x1xf32, #layout2> {ttir.name = "input_1_add_50_fork_clone1108"} loc("ResNet":0:0), %arg9: tensor<256x1x1xf32, #layout2> {ttir.name = "input_1_add_65"} loc("ResNet":0:0), %arg10: tensor<256x1x1xf32, #layout2> {ttir.name = "input_1_add_65_fork_clone1112"} loc("ResNet":0:0), %arg11: tensor<64x1x1xf32, #layout1> {ttir.name = "input_1_add_82"} loc("ResNet":0:0), %arg12: tensor<64x1x1xf32, #layout1> {ttir.name = "input_1_add_82_fork_clone1238"} loc("ResNet":0:0), %arg13: tensor<64x1x1xf32, #layout1> {ttir.name = "input_1_add_98"} loc("ResNet":0:0), %arg14: tensor<64x1x1xf32, #layout1> {ttir.name = "input_1_add_98_fork_clone1152"} loc("ResNet":0:0), %arg15: tensor<256x1x1xf32, #layout2> {ttir.name = "input_1_add_114"} loc("ResNet":0:0), %arg16: tensor<256x1x1xf32, #layout2> {ttir.name = "input_1_add_114_fork_clone1051"} loc("ResNet":0:0), %arg17: tensor<64x1x1xf32, #layout1> {ttir.name = "input_1_add_131"} loc("ResNet":0:0), %arg18: tensor<64x1x1xf32, #layout1> {ttir.name = "input_1_add_131_fork_clone1192"} loc("ResNet":0:0), %arg19: tensor<64x1x1xf32, #layout1> {ttir.name = "input_1_add_147"} loc("ResNet":0:0), %arg20: tensor<64x1x1xf32, #layout1> {ttir.name = "input_1_add_147_fork_clone1096"} loc("ResNet":0:0), %arg21: tensor<256x1x1xf32, #layout2> {ttir.name = "input_1_add_163"} loc("ResNet":0:0), %arg22: tensor<256x1x1xf32, #layout2> {ttir.name = "input_1_add_163_fork_clone992"} loc("ResNet":0:0), %arg23: tensor<128x1x1xf32, #layout3> {ttir.name = "input_1_add_180"} loc("ResNet":0:0), %arg24: tensor<128x1x1xf32, #layout3> {ttir.name = "input_1_add_180_fork_clone1065"} loc("ResNet":0:0), %arg25: tensor<128x1x1xf32, #layout3> {ttir.name = "input_1_add_196"} loc("ResNet":0:0), %arg26: tensor<128x1x1xf32, #layout3> {ttir.name = "input_1_add_196_fork_clone962"} loc("ResNet":0:0), %arg27: tensor<512x1x1xf32, #layout4> {ttir.name = "input_1_add_212"} loc("ResNet":0:0), %arg28: tensor<512x1x1xf32, #layout4> {ttir.name = "input_1_add_212_fork_clone853"} loc("ResNet":0:0), %arg29: tensor<512x1x1xf32, #layout4> {ttir.name = "input_1_add_227"} loc("ResNet":0:0), %arg30: tensor<512x1x1xf32, #layout4> {ttir.name = "input_1_add_227_fork_clone857"} loc("ResNet":0:0), %arg31: tensor<128x1x1xf32, #layout3> {ttir.name = "input_1_add_244"} loc("ResNet":0:0), %arg32: tensor<128x1x1xf32, #layout3> {ttir.name = "input_1_add_244_fork_clone1007"} loc("ResNet":0:0), %arg33: tensor<128x1x1xf32, #layout3> {ttir.name = "input_1_add_260"} loc("ResNet":0:0), %arg34: tensor<128x1x1xf32, #layout3> {ttir.name = "input_1_add_260_fork_clone901"} loc("ResNet":0:0), %arg35: tensor<512x1x1xf32, #layout4> {ttir.name = "input_1_add_276"} loc("ResNet":0:0), %arg36: tensor<512x1x1xf32, #layout4> {ttir.name = "input_1_add_276_fork_clone791"} loc("ResNet":0:0), %arg37: tensor<128x1x1xf32, #layout3> {ttir.name = "input_1_add_293"} loc("ResNet":0:0), %arg38: tensor<128x1x1xf32, #layout3> {ttir.name = "input_1_add_293_fork_clone950"} loc("ResNet":0:0), %arg39: tensor<128x1x1xf32, #layout3> {ttir.name = "input_1_add_309"} loc("ResNet":0:0), %arg40: tensor<128x1x1xf32, #layout3> {ttir.name = "input_1_add_309_fork_clone841"} loc("ResNet":0:0), %arg41: tensor<512x1x1xf32, #layout4> {ttir.name = "input_1_add_325"} loc("ResNet":0:0), %arg42: tensor<512x1x1xf32, #layout4> {ttir.name = "input_1_add_325_fork_clone735"} loc("ResNet":0:0), %arg43: tensor<128x1x1xf32, #layout3> {ttir.name = "input_1_add_342"} loc("ResNet":0:0), %arg44: tensor<128x1x1xf32, #layout3> {ttir.name = "input_1_add_342_fork_clone889"} loc("ResNet":0:0), %arg45: tensor<128x1x1xf32, #layout3> {ttir.name = "input_1_add_358"} loc("ResNet":0:0), %arg46: tensor<128x1x1xf32, #layout3> {ttir.name = "input_1_add_358_fork_clone779"} loc("ResNet":0:0), %arg47: tensor<512x1x1xf32, #layout4> {ttir.name = "input_1_add_374"} loc("ResNet":0:0), %arg48: tensor<512x1x1xf32, #layout4> {ttir.name = "input_1_add_374_fork_clone677"} loc("ResNet":0:0), %arg49: tensor<256x1x1xf32, #layout2> {ttir.name = "input_1_add_391"} loc("ResNet":0:0), %arg50: tensor<256x1x1xf32, #layout2> {ttir.name = "input_1_add_391_fork_clone748"} loc("ResNet":0:0), %arg51: tensor<256x1x1xf32, #layout2> {ttir.name = "input_1_add_407"} loc("ResNet":0:0), %arg52: tensor<256x1x1xf32, #layout2> {ttir.name = "input_1_add_407_fork_clone645"} loc("ResNet":0:0), %arg53: tensor<1024x1x1xf32, #layout5> {ttir.name = "input_1_add_423"} loc("ResNet":0:0), %arg54: tensor<1024x1x1xf32, #layout5> {ttir.name = "input_1_add_423_fork_clone524"} loc("ResNet":0:0), %arg55: tensor<1024x1x1xf32, #layout5> {ttir.name = "input_1_add_438"} loc("ResNet":0:0), %arg56: tensor<1024x1x1xf32, #layout5> {ttir.name = "input_1_add_438_fork_clone528"} loc("ResNet":0:0), %arg57: tensor<256x1x1xf32, #layout2> {ttir.name = "input_1_add_455"} loc("ResNet":0:0), %arg58: tensor<256x1x1xf32, #layout2> {ttir.name = "input_1_add_455_fork_clone692"} loc("ResNet":0:0), %arg59: tensor<256x1x1xf32, #layout2> {ttir.name = "input_1_add_471"} loc("ResNet":0:0), %arg60: tensor<256x1x1xf32, #layout2> {ttir.name = "input_1_add_471_fork_clone580"} loc("ResNet":0:0), %arg61: tensor<1024x1x1xf32, #layout5> {ttir.name = "input_1_add_487"} loc("ResNet":0:0), %arg62: tensor<1024x1x1xf32, #layout5> {ttir.name = "input_1_add_487_fork_clone453"} loc("ResNet":0:0), %arg63: tensor<256x1x1xf32, #layout2> {ttir.name = "input_1_add_504"} loc("ResNet":0:0), %arg64: tensor<256x1x1xf32, #layout2> {ttir.name = "input_1_add_504_fork_clone633"} loc("ResNet":0:0), %arg65: tensor<256x1x1xf32, #layout2> {ttir.name = "input_1_add_520"} loc("ResNet":0:0), %arg66: tensor<256x1x1xf32, #layout2> {ttir.name = "input_1_add_520_fork_clone512"} loc("ResNet":0:0), %arg67: tensor<1024x1x1xf32, #layout5> {ttir.name = "input_1_add_536"} loc("ResNet":0:0), %arg68: tensor<1024x1x1xf32, #layout5> {ttir.name = "input_1_add_536_fork_clone389"} loc("ResNet":0:0), %arg69: tensor<256x1x1xf32, #layout2> {ttir.name = "input_1_add_553"} loc("ResNet":0:0), %arg70: tensor<256x1x1xf32, #layout2> {ttir.name = "input_1_add_553_fork_clone568"} loc("ResNet":0:0), %arg71: tensor<256x1x1xf32, #layout2> {ttir.name = "input_1_add_569"} loc("ResNet":0:0), %arg72: tensor<256x1x1xf32, #layout2> {ttir.name = "input_1_add_569_fork_clone441"} loc("ResNet":0:0), %arg73: tensor<1024x1x1xf32, #layout5> {ttir.name = "input_1_add_585"} loc("ResNet":0:0), %arg74: tensor<1024x1x1xf32, #layout5> {ttir.name = "input_1_add_585_fork_clone329"} loc("ResNet":0:0), %arg75: tensor<256x1x1xf32, #layout2> {ttir.name = "input_1_add_602"} loc("ResNet":0:0), %arg76: tensor<256x1x1xf32, #layout2> {ttir.name = "input_1_add_602_fork_clone500"} loc("ResNet":0:0), %arg77: tensor<256x1x1xf32, #layout2> {ttir.name = "input_1_add_618"} loc("ResNet":0:0), %arg78: tensor<256x1x1xf32, #layout2> {ttir.name = "input_1_add_618_fork_clone377"} loc("ResNet":0:0), %arg79: tensor<1024x1x1xf32, #layout5> {ttir.name = "input_1_add_634"} loc("ResNet":0:0), %arg80: tensor<1024x1x1xf32, #layout5> {ttir.name = "input_1_add_634_fork_clone274"} loc("ResNet":0:0), %arg81: tensor<256x1x1xf32, #layout2> {ttir.name = "input_1_add_651"} loc("ResNet":0:0), %arg82: tensor<256x1x1xf32, #layout2> {ttir.name = "input_1_add_651_fork_clone429"} loc("ResNet":0:0), %arg83: tensor<256x1x1xf32, #layout2> {ttir.name = "input_1_add_667"} loc("ResNet":0:0), %arg84: tensor<256x1x1xf32, #layout2> {ttir.name = "input_1_add_667_fork_clone317"} loc("ResNet":0:0), %arg85: tensor<1024x1x1xf32, #layout5> {ttir.name = "input_1_add_683"} loc("ResNet":0:0), %arg86: tensor<1024x1x1xf32, #layout5> {ttir.name = "input_1_add_683_fork_clone219"} loc("ResNet":0:0), %arg87: tensor<512x1x1xf32, #layout4> {ttir.name = "input_1_add_700"} loc("ResNet":0:0), %arg88: tensor<512x1x1xf32, #layout4> {ttir.name = "input_1_add_700_fork_clone287"} loc("ResNet":0:0), %arg89: tensor<512x1x1xf32, #layout4> {ttir.name = "input_1_add_716"} loc("ResNet":0:0), %arg90: tensor<512x1x1xf32, #layout4> {ttir.name = "input_1_add_716_fork_clone190"} loc("ResNet":0:0), %arg91: tensor<2048x1x1xf32, #layout6> {ttir.name = "input_1_add_732"} loc("ResNet":0:0), %arg92: tensor<2048x1x1xf32, #layout6> {ttir.name = "input_1_add_732_fork_clone101"} loc("ResNet":0:0), %arg93: tensor<2048x1x1xf32, #layout6> {ttir.name = "input_1_add_747"} loc("ResNet":0:0), %arg94: tensor<2048x1x1xf32, #layout6> {ttir.name = "input_1_add_747_fork_clone105"} loc("ResNet":0:0), %arg95: tensor<512x1x1xf32, #layout4> {ttir.name = "input_1_add_764"} loc("ResNet":0:0), %arg96: tensor<512x1x1xf32, #layout4> {ttir.name = "input_1_add_764_fork_clone233"} loc("ResNet":0:0), %arg97: tensor<512x1x1xf32, #layout4> {ttir.name = "input_1_add_780"} loc("ResNet":0:0), %arg98: tensor<512x1x1xf32, #layout4> {ttir.name = "input_1_add_780_fork_clone138"} loc("ResNet":0:0), %arg99: tensor<2048x1x1xf32, #layout6> {ttir.name = "input_1_add_796"} loc("ResNet":0:0), %arg100: tensor<2048x1x1xf32, #layout6> {ttir.name = "input_1_add_796_fork_clone61"} loc("ResNet":0:0), %arg101: tensor<512x1x1xf32, #layout4> {ttir.name = "input_1_add_813"} loc("ResNet":0:0), %arg102: tensor<512x1x1xf32, #layout4> {ttir.name = "input_1_add_813_fork_clone178"} loc("ResNet":0:0), %arg103: tensor<512x1x1xf32, #layout4> {ttir.name = "input_1_add_829"} loc("ResNet":0:0), %arg104: tensor<512x1x1xf32, #layout4> {ttir.name = "input_1_add_829_fork_clone89"} loc("ResNet":0:0), %arg105: tensor<2048x1x1xf32, #layout6> {ttir.name = "input_1_add_845"} loc("ResNet":0:0), %arg106: tensor<2048x1x1xf32, #layout6> {ttir.name = "input_1_add_845_fork_clone32"} loc("ResNet":0:0), %arg107: tensor<64x3x7x7xf32, #layout7> {ttir.name = "conv1.weight"} loc("ResNet":0:0), %arg108: tensor<64x64x1x1xf32, #layout8> {ttir.name = "layer1.0.conv1.weight"} loc("ResNet":0:0), %arg109: tensor<64x64x3x3xf32, #layout9> {ttir.name = "layer1.0.conv2.weight"} loc("ResNet":0:0), %arg110: tensor<256x64x1x1xf32, #layout10> {ttir.name = "layer1.0.conv3.weight"} loc("ResNet":0:0), %arg111: tensor<256x64x1x1xf32, #layout10> {ttir.name = "layer1.0.downsample.0.weight"} loc("ResNet":0:0), %arg112: tensor<64x256x1x1xf32, #layout11> {ttir.name = "layer1.1.conv1.weight"} loc("ResNet":0:0), %arg113: tensor<64x64x3x3xf32, #layout9> {ttir.name = "layer1.1.conv2.weight"} loc("ResNet":0:0), %arg114: tensor<256x64x1x1xf32, #layout10> {ttir.name = "layer1.1.conv3.weight"} loc("ResNet":0:0), %arg115: tensor<64x256x1x1xf32, #layout11> {ttir.name = "layer1.2.conv1.weight"} loc("ResNet":0:0), %arg116: tensor<64x64x3x3xf32, #layout9> {ttir.name = "layer1.2.conv2.weight"} loc("ResNet":0:0), %arg117: tensor<256x64x1x1xf32, #layout10> {ttir.name = "layer1.2.conv3.weight"} loc("ResNet":0:0), %arg118: tensor<128x256x1x1xf32, #layout12> {ttir.name = "layer2.0.conv1.weight"} loc("ResNet":0:0), %arg119: tensor<128x128x3x3xf32, #layout13> {ttir.name = "layer2.0.conv2.weight"} loc("ResNet":0:0), %arg120: tensor<512x128x1x1xf32, #layout14> {ttir.name = "layer2.0.conv3.weight"} loc("ResNet":0:0), %arg121: tensor<512x256x1x1xf32, #layout15> {ttir.name = "layer2.0.downsample.0.weight"} loc("ResNet":0:0), %arg122: tensor<128x512x1x1xf32, #layout16> {ttir.name = "layer2.1.conv1.weight"} loc("ResNet":0:0), %arg123: tensor<128x128x3x3xf32, #layout13> {ttir.name = "layer2.1.conv2.weight"} loc("ResNet":0:0), %arg124: tensor<512x128x1x1xf32, #layout14> {ttir.name = "layer2.1.conv3.weight"} loc("ResNet":0:0), %arg125: tensor<128x512x1x1xf32, #layout16> {ttir.name = "layer2.2.conv1.weight"} loc("ResNet":0:0), %arg126: tensor<128x128x3x3xf32, #layout13> {ttir.name = "layer2.2.conv2.weight"} loc("ResNet":0:0), %arg127: tensor<512x128x1x1xf32, #layout14> {ttir.name = "layer2.2.conv3.weight"} loc("ResNet":0:0), %arg128: tensor<128x512x1x1xf32, #layout16> {ttir.name = "layer2.3.conv1.weight"} loc("ResNet":0:0), %arg129: tensor<128x128x3x3xf32, #layout13> {ttir.name = "layer2.3.conv2.weight"} loc("ResNet":0:0), %arg130: tensor<512x128x1x1xf32, #layout14> {ttir.name = "layer2.3.conv3.weight"} loc("ResNet":0:0), %arg131: tensor<256x512x1x1xf32, #layout17> {ttir.name = "layer3.0.conv1.weight"} loc("ResNet":0:0), %arg132: tensor<256x256x3x3xf32, #layout18> {ttir.name = "layer3.0.conv2.weight"} loc("ResNet":0:0), %arg133: tensor<1024x256x1x1xf32, #layout19> {ttir.name = "layer3.0.conv3.weight"} loc("ResNet":0:0), %arg134: tensor<1024x512x1x1xf32, #layout20> {ttir.name = "layer3.0.downsample.0.weight"} loc("ResNet":0:0), %arg135: tensor<256x1024x1x1xf32, #layout21> {ttir.name = "layer3.1.conv1.weight"} loc("ResNet":0:0), %arg136: tensor<256x256x3x3xf32, #layout18> {ttir.name = "layer3.1.conv2.weight"} loc("ResNet":0:0), %arg137: tensor<1024x256x1x1xf32, #layout19> {ttir.name = "layer3.1.conv3.weight"} loc("ResNet":0:0), %arg138: tensor<256x1024x1x1xf32, #layout21> {ttir.name = "layer3.2.conv1.weight"} loc("ResNet":0:0), %arg139: tensor<256x256x3x3xf32, #layout18> {ttir.name = "layer3.2.conv2.weight"} loc("ResNet":0:0), %arg140: tensor<1024x256x1x1xf32, #layout19> {ttir.name = "layer3.2.conv3.weight"} loc("ResNet":0:0), %arg141: tensor<256x1024x1x1xf32, #layout21> {ttir.name = "layer3.3.conv1.weight"} loc("ResNet":0:0), %arg142: tensor<256x256x3x3xf32, #layout18> {ttir.name = "layer3.3.conv2.weight"} loc("ResNet":0:0), %arg143: tensor<1024x256x1x1xf32, #layout19> {ttir.name = "layer3.3.conv3.weight"} loc("ResNet":0:0), %arg144: tensor<256x1024x1x1xf32, #layout21> {ttir.name = "layer3.4.conv1.weight"} loc("ResNet":0:0), %arg145: tensor<256x256x3x3xf32, #layout18> {ttir.name = "layer3.4.conv2.weight"} loc("ResNet":0:0), %arg146: tensor<1024x256x1x1xf32, #layout19> {ttir.name = "layer3.4.conv3.weight"} loc("ResNet":0:0), %arg147: tensor<256x1024x1x1xf32, #layout21> {ttir.name = "layer3.5.conv1.weight"} loc("ResNet":0:0), %arg148: tensor<256x256x3x3xf32, #layout18> {ttir.name = "layer3.5.conv2.weight"} loc("ResNet":0:0), %arg149: tensor<1024x256x1x1xf32, #layout19> {ttir.name = "layer3.5.conv3.weight"} loc("ResNet":0:0), %arg150: tensor<512x1024x1x1xf32, #layout22> {ttir.name = "layer4.0.conv1.weight"} loc("ResNet":0:0), %arg151: tensor<512x512x3x3xf32, #layout23> {ttir.name = "layer4.0.conv2.weight"} loc("ResNet":0:0), %arg152: tensor<2048x512x1x1xf32, #layout24> {ttir.name = "layer4.0.conv3.weight"} loc("ResNet":0:0), %arg153: tensor<2048x1024x1x1xf32, #layout25> {ttir.name = "layer4.0.downsample.0.weight"} loc("ResNet":0:0), %arg154: tensor<512x2048x1x1xf32, #layout26> {ttir.name = "layer4.1.conv1.weight"} loc("ResNet":0:0), %arg155: tensor<512x512x3x3xf32, #layout23> {ttir.name = "layer4.1.conv2.weight"} loc("ResNet":0:0), %arg156: tensor<2048x512x1x1xf32, #layout24> {ttir.name = "layer4.1.conv3.weight"} loc("ResNet":0:0), %arg157: tensor<512x2048x1x1xf32, #layout26> {ttir.name = "layer4.2.conv1.weight"} loc("ResNet":0:0), %arg158: tensor<512x512x3x3xf32, #layout23> {ttir.name = "layer4.2.conv2.weight"} loc("ResNet":0:0), %arg159: tensor<2048x512x1x1xf32, #layout24> {ttir.name = "layer4.2.conv3.weight"} loc("ResNet":0:0), %arg160: tensor<2048x1000xf32, #layout27> {ttir.name = "fc.weight"} loc("ResNet":0:0), %arg161: tensor<1000xf32, #layout28> {ttir.name = "fc.bias"} loc("ResNet":0:0)) -> (tensor<1x1000xf32, #layout29> {ttir.name = "ResNet.output_add_867"}) {
    %0 = "ttnn.get_device"() <{mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !tt.device<#device> loc(#loc447)
    %1 = "ttnn.to_layout"(%arg0, %0) <{layout = #ttnn.layout<tile>}> : (tensor<1x3x224x224xf32, #layout>, !tt.device<#device>) -> tensor<1x3x224x224xf32, #layout30> loc(#loc447)
    %2 = "ttnn.to_device"(%1, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<1x3x224x224xf32, #layout30>, !tt.device<#device>) -> tensor<1x3x224x224xf32, #layout30> loc(#loc447)
    %3 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x224x3x224>}> : (!tt.device<#device>) -> tensor<1x224x3x224xf32, #layout31> loc(#loc447)
    %4 = "ttnn.transpose"(%2, %3) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x3x224x224xf32, #layout30>, tensor<1x224x3x224xf32, #layout31>) -> tensor<1x224x3x224xf32, #layout31> loc(#loc447)
    %5 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x224x224x3>}> : (!tt.device<#device>) -> tensor<1x224x224x3xf32, #layout32> loc(#loc448)
    %6 = "ttnn.transpose"(%4, %5) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x224x3x224xf32, #layout31>, tensor<1x224x224x3xf32, #layout32>) -> tensor<1x224x224x3xf32, #layout32> loc(#loc448)
    %7 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x112x112x64>}> : (!tt.device<#device>) -> tensor<1x112x112x64xf32, #layout33> loc(#loc449)
    %8 = "ttnn.conv2d"(%6, %arg107, %7, %0) <{batch_size = 1 : i32, dilation_height = 1 : i32, dilation_width = 1 : i32, groups = 1 : i32, in_channels = 3 : i32, input_height = 224 : i32, input_width = 224 : i32, kernel_height = 7 : i32, kernel_width = 7 : i32, out_channels = 64 : i32, padding_height = 3 : i32, padding_width = 3 : i32, stride_height = 2 : i32, stride_width = 2 : i32}> : (tensor<1x224x224x3xf32, #layout32>, tensor<64x3x7x7xf32, #layout7>, tensor<1x112x112x64xf32, #layout33>, !tt.device<#device>) -> tensor<1x112x112x64xf32, #layout33> loc(#loc449)
    %9 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x112x64x112>}> : (!tt.device<#device>) -> tensor<1x112x64x112xf32, #layout34> loc(#loc450)
    %10 = "ttnn.transpose"(%8, %9) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x112x112x64xf32, #layout33>, tensor<1x112x64x112xf32, #layout34>) -> tensor<1x112x64x112xf32, #layout34> loc(#loc450)
    %11 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x64x112x112>}> : (!tt.device<#device>) -> tensor<1x64x112x112xf32, #layout35> loc(#loc451)
    %12 = "ttnn.transpose"(%10, %11) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x112x64x112xf32, #layout34>, tensor<1x64x112x112xf32, #layout35>) -> tensor<1x64x112x112xf32, #layout35> loc(#loc451)
    %13 = "ttnn.to_layout"(%arg1, %0) <{layout = #ttnn.layout<tile>}> : (tensor<64x1x1xf32, #layout1>, !tt.device<#device>) -> tensor<64x1x1xf32, #layout36> loc(#loc452)
    %14 = "ttnn.to_device"(%13, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<64x1x1xf32, #layout36>, !tt.device<#device>) -> tensor<64x1x1xf32, #layout36> loc(#loc452)
    %15 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x64x112x112>}> : (!tt.device<#device>) -> tensor<1x64x112x112xf32, #layout35> loc(#loc452)
    %16 = "ttnn.multiply"(%12, %14, %15) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x112x112xf32, #layout35>, tensor<64x1x1xf32, #layout36>, tensor<1x64x112x112xf32, #layout35>) -> tensor<1x64x112x112xf32, #layout35> loc(#loc452)
    %17 = "ttnn.to_layout"(%arg2, %0) <{layout = #ttnn.layout<tile>}> : (tensor<64x1x1xf32, #layout1>, !tt.device<#device>) -> tensor<64x1x1xf32, #layout36> loc(#loc453)
    %18 = "ttnn.to_device"(%17, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<64x1x1xf32, #layout36>, !tt.device<#device>) -> tensor<64x1x1xf32, #layout36> loc(#loc453)
    %19 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x64x112x112>}> : (!tt.device<#device>) -> tensor<1x64x112x112xf32, #layout35> loc(#loc453)
    %20 = "ttnn.add"(%16, %18, %19) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x112x112xf32, #layout35>, tensor<64x1x1xf32, #layout36>, tensor<1x64x112x112xf32, #layout35>) -> tensor<1x64x112x112xf32, #layout35> loc(#loc453)
    %21 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x64x112x112>}> : (!tt.device<#device>) -> tensor<1x64x112x112xf32, #layout35> loc(#loc454)
    %22 = "ttnn.relu"(%20, %21) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x64x112x112xf32, #layout35>, tensor<1x64x112x112xf32, #layout35>) -> tensor<1x64x112x112xf32, #layout35> loc(#loc454)
    %23 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x1x7168x112>}> : (!tt.device<#device>) -> tensor<1x1x7168x112xf32, #layout37> loc(#loc455)
    %24 = "ttnn.reshape"(%22, %23) <{shape = [1 : i32, 1 : i32, 7168 : i32, 112 : i32]}> : (tensor<1x64x112x112xf32, #layout35>, tensor<1x1x7168x112xf32, #layout37>) -> tensor<1x1x7168x112xf32, #layout37> loc(#loc455)
    %25 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x1x3584x56>}> : (!tt.device<#device>) -> tensor<1x1x3584x56xf32, #layout38> loc(#loc455)
    %26 = "ttnn.max_pool2d"(%24, %25, %0) <{batch_size = 1 : si32, ceil_mode = false, channels = 112 : si32, dilation_height = 1 : si32, dilation_width = 1 : si32, input_height = 64 : si32, input_width = 112 : si32, kernel_height = 3 : si32, kernel_width = 3 : si32, padding_height = 1 : si32, padding_width = 1 : si32, stride_height = 2 : si32, stride_width = 2 : si32}> : (tensor<1x1x7168x112xf32, #layout37>, tensor<1x1x3584x56xf32, #layout38>, !tt.device<#device>) -> tensor<1x1x3584x56xf32, #layout38> loc(#loc455)
    %27 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x64x56x56>}> : (!tt.device<#device>) -> tensor<1x64x56x56xf32, #layout39> loc(#loc455)
    %28 = "ttnn.reshape"(%26, %27) <{shape = [1 : i32, 64 : i32, 56 : i32, 56 : i32]}> : (tensor<1x1x3584x56xf32, #layout38>, tensor<1x64x56x56xf32, #layout39>) -> tensor<1x64x56x56xf32, #layout39> loc(#loc455)
    %29 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x56x64x56>}> : (!tt.device<#device>) -> tensor<1x56x64x56xf32, #layout40> loc(#loc456)
    %30 = "ttnn.transpose"(%28, %29) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x64x56x56xf32, #layout39>, tensor<1x56x64x56xf32, #layout40>) -> tensor<1x56x64x56xf32, #layout40> loc(#loc456)
    %31 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x56x56x64>}> : (!tt.device<#device>) -> tensor<1x56x56x64xf32, #layout41> loc(#loc457)
    %32 = "ttnn.transpose"(%30, %31) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x56x64x56xf32, #layout40>, tensor<1x56x56x64xf32, #layout41>) -> tensor<1x56x56x64xf32, #layout41> loc(#loc457)
    %33 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x56x56x64>}> : (!tt.device<#device>) -> tensor<1x56x56x64xf32, #layout41> loc(#loc458)
    %34 = "ttnn.conv2d"(%32, %arg108, %33, %0) <{batch_size = 1 : i32, dilation_height = 1 : i32, dilation_width = 1 : i32, groups = 1 : i32, in_channels = 64 : i32, input_height = 56 : i32, input_width = 56 : i32, kernel_height = 1 : i32, kernel_width = 1 : i32, out_channels = 64 : i32, padding_height = 0 : i32, padding_width = 0 : i32, stride_height = 1 : i32, stride_width = 1 : i32}> : (tensor<1x56x56x64xf32, #layout41>, tensor<64x64x1x1xf32, #layout8>, tensor<1x56x56x64xf32, #layout41>, !tt.device<#device>) -> tensor<1x56x56x64xf32, #layout41> loc(#loc458)
    %35 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x56x64x56>}> : (!tt.device<#device>) -> tensor<1x56x64x56xf32, #layout40> loc(#loc459)
    %36 = "ttnn.transpose"(%34, %35) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x56x56x64xf32, #layout41>, tensor<1x56x64x56xf32, #layout40>) -> tensor<1x56x64x56xf32, #layout40> loc(#loc459)
    %37 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x64x56x56>}> : (!tt.device<#device>) -> tensor<1x64x56x56xf32, #layout39> loc(#loc460)
    %38 = "ttnn.transpose"(%36, %37) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x56x64x56xf32, #layout40>, tensor<1x64x56x56xf32, #layout39>) -> tensor<1x64x56x56xf32, #layout39> loc(#loc460)
    %39 = "ttnn.to_layout"(%arg3, %0) <{layout = #ttnn.layout<tile>}> : (tensor<64x1x1xf32, #layout1>, !tt.device<#device>) -> tensor<64x1x1xf32, #layout36> loc(#loc461)
    %40 = "ttnn.to_device"(%39, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<64x1x1xf32, #layout36>, !tt.device<#device>) -> tensor<64x1x1xf32, #layout36> loc(#loc461)
    %41 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x64x56x56>}> : (!tt.device<#device>) -> tensor<1x64x56x56xf32, #layout39> loc(#loc461)
    %42 = "ttnn.multiply"(%38, %40, %41) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x56x56xf32, #layout39>, tensor<64x1x1xf32, #layout36>, tensor<1x64x56x56xf32, #layout39>) -> tensor<1x64x56x56xf32, #layout39> loc(#loc461)
    %43 = "ttnn.to_layout"(%arg4, %0) <{layout = #ttnn.layout<tile>}> : (tensor<64x1x1xf32, #layout1>, !tt.device<#device>) -> tensor<64x1x1xf32, #layout36> loc(#loc462)
    %44 = "ttnn.to_device"(%43, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<64x1x1xf32, #layout36>, !tt.device<#device>) -> tensor<64x1x1xf32, #layout36> loc(#loc462)
    %45 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x64x56x56>}> : (!tt.device<#device>) -> tensor<1x64x56x56xf32, #layout39> loc(#loc462)
    %46 = "ttnn.add"(%42, %44, %45) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x56x56xf32, #layout39>, tensor<64x1x1xf32, #layout36>, tensor<1x64x56x56xf32, #layout39>) -> tensor<1x64x56x56xf32, #layout39> loc(#loc462)
    %47 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x64x56x56>}> : (!tt.device<#device>) -> tensor<1x64x56x56xf32, #layout39> loc(#loc463)
    %48 = "ttnn.relu"(%46, %47) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x64x56x56xf32, #layout39>, tensor<1x64x56x56xf32, #layout39>) -> tensor<1x64x56x56xf32, #layout39> loc(#loc463)
    %49 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x56x64x56>}> : (!tt.device<#device>) -> tensor<1x56x64x56xf32, #layout40> loc(#loc464)
    %50 = "ttnn.transpose"(%48, %49) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x64x56x56xf32, #layout39>, tensor<1x56x64x56xf32, #layout40>) -> tensor<1x56x64x56xf32, #layout40> loc(#loc464)
    %51 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x56x56x64>}> : (!tt.device<#device>) -> tensor<1x56x56x64xf32, #layout41> loc(#loc465)
    %52 = "ttnn.transpose"(%50, %51) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x56x64x56xf32, #layout40>, tensor<1x56x56x64xf32, #layout41>) -> tensor<1x56x56x64xf32, #layout41> loc(#loc465)
    %53 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x56x56x64>}> : (!tt.device<#device>) -> tensor<1x56x56x64xf32, #layout41> loc(#loc466)
    %54 = "ttnn.conv2d"(%52, %arg109, %53, %0) <{batch_size = 1 : i32, dilation_height = 1 : i32, dilation_width = 1 : i32, groups = 1 : i32, in_channels = 64 : i32, input_height = 56 : i32, input_width = 56 : i32, kernel_height = 3 : i32, kernel_width = 3 : i32, out_channels = 64 : i32, padding_height = 1 : i32, padding_width = 1 : i32, stride_height = 1 : i32, stride_width = 1 : i32}> : (tensor<1x56x56x64xf32, #layout41>, tensor<64x64x3x3xf32, #layout9>, tensor<1x56x56x64xf32, #layout41>, !tt.device<#device>) -> tensor<1x56x56x64xf32, #layout41> loc(#loc466)
    %55 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x56x64x56>}> : (!tt.device<#device>) -> tensor<1x56x64x56xf32, #layout40> loc(#loc467)
    %56 = "ttnn.transpose"(%54, %55) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x56x56x64xf32, #layout41>, tensor<1x56x64x56xf32, #layout40>) -> tensor<1x56x64x56xf32, #layout40> loc(#loc467)
    %57 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x64x56x56>}> : (!tt.device<#device>) -> tensor<1x64x56x56xf32, #layout39> loc(#loc468)
    %58 = "ttnn.transpose"(%56, %57) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x56x64x56xf32, #layout40>, tensor<1x64x56x56xf32, #layout39>) -> tensor<1x64x56x56xf32, #layout39> loc(#loc468)
    %59 = "ttnn.to_layout"(%arg5, %0) <{layout = #ttnn.layout<tile>}> : (tensor<64x1x1xf32, #layout1>, !tt.device<#device>) -> tensor<64x1x1xf32, #layout36> loc(#loc469)
    %60 = "ttnn.to_device"(%59, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<64x1x1xf32, #layout36>, !tt.device<#device>) -> tensor<64x1x1xf32, #layout36> loc(#loc469)
    %61 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x64x56x56>}> : (!tt.device<#device>) -> tensor<1x64x56x56xf32, #layout39> loc(#loc469)
    %62 = "ttnn.multiply"(%58, %60, %61) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x56x56xf32, #layout39>, tensor<64x1x1xf32, #layout36>, tensor<1x64x56x56xf32, #layout39>) -> tensor<1x64x56x56xf32, #layout39> loc(#loc469)
    %63 = "ttnn.to_layout"(%arg6, %0) <{layout = #ttnn.layout<tile>}> : (tensor<64x1x1xf32, #layout1>, !tt.device<#device>) -> tensor<64x1x1xf32, #layout36> loc(#loc470)
    %64 = "ttnn.to_device"(%63, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<64x1x1xf32, #layout36>, !tt.device<#device>) -> tensor<64x1x1xf32, #layout36> loc(#loc470)
    %65 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x64x56x56>}> : (!tt.device<#device>) -> tensor<1x64x56x56xf32, #layout39> loc(#loc470)
    %66 = "ttnn.add"(%62, %64, %65) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x56x56xf32, #layout39>, tensor<64x1x1xf32, #layout36>, tensor<1x64x56x56xf32, #layout39>) -> tensor<1x64x56x56xf32, #layout39> loc(#loc470)
    %67 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x64x56x56>}> : (!tt.device<#device>) -> tensor<1x64x56x56xf32, #layout39> loc(#loc471)
    %68 = "ttnn.relu"(%66, %67) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x64x56x56xf32, #layout39>, tensor<1x64x56x56xf32, #layout39>) -> tensor<1x64x56x56xf32, #layout39> loc(#loc471)
    %69 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x56x64x56>}> : (!tt.device<#device>) -> tensor<1x56x64x56xf32, #layout40> loc(#loc472)
    %70 = "ttnn.transpose"(%68, %69) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x64x56x56xf32, #layout39>, tensor<1x56x64x56xf32, #layout40>) -> tensor<1x56x64x56xf32, #layout40> loc(#loc472)
    %71 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x56x56x64>}> : (!tt.device<#device>) -> tensor<1x56x56x64xf32, #layout41> loc(#loc473)
    %72 = "ttnn.transpose"(%70, %71) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x56x64x56xf32, #layout40>, tensor<1x56x56x64xf32, #layout41>) -> tensor<1x56x56x64xf32, #layout41> loc(#loc473)
    %73 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x56x56x256>}> : (!tt.device<#device>) -> tensor<1x56x56x256xf32, #layout42> loc(#loc474)
    %74 = "ttnn.conv2d"(%72, %arg110, %73, %0) <{batch_size = 1 : i32, dilation_height = 1 : i32, dilation_width = 1 : i32, groups = 1 : i32, in_channels = 64 : i32, input_height = 56 : i32, input_width = 56 : i32, kernel_height = 1 : i32, kernel_width = 1 : i32, out_channels = 256 : i32, padding_height = 0 : i32, padding_width = 0 : i32, stride_height = 1 : i32, stride_width = 1 : i32}> : (tensor<1x56x56x64xf32, #layout41>, tensor<256x64x1x1xf32, #layout10>, tensor<1x56x56x256xf32, #layout42>, !tt.device<#device>) -> tensor<1x56x56x256xf32, #layout42> loc(#loc474)
    %75 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x56x256x56>}> : (!tt.device<#device>) -> tensor<1x56x256x56xf32, #layout43> loc(#loc475)
    %76 = "ttnn.transpose"(%74, %75) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x56x56x256xf32, #layout42>, tensor<1x56x256x56xf32, #layout43>) -> tensor<1x56x256x56xf32, #layout43> loc(#loc475)
    %77 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x256x56x56>}> : (!tt.device<#device>) -> tensor<1x256x56x56xf32, #layout44> loc(#loc476)
    %78 = "ttnn.transpose"(%76, %77) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x56x256x56xf32, #layout43>, tensor<1x256x56x56xf32, #layout44>) -> tensor<1x256x56x56xf32, #layout44> loc(#loc476)
    %79 = "ttnn.to_layout"(%arg7, %0) <{layout = #ttnn.layout<tile>}> : (tensor<256x1x1xf32, #layout2>, !tt.device<#device>) -> tensor<256x1x1xf32, #layout45> loc(#loc477)
    %80 = "ttnn.to_device"(%79, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<256x1x1xf32, #layout45>, !tt.device<#device>) -> tensor<256x1x1xf32, #layout45> loc(#loc477)
    %81 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x256x56x56>}> : (!tt.device<#device>) -> tensor<1x256x56x56xf32, #layout44> loc(#loc477)
    %82 = "ttnn.multiply"(%78, %80, %81) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x256x56x56xf32, #layout44>, tensor<256x1x1xf32, #layout45>, tensor<1x256x56x56xf32, #layout44>) -> tensor<1x256x56x56xf32, #layout44> loc(#loc477)
    %83 = "ttnn.to_layout"(%arg8, %0) <{layout = #ttnn.layout<tile>}> : (tensor<256x1x1xf32, #layout2>, !tt.device<#device>) -> tensor<256x1x1xf32, #layout45> loc(#loc478)
    %84 = "ttnn.to_device"(%83, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<256x1x1xf32, #layout45>, !tt.device<#device>) -> tensor<256x1x1xf32, #layout45> loc(#loc478)
    %85 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x256x56x56>}> : (!tt.device<#device>) -> tensor<1x256x56x56xf32, #layout44> loc(#loc478)
    %86 = "ttnn.add"(%82, %84, %85) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x256x56x56xf32, #layout44>, tensor<256x1x1xf32, #layout45>, tensor<1x256x56x56xf32, #layout44>) -> tensor<1x256x56x56xf32, #layout44> loc(#loc478)
    %87 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x56x64x56>}> : (!tt.device<#device>) -> tensor<1x56x64x56xf32, #layout40> loc(#loc479)
    %88 = "ttnn.transpose"(%28, %87) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x64x56x56xf32, #layout39>, tensor<1x56x64x56xf32, #layout40>) -> tensor<1x56x64x56xf32, #layout40> loc(#loc479)
    %89 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x56x56x64>}> : (!tt.device<#device>) -> tensor<1x56x56x64xf32, #layout41> loc(#loc480)
    %90 = "ttnn.transpose"(%88, %89) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x56x64x56xf32, #layout40>, tensor<1x56x56x64xf32, #layout41>) -> tensor<1x56x56x64xf32, #layout41> loc(#loc480)
    %91 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x56x56x256>}> : (!tt.device<#device>) -> tensor<1x56x56x256xf32, #layout42> loc(#loc481)
    %92 = "ttnn.conv2d"(%90, %arg111, %91, %0) <{batch_size = 1 : i32, dilation_height = 1 : i32, dilation_width = 1 : i32, groups = 1 : i32, in_channels = 64 : i32, input_height = 56 : i32, input_width = 56 : i32, kernel_height = 1 : i32, kernel_width = 1 : i32, out_channels = 256 : i32, padding_height = 0 : i32, padding_width = 0 : i32, stride_height = 1 : i32, stride_width = 1 : i32}> : (tensor<1x56x56x64xf32, #layout41>, tensor<256x64x1x1xf32, #layout10>, tensor<1x56x56x256xf32, #layout42>, !tt.device<#device>) -> tensor<1x56x56x256xf32, #layout42> loc(#loc481)
    %93 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x56x256x56>}> : (!tt.device<#device>) -> tensor<1x56x256x56xf32, #layout43> loc(#loc482)
    %94 = "ttnn.transpose"(%92, %93) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x56x56x256xf32, #layout42>, tensor<1x56x256x56xf32, #layout43>) -> tensor<1x56x256x56xf32, #layout43> loc(#loc482)
    %95 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x256x56x56>}> : (!tt.device<#device>) -> tensor<1x256x56x56xf32, #layout44> loc(#loc483)
    %96 = "ttnn.transpose"(%94, %95) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x56x256x56xf32, #layout43>, tensor<1x256x56x56xf32, #layout44>) -> tensor<1x256x56x56xf32, #layout44> loc(#loc483)
    %97 = "ttnn.to_layout"(%arg9, %0) <{layout = #ttnn.layout<tile>}> : (tensor<256x1x1xf32, #layout2>, !tt.device<#device>) -> tensor<256x1x1xf32, #layout45> loc(#loc484)
    %98 = "ttnn.to_device"(%97, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<256x1x1xf32, #layout45>, !tt.device<#device>) -> tensor<256x1x1xf32, #layout45> loc(#loc484)
    %99 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x256x56x56>}> : (!tt.device<#device>) -> tensor<1x256x56x56xf32, #layout44> loc(#loc484)
    %100 = "ttnn.multiply"(%96, %98, %99) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x256x56x56xf32, #layout44>, tensor<256x1x1xf32, #layout45>, tensor<1x256x56x56xf32, #layout44>) -> tensor<1x256x56x56xf32, #layout44> loc(#loc484)
    %101 = "ttnn.to_layout"(%arg10, %0) <{layout = #ttnn.layout<tile>}> : (tensor<256x1x1xf32, #layout2>, !tt.device<#device>) -> tensor<256x1x1xf32, #layout45> loc(#loc485)
    %102 = "ttnn.to_device"(%101, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<256x1x1xf32, #layout45>, !tt.device<#device>) -> tensor<256x1x1xf32, #layout45> loc(#loc485)
    %103 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x256x56x56>}> : (!tt.device<#device>) -> tensor<1x256x56x56xf32, #layout44> loc(#loc485)
    %104 = "ttnn.add"(%100, %102, %103) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x256x56x56xf32, #layout44>, tensor<256x1x1xf32, #layout45>, tensor<1x256x56x56xf32, #layout44>) -> tensor<1x256x56x56xf32, #layout44> loc(#loc485)
    %105 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x256x56x56>}> : (!tt.device<#device>) -> tensor<1x256x56x56xf32, #layout44> loc(#loc486)
    %106 = "ttnn.add"(%86, %104, %105) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x256x56x56xf32, #layout44>, tensor<1x256x56x56xf32, #layout44>, tensor<1x256x56x56xf32, #layout44>) -> tensor<1x256x56x56xf32, #layout44> loc(#loc486)
    %107 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x256x56x56>}> : (!tt.device<#device>) -> tensor<1x256x56x56xf32, #layout44> loc(#loc487)
    %108 = "ttnn.relu"(%106, %107) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x256x56x56xf32, #layout44>, tensor<1x256x56x56xf32, #layout44>) -> tensor<1x256x56x56xf32, #layout44> loc(#loc487)
    %109 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x56x256x56>}> : (!tt.device<#device>) -> tensor<1x56x256x56xf32, #layout43> loc(#loc488)
    %110 = "ttnn.transpose"(%108, %109) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x256x56x56xf32, #layout44>, tensor<1x56x256x56xf32, #layout43>) -> tensor<1x56x256x56xf32, #layout43> loc(#loc488)
    %111 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x56x56x256>}> : (!tt.device<#device>) -> tensor<1x56x56x256xf32, #layout42> loc(#loc489)
    %112 = "ttnn.transpose"(%110, %111) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x56x256x56xf32, #layout43>, tensor<1x56x56x256xf32, #layout42>) -> tensor<1x56x56x256xf32, #layout42> loc(#loc489)
    %113 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x56x56x64>}> : (!tt.device<#device>) -> tensor<1x56x56x64xf32, #layout41> loc(#loc490)
    %114 = "ttnn.conv2d"(%112, %arg112, %113, %0) <{batch_size = 1 : i32, dilation_height = 1 : i32, dilation_width = 1 : i32, groups = 1 : i32, in_channels = 256 : i32, input_height = 56 : i32, input_width = 56 : i32, kernel_height = 1 : i32, kernel_width = 1 : i32, out_channels = 64 : i32, padding_height = 0 : i32, padding_width = 0 : i32, stride_height = 1 : i32, stride_width = 1 : i32}> : (tensor<1x56x56x256xf32, #layout42>, tensor<64x256x1x1xf32, #layout11>, tensor<1x56x56x64xf32, #layout41>, !tt.device<#device>) -> tensor<1x56x56x64xf32, #layout41> loc(#loc490)
    %115 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x56x64x56>}> : (!tt.device<#device>) -> tensor<1x56x64x56xf32, #layout40> loc(#loc491)
    %116 = "ttnn.transpose"(%114, %115) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x56x56x64xf32, #layout41>, tensor<1x56x64x56xf32, #layout40>) -> tensor<1x56x64x56xf32, #layout40> loc(#loc491)
    %117 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x64x56x56>}> : (!tt.device<#device>) -> tensor<1x64x56x56xf32, #layout39> loc(#loc492)
    %118 = "ttnn.transpose"(%116, %117) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x56x64x56xf32, #layout40>, tensor<1x64x56x56xf32, #layout39>) -> tensor<1x64x56x56xf32, #layout39> loc(#loc492)
    %119 = "ttnn.to_layout"(%arg11, %0) <{layout = #ttnn.layout<tile>}> : (tensor<64x1x1xf32, #layout1>, !tt.device<#device>) -> tensor<64x1x1xf32, #layout36> loc(#loc493)
    %120 = "ttnn.to_device"(%119, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<64x1x1xf32, #layout36>, !tt.device<#device>) -> tensor<64x1x1xf32, #layout36> loc(#loc493)
    %121 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x64x56x56>}> : (!tt.device<#device>) -> tensor<1x64x56x56xf32, #layout39> loc(#loc493)
    %122 = "ttnn.multiply"(%118, %120, %121) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x56x56xf32, #layout39>, tensor<64x1x1xf32, #layout36>, tensor<1x64x56x56xf32, #layout39>) -> tensor<1x64x56x56xf32, #layout39> loc(#loc493)
    %123 = "ttnn.to_layout"(%arg12, %0) <{layout = #ttnn.layout<tile>}> : (tensor<64x1x1xf32, #layout1>, !tt.device<#device>) -> tensor<64x1x1xf32, #layout36> loc(#loc494)
    %124 = "ttnn.to_device"(%123, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<64x1x1xf32, #layout36>, !tt.device<#device>) -> tensor<64x1x1xf32, #layout36> loc(#loc494)
    %125 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x64x56x56>}> : (!tt.device<#device>) -> tensor<1x64x56x56xf32, #layout39> loc(#loc494)
    %126 = "ttnn.add"(%122, %124, %125) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x56x56xf32, #layout39>, tensor<64x1x1xf32, #layout36>, tensor<1x64x56x56xf32, #layout39>) -> tensor<1x64x56x56xf32, #layout39> loc(#loc494)
    %127 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x64x56x56>}> : (!tt.device<#device>) -> tensor<1x64x56x56xf32, #layout39> loc(#loc495)
    %128 = "ttnn.relu"(%126, %127) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x64x56x56xf32, #layout39>, tensor<1x64x56x56xf32, #layout39>) -> tensor<1x64x56x56xf32, #layout39> loc(#loc495)
    %129 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x56x64x56>}> : (!tt.device<#device>) -> tensor<1x56x64x56xf32, #layout40> loc(#loc496)
    %130 = "ttnn.transpose"(%128, %129) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x64x56x56xf32, #layout39>, tensor<1x56x64x56xf32, #layout40>) -> tensor<1x56x64x56xf32, #layout40> loc(#loc496)
    %131 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x56x56x64>}> : (!tt.device<#device>) -> tensor<1x56x56x64xf32, #layout41> loc(#loc497)
    %132 = "ttnn.transpose"(%130, %131) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x56x64x56xf32, #layout40>, tensor<1x56x56x64xf32, #layout41>) -> tensor<1x56x56x64xf32, #layout41> loc(#loc497)
    %133 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x56x56x64>}> : (!tt.device<#device>) -> tensor<1x56x56x64xf32, #layout41> loc(#loc498)
    %134 = "ttnn.conv2d"(%132, %arg113, %133, %0) <{batch_size = 1 : i32, dilation_height = 1 : i32, dilation_width = 1 : i32, groups = 1 : i32, in_channels = 64 : i32, input_height = 56 : i32, input_width = 56 : i32, kernel_height = 3 : i32, kernel_width = 3 : i32, out_channels = 64 : i32, padding_height = 1 : i32, padding_width = 1 : i32, stride_height = 1 : i32, stride_width = 1 : i32}> : (tensor<1x56x56x64xf32, #layout41>, tensor<64x64x3x3xf32, #layout9>, tensor<1x56x56x64xf32, #layout41>, !tt.device<#device>) -> tensor<1x56x56x64xf32, #layout41> loc(#loc498)
    %135 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x56x64x56>}> : (!tt.device<#device>) -> tensor<1x56x64x56xf32, #layout40> loc(#loc499)
    %136 = "ttnn.transpose"(%134, %135) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x56x56x64xf32, #layout41>, tensor<1x56x64x56xf32, #layout40>) -> tensor<1x56x64x56xf32, #layout40> loc(#loc499)
    %137 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x64x56x56>}> : (!tt.device<#device>) -> tensor<1x64x56x56xf32, #layout39> loc(#loc500)
    %138 = "ttnn.transpose"(%136, %137) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x56x64x56xf32, #layout40>, tensor<1x64x56x56xf32, #layout39>) -> tensor<1x64x56x56xf32, #layout39> loc(#loc500)
    %139 = "ttnn.to_layout"(%arg13, %0) <{layout = #ttnn.layout<tile>}> : (tensor<64x1x1xf32, #layout1>, !tt.device<#device>) -> tensor<64x1x1xf32, #layout36> loc(#loc501)
    %140 = "ttnn.to_device"(%139, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<64x1x1xf32, #layout36>, !tt.device<#device>) -> tensor<64x1x1xf32, #layout36> loc(#loc501)
    %141 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x64x56x56>}> : (!tt.device<#device>) -> tensor<1x64x56x56xf32, #layout39> loc(#loc501)
    %142 = "ttnn.multiply"(%138, %140, %141) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x56x56xf32, #layout39>, tensor<64x1x1xf32, #layout36>, tensor<1x64x56x56xf32, #layout39>) -> tensor<1x64x56x56xf32, #layout39> loc(#loc501)
    %143 = "ttnn.to_layout"(%arg14, %0) <{layout = #ttnn.layout<tile>}> : (tensor<64x1x1xf32, #layout1>, !tt.device<#device>) -> tensor<64x1x1xf32, #layout36> loc(#loc502)
    %144 = "ttnn.to_device"(%143, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<64x1x1xf32, #layout36>, !tt.device<#device>) -> tensor<64x1x1xf32, #layout36> loc(#loc502)
    %145 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x64x56x56>}> : (!tt.device<#device>) -> tensor<1x64x56x56xf32, #layout39> loc(#loc502)
    %146 = "ttnn.add"(%142, %144, %145) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x56x56xf32, #layout39>, tensor<64x1x1xf32, #layout36>, tensor<1x64x56x56xf32, #layout39>) -> tensor<1x64x56x56xf32, #layout39> loc(#loc502)
    %147 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x64x56x56>}> : (!tt.device<#device>) -> tensor<1x64x56x56xf32, #layout39> loc(#loc503)
    %148 = "ttnn.relu"(%146, %147) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x64x56x56xf32, #layout39>, tensor<1x64x56x56xf32, #layout39>) -> tensor<1x64x56x56xf32, #layout39> loc(#loc503)
    %149 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x56x64x56>}> : (!tt.device<#device>) -> tensor<1x56x64x56xf32, #layout40> loc(#loc504)
    %150 = "ttnn.transpose"(%148, %149) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x64x56x56xf32, #layout39>, tensor<1x56x64x56xf32, #layout40>) -> tensor<1x56x64x56xf32, #layout40> loc(#loc504)
    %151 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x56x56x64>}> : (!tt.device<#device>) -> tensor<1x56x56x64xf32, #layout41> loc(#loc505)
    %152 = "ttnn.transpose"(%150, %151) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x56x64x56xf32, #layout40>, tensor<1x56x56x64xf32, #layout41>) -> tensor<1x56x56x64xf32, #layout41> loc(#loc505)
    %153 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x56x56x256>}> : (!tt.device<#device>) -> tensor<1x56x56x256xf32, #layout42> loc(#loc506)
    %154 = "ttnn.conv2d"(%152, %arg114, %153, %0) <{batch_size = 1 : i32, dilation_height = 1 : i32, dilation_width = 1 : i32, groups = 1 : i32, in_channels = 64 : i32, input_height = 56 : i32, input_width = 56 : i32, kernel_height = 1 : i32, kernel_width = 1 : i32, out_channels = 256 : i32, padding_height = 0 : i32, padding_width = 0 : i32, stride_height = 1 : i32, stride_width = 1 : i32}> : (tensor<1x56x56x64xf32, #layout41>, tensor<256x64x1x1xf32, #layout10>, tensor<1x56x56x256xf32, #layout42>, !tt.device<#device>) -> tensor<1x56x56x256xf32, #layout42> loc(#loc506)
    %155 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x56x256x56>}> : (!tt.device<#device>) -> tensor<1x56x256x56xf32, #layout43> loc(#loc507)
    %156 = "ttnn.transpose"(%154, %155) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x56x56x256xf32, #layout42>, tensor<1x56x256x56xf32, #layout43>) -> tensor<1x56x256x56xf32, #layout43> loc(#loc507)
    %157 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x256x56x56>}> : (!tt.device<#device>) -> tensor<1x256x56x56xf32, #layout44> loc(#loc508)
    %158 = "ttnn.transpose"(%156, %157) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x56x256x56xf32, #layout43>, tensor<1x256x56x56xf32, #layout44>) -> tensor<1x256x56x56xf32, #layout44> loc(#loc508)
    %159 = "ttnn.to_layout"(%arg15, %0) <{layout = #ttnn.layout<tile>}> : (tensor<256x1x1xf32, #layout2>, !tt.device<#device>) -> tensor<256x1x1xf32, #layout45> loc(#loc509)
    %160 = "ttnn.to_device"(%159, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<256x1x1xf32, #layout45>, !tt.device<#device>) -> tensor<256x1x1xf32, #layout45> loc(#loc509)
    %161 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x256x56x56>}> : (!tt.device<#device>) -> tensor<1x256x56x56xf32, #layout44> loc(#loc509)
    %162 = "ttnn.multiply"(%158, %160, %161) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x256x56x56xf32, #layout44>, tensor<256x1x1xf32, #layout45>, tensor<1x256x56x56xf32, #layout44>) -> tensor<1x256x56x56xf32, #layout44> loc(#loc509)
    %163 = "ttnn.to_layout"(%arg16, %0) <{layout = #ttnn.layout<tile>}> : (tensor<256x1x1xf32, #layout2>, !tt.device<#device>) -> tensor<256x1x1xf32, #layout45> loc(#loc510)
    %164 = "ttnn.to_device"(%163, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<256x1x1xf32, #layout45>, !tt.device<#device>) -> tensor<256x1x1xf32, #layout45> loc(#loc510)
    %165 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x256x56x56>}> : (!tt.device<#device>) -> tensor<1x256x56x56xf32, #layout44> loc(#loc510)
    %166 = "ttnn.add"(%162, %164, %165) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x256x56x56xf32, #layout44>, tensor<256x1x1xf32, #layout45>, tensor<1x256x56x56xf32, #layout44>) -> tensor<1x256x56x56xf32, #layout44> loc(#loc510)
    %167 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x256x56x56>}> : (!tt.device<#device>) -> tensor<1x256x56x56xf32, #layout44> loc(#loc511)
    %168 = "ttnn.add"(%166, %108, %167) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x256x56x56xf32, #layout44>, tensor<1x256x56x56xf32, #layout44>, tensor<1x256x56x56xf32, #layout44>) -> tensor<1x256x56x56xf32, #layout44> loc(#loc511)
    %169 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x256x56x56>}> : (!tt.device<#device>) -> tensor<1x256x56x56xf32, #layout44> loc(#loc512)
    %170 = "ttnn.relu"(%168, %169) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x256x56x56xf32, #layout44>, tensor<1x256x56x56xf32, #layout44>) -> tensor<1x256x56x56xf32, #layout44> loc(#loc512)
    %171 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x56x256x56>}> : (!tt.device<#device>) -> tensor<1x56x256x56xf32, #layout43> loc(#loc513)
    %172 = "ttnn.transpose"(%170, %171) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x256x56x56xf32, #layout44>, tensor<1x56x256x56xf32, #layout43>) -> tensor<1x56x256x56xf32, #layout43> loc(#loc513)
    %173 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x56x56x256>}> : (!tt.device<#device>) -> tensor<1x56x56x256xf32, #layout42> loc(#loc514)
    %174 = "ttnn.transpose"(%172, %173) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x56x256x56xf32, #layout43>, tensor<1x56x56x256xf32, #layout42>) -> tensor<1x56x56x256xf32, #layout42> loc(#loc514)
    %175 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x56x56x64>}> : (!tt.device<#device>) -> tensor<1x56x56x64xf32, #layout41> loc(#loc515)
    %176 = "ttnn.conv2d"(%174, %arg115, %175, %0) <{batch_size = 1 : i32, dilation_height = 1 : i32, dilation_width = 1 : i32, groups = 1 : i32, in_channels = 256 : i32, input_height = 56 : i32, input_width = 56 : i32, kernel_height = 1 : i32, kernel_width = 1 : i32, out_channels = 64 : i32, padding_height = 0 : i32, padding_width = 0 : i32, stride_height = 1 : i32, stride_width = 1 : i32}> : (tensor<1x56x56x256xf32, #layout42>, tensor<64x256x1x1xf32, #layout11>, tensor<1x56x56x64xf32, #layout41>, !tt.device<#device>) -> tensor<1x56x56x64xf32, #layout41> loc(#loc515)
    %177 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x56x64x56>}> : (!tt.device<#device>) -> tensor<1x56x64x56xf32, #layout40> loc(#loc516)
    %178 = "ttnn.transpose"(%176, %177) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x56x56x64xf32, #layout41>, tensor<1x56x64x56xf32, #layout40>) -> tensor<1x56x64x56xf32, #layout40> loc(#loc516)
    %179 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x64x56x56>}> : (!tt.device<#device>) -> tensor<1x64x56x56xf32, #layout39> loc(#loc517)
    %180 = "ttnn.transpose"(%178, %179) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x56x64x56xf32, #layout40>, tensor<1x64x56x56xf32, #layout39>) -> tensor<1x64x56x56xf32, #layout39> loc(#loc517)
    %181 = "ttnn.to_layout"(%arg17, %0) <{layout = #ttnn.layout<tile>}> : (tensor<64x1x1xf32, #layout1>, !tt.device<#device>) -> tensor<64x1x1xf32, #layout36> loc(#loc518)
    %182 = "ttnn.to_device"(%181, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<64x1x1xf32, #layout36>, !tt.device<#device>) -> tensor<64x1x1xf32, #layout36> loc(#loc518)
    %183 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x64x56x56>}> : (!tt.device<#device>) -> tensor<1x64x56x56xf32, #layout39> loc(#loc518)
    %184 = "ttnn.multiply"(%180, %182, %183) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x56x56xf32, #layout39>, tensor<64x1x1xf32, #layout36>, tensor<1x64x56x56xf32, #layout39>) -> tensor<1x64x56x56xf32, #layout39> loc(#loc518)
    %185 = "ttnn.to_layout"(%arg18, %0) <{layout = #ttnn.layout<tile>}> : (tensor<64x1x1xf32, #layout1>, !tt.device<#device>) -> tensor<64x1x1xf32, #layout36> loc(#loc519)
    %186 = "ttnn.to_device"(%185, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<64x1x1xf32, #layout36>, !tt.device<#device>) -> tensor<64x1x1xf32, #layout36> loc(#loc519)
    %187 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x64x56x56>}> : (!tt.device<#device>) -> tensor<1x64x56x56xf32, #layout39> loc(#loc519)
    %188 = "ttnn.add"(%184, %186, %187) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x56x56xf32, #layout39>, tensor<64x1x1xf32, #layout36>, tensor<1x64x56x56xf32, #layout39>) -> tensor<1x64x56x56xf32, #layout39> loc(#loc519)
    %189 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x64x56x56>}> : (!tt.device<#device>) -> tensor<1x64x56x56xf32, #layout39> loc(#loc520)
    %190 = "ttnn.relu"(%188, %189) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x64x56x56xf32, #layout39>, tensor<1x64x56x56xf32, #layout39>) -> tensor<1x64x56x56xf32, #layout39> loc(#loc520)
    %191 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x56x64x56>}> : (!tt.device<#device>) -> tensor<1x56x64x56xf32, #layout40> loc(#loc521)
    %192 = "ttnn.transpose"(%190, %191) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x64x56x56xf32, #layout39>, tensor<1x56x64x56xf32, #layout40>) -> tensor<1x56x64x56xf32, #layout40> loc(#loc521)
    %193 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x56x56x64>}> : (!tt.device<#device>) -> tensor<1x56x56x64xf32, #layout41> loc(#loc522)
    %194 = "ttnn.transpose"(%192, %193) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x56x64x56xf32, #layout40>, tensor<1x56x56x64xf32, #layout41>) -> tensor<1x56x56x64xf32, #layout41> loc(#loc522)
    %195 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x56x56x64>}> : (!tt.device<#device>) -> tensor<1x56x56x64xf32, #layout41> loc(#loc523)
    %196 = "ttnn.conv2d"(%194, %arg116, %195, %0) <{batch_size = 1 : i32, dilation_height = 1 : i32, dilation_width = 1 : i32, groups = 1 : i32, in_channels = 64 : i32, input_height = 56 : i32, input_width = 56 : i32, kernel_height = 3 : i32, kernel_width = 3 : i32, out_channels = 64 : i32, padding_height = 1 : i32, padding_width = 1 : i32, stride_height = 1 : i32, stride_width = 1 : i32}> : (tensor<1x56x56x64xf32, #layout41>, tensor<64x64x3x3xf32, #layout9>, tensor<1x56x56x64xf32, #layout41>, !tt.device<#device>) -> tensor<1x56x56x64xf32, #layout41> loc(#loc523)
    %197 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x56x64x56>}> : (!tt.device<#device>) -> tensor<1x56x64x56xf32, #layout40> loc(#loc524)
    %198 = "ttnn.transpose"(%196, %197) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x56x56x64xf32, #layout41>, tensor<1x56x64x56xf32, #layout40>) -> tensor<1x56x64x56xf32, #layout40> loc(#loc524)
    %199 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x64x56x56>}> : (!tt.device<#device>) -> tensor<1x64x56x56xf32, #layout39> loc(#loc525)
    %200 = "ttnn.transpose"(%198, %199) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x56x64x56xf32, #layout40>, tensor<1x64x56x56xf32, #layout39>) -> tensor<1x64x56x56xf32, #layout39> loc(#loc525)
    %201 = "ttnn.to_layout"(%arg19, %0) <{layout = #ttnn.layout<tile>}> : (tensor<64x1x1xf32, #layout1>, !tt.device<#device>) -> tensor<64x1x1xf32, #layout36> loc(#loc526)
    %202 = "ttnn.to_device"(%201, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<64x1x1xf32, #layout36>, !tt.device<#device>) -> tensor<64x1x1xf32, #layout36> loc(#loc526)
    %203 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x64x56x56>}> : (!tt.device<#device>) -> tensor<1x64x56x56xf32, #layout39> loc(#loc526)
    %204 = "ttnn.multiply"(%200, %202, %203) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x56x56xf32, #layout39>, tensor<64x1x1xf32, #layout36>, tensor<1x64x56x56xf32, #layout39>) -> tensor<1x64x56x56xf32, #layout39> loc(#loc526)
    %205 = "ttnn.to_layout"(%arg20, %0) <{layout = #ttnn.layout<tile>}> : (tensor<64x1x1xf32, #layout1>, !tt.device<#device>) -> tensor<64x1x1xf32, #layout36> loc(#loc527)
    %206 = "ttnn.to_device"(%205, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<64x1x1xf32, #layout36>, !tt.device<#device>) -> tensor<64x1x1xf32, #layout36> loc(#loc527)
    %207 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x64x56x56>}> : (!tt.device<#device>) -> tensor<1x64x56x56xf32, #layout39> loc(#loc527)
    %208 = "ttnn.add"(%204, %206, %207) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x64x56x56xf32, #layout39>, tensor<64x1x1xf32, #layout36>, tensor<1x64x56x56xf32, #layout39>) -> tensor<1x64x56x56xf32, #layout39> loc(#loc527)
    %209 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x64x56x56>}> : (!tt.device<#device>) -> tensor<1x64x56x56xf32, #layout39> loc(#loc528)
    %210 = "ttnn.relu"(%208, %209) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x64x56x56xf32, #layout39>, tensor<1x64x56x56xf32, #layout39>) -> tensor<1x64x56x56xf32, #layout39> loc(#loc528)
    %211 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x56x64x56>}> : (!tt.device<#device>) -> tensor<1x56x64x56xf32, #layout40> loc(#loc529)
    %212 = "ttnn.transpose"(%210, %211) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x64x56x56xf32, #layout39>, tensor<1x56x64x56xf32, #layout40>) -> tensor<1x56x64x56xf32, #layout40> loc(#loc529)
    %213 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x56x56x64>}> : (!tt.device<#device>) -> tensor<1x56x56x64xf32, #layout41> loc(#loc530)
    %214 = "ttnn.transpose"(%212, %213) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x56x64x56xf32, #layout40>, tensor<1x56x56x64xf32, #layout41>) -> tensor<1x56x56x64xf32, #layout41> loc(#loc530)
    %215 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x56x56x256>}> : (!tt.device<#device>) -> tensor<1x56x56x256xf32, #layout42> loc(#loc531)
    %216 = "ttnn.conv2d"(%214, %arg117, %215, %0) <{batch_size = 1 : i32, dilation_height = 1 : i32, dilation_width = 1 : i32, groups = 1 : i32, in_channels = 64 : i32, input_height = 56 : i32, input_width = 56 : i32, kernel_height = 1 : i32, kernel_width = 1 : i32, out_channels = 256 : i32, padding_height = 0 : i32, padding_width = 0 : i32, stride_height = 1 : i32, stride_width = 1 : i32}> : (tensor<1x56x56x64xf32, #layout41>, tensor<256x64x1x1xf32, #layout10>, tensor<1x56x56x256xf32, #layout42>, !tt.device<#device>) -> tensor<1x56x56x256xf32, #layout42> loc(#loc531)
    %217 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x56x256x56>}> : (!tt.device<#device>) -> tensor<1x56x256x56xf32, #layout43> loc(#loc532)
    %218 = "ttnn.transpose"(%216, %217) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x56x56x256xf32, #layout42>, tensor<1x56x256x56xf32, #layout43>) -> tensor<1x56x256x56xf32, #layout43> loc(#loc532)
    %219 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x256x56x56>}> : (!tt.device<#device>) -> tensor<1x256x56x56xf32, #layout44> loc(#loc533)
    %220 = "ttnn.transpose"(%218, %219) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x56x256x56xf32, #layout43>, tensor<1x256x56x56xf32, #layout44>) -> tensor<1x256x56x56xf32, #layout44> loc(#loc533)
    %221 = "ttnn.to_layout"(%arg21, %0) <{layout = #ttnn.layout<tile>}> : (tensor<256x1x1xf32, #layout2>, !tt.device<#device>) -> tensor<256x1x1xf32, #layout45> loc(#loc534)
    %222 = "ttnn.to_device"(%221, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<256x1x1xf32, #layout45>, !tt.device<#device>) -> tensor<256x1x1xf32, #layout45> loc(#loc534)
    %223 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x256x56x56>}> : (!tt.device<#device>) -> tensor<1x256x56x56xf32, #layout44> loc(#loc534)
    %224 = "ttnn.multiply"(%220, %222, %223) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x256x56x56xf32, #layout44>, tensor<256x1x1xf32, #layout45>, tensor<1x256x56x56xf32, #layout44>) -> tensor<1x256x56x56xf32, #layout44> loc(#loc534)
    %225 = "ttnn.to_layout"(%arg22, %0) <{layout = #ttnn.layout<tile>}> : (tensor<256x1x1xf32, #layout2>, !tt.device<#device>) -> tensor<256x1x1xf32, #layout45> loc(#loc535)
    %226 = "ttnn.to_device"(%225, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<256x1x1xf32, #layout45>, !tt.device<#device>) -> tensor<256x1x1xf32, #layout45> loc(#loc535)
    %227 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x256x56x56>}> : (!tt.device<#device>) -> tensor<1x256x56x56xf32, #layout44> loc(#loc535)
    %228 = "ttnn.add"(%224, %226, %227) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x256x56x56xf32, #layout44>, tensor<256x1x1xf32, #layout45>, tensor<1x256x56x56xf32, #layout44>) -> tensor<1x256x56x56xf32, #layout44> loc(#loc535)
    %229 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x256x56x56>}> : (!tt.device<#device>) -> tensor<1x256x56x56xf32, #layout44> loc(#loc536)
    %230 = "ttnn.add"(%228, %170, %229) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x256x56x56xf32, #layout44>, tensor<1x256x56x56xf32, #layout44>, tensor<1x256x56x56xf32, #layout44>) -> tensor<1x256x56x56xf32, #layout44> loc(#loc536)
    %231 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x256x56x56>}> : (!tt.device<#device>) -> tensor<1x256x56x56xf32, #layout44> loc(#loc537)
    %232 = "ttnn.relu"(%230, %231) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x256x56x56xf32, #layout44>, tensor<1x256x56x56xf32, #layout44>) -> tensor<1x256x56x56xf32, #layout44> loc(#loc537)
    %233 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x56x256x56>}> : (!tt.device<#device>) -> tensor<1x56x256x56xf32, #layout43> loc(#loc538)
    %234 = "ttnn.transpose"(%232, %233) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x256x56x56xf32, #layout44>, tensor<1x56x256x56xf32, #layout43>) -> tensor<1x56x256x56xf32, #layout43> loc(#loc538)
    %235 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x56x56x256>}> : (!tt.device<#device>) -> tensor<1x56x56x256xf32, #layout42> loc(#loc539)
    %236 = "ttnn.transpose"(%234, %235) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x56x256x56xf32, #layout43>, tensor<1x56x56x256xf32, #layout42>) -> tensor<1x56x56x256xf32, #layout42> loc(#loc539)
    %237 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x56x56x128>}> : (!tt.device<#device>) -> tensor<1x56x56x128xf32, #layout46> loc(#loc540)
    %238 = "ttnn.conv2d"(%236, %arg118, %237, %0) <{batch_size = 1 : i32, dilation_height = 1 : i32, dilation_width = 1 : i32, groups = 1 : i32, in_channels = 256 : i32, input_height = 56 : i32, input_width = 56 : i32, kernel_height = 1 : i32, kernel_width = 1 : i32, out_channels = 128 : i32, padding_height = 0 : i32, padding_width = 0 : i32, stride_height = 1 : i32, stride_width = 1 : i32}> : (tensor<1x56x56x256xf32, #layout42>, tensor<128x256x1x1xf32, #layout12>, tensor<1x56x56x128xf32, #layout46>, !tt.device<#device>) -> tensor<1x56x56x128xf32, #layout46> loc(#loc540)
    %239 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x56x128x56>}> : (!tt.device<#device>) -> tensor<1x56x128x56xf32, #layout47> loc(#loc541)
    %240 = "ttnn.transpose"(%238, %239) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x56x56x128xf32, #layout46>, tensor<1x56x128x56xf32, #layout47>) -> tensor<1x56x128x56xf32, #layout47> loc(#loc541)
    %241 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x128x56x56>}> : (!tt.device<#device>) -> tensor<1x128x56x56xf32, #layout48> loc(#loc542)
    %242 = "ttnn.transpose"(%240, %241) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x56x128x56xf32, #layout47>, tensor<1x128x56x56xf32, #layout48>) -> tensor<1x128x56x56xf32, #layout48> loc(#loc542)
    %243 = "ttnn.to_layout"(%arg23, %0) <{layout = #ttnn.layout<tile>}> : (tensor<128x1x1xf32, #layout3>, !tt.device<#device>) -> tensor<128x1x1xf32, #layout49> loc(#loc543)
    %244 = "ttnn.to_device"(%243, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<128x1x1xf32, #layout49>, !tt.device<#device>) -> tensor<128x1x1xf32, #layout49> loc(#loc543)
    %245 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x128x56x56>}> : (!tt.device<#device>) -> tensor<1x128x56x56xf32, #layout48> loc(#loc543)
    %246 = "ttnn.multiply"(%242, %244, %245) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128x56x56xf32, #layout48>, tensor<128x1x1xf32, #layout49>, tensor<1x128x56x56xf32, #layout48>) -> tensor<1x128x56x56xf32, #layout48> loc(#loc543)
    %247 = "ttnn.to_layout"(%arg24, %0) <{layout = #ttnn.layout<tile>}> : (tensor<128x1x1xf32, #layout3>, !tt.device<#device>) -> tensor<128x1x1xf32, #layout49> loc(#loc544)
    %248 = "ttnn.to_device"(%247, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<128x1x1xf32, #layout49>, !tt.device<#device>) -> tensor<128x1x1xf32, #layout49> loc(#loc544)
    %249 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x128x56x56>}> : (!tt.device<#device>) -> tensor<1x128x56x56xf32, #layout48> loc(#loc544)
    %250 = "ttnn.add"(%246, %248, %249) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128x56x56xf32, #layout48>, tensor<128x1x1xf32, #layout49>, tensor<1x128x56x56xf32, #layout48>) -> tensor<1x128x56x56xf32, #layout48> loc(#loc544)
    %251 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x128x56x56>}> : (!tt.device<#device>) -> tensor<1x128x56x56xf32, #layout48> loc(#loc545)
    %252 = "ttnn.relu"(%250, %251) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128x56x56xf32, #layout48>, tensor<1x128x56x56xf32, #layout48>) -> tensor<1x128x56x56xf32, #layout48> loc(#loc545)
    %253 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x56x128x56>}> : (!tt.device<#device>) -> tensor<1x56x128x56xf32, #layout47> loc(#loc546)
    %254 = "ttnn.transpose"(%252, %253) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x128x56x56xf32, #layout48>, tensor<1x56x128x56xf32, #layout47>) -> tensor<1x56x128x56xf32, #layout47> loc(#loc546)
    %255 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x56x56x128>}> : (!tt.device<#device>) -> tensor<1x56x56x128xf32, #layout46> loc(#loc547)
    %256 = "ttnn.transpose"(%254, %255) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x56x128x56xf32, #layout47>, tensor<1x56x56x128xf32, #layout46>) -> tensor<1x56x56x128xf32, #layout46> loc(#loc547)
    %257 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x28x28x128>}> : (!tt.device<#device>) -> tensor<1x28x28x128xf32, #layout50> loc(#loc548)
    %258 = "ttnn.conv2d"(%256, %arg119, %257, %0) <{batch_size = 1 : i32, dilation_height = 1 : i32, dilation_width = 1 : i32, groups = 1 : i32, in_channels = 128 : i32, input_height = 56 : i32, input_width = 56 : i32, kernel_height = 3 : i32, kernel_width = 3 : i32, out_channels = 128 : i32, padding_height = 1 : i32, padding_width = 1 : i32, stride_height = 2 : i32, stride_width = 2 : i32}> : (tensor<1x56x56x128xf32, #layout46>, tensor<128x128x3x3xf32, #layout13>, tensor<1x28x28x128xf32, #layout50>, !tt.device<#device>) -> tensor<1x28x28x128xf32, #layout50> loc(#loc548)
    %259 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x28x128x28>}> : (!tt.device<#device>) -> tensor<1x28x128x28xf32, #layout51> loc(#loc549)
    %260 = "ttnn.transpose"(%258, %259) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x28x28x128xf32, #layout50>, tensor<1x28x128x28xf32, #layout51>) -> tensor<1x28x128x28xf32, #layout51> loc(#loc549)
    %261 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x128x28x28>}> : (!tt.device<#device>) -> tensor<1x128x28x28xf32, #layout52> loc(#loc550)
    %262 = "ttnn.transpose"(%260, %261) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x28x128x28xf32, #layout51>, tensor<1x128x28x28xf32, #layout52>) -> tensor<1x128x28x28xf32, #layout52> loc(#loc550)
    %263 = "ttnn.to_layout"(%arg25, %0) <{layout = #ttnn.layout<tile>}> : (tensor<128x1x1xf32, #layout3>, !tt.device<#device>) -> tensor<128x1x1xf32, #layout49> loc(#loc551)
    %264 = "ttnn.to_device"(%263, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<128x1x1xf32, #layout49>, !tt.device<#device>) -> tensor<128x1x1xf32, #layout49> loc(#loc551)
    %265 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x128x28x28>}> : (!tt.device<#device>) -> tensor<1x128x28x28xf32, #layout52> loc(#loc551)
    %266 = "ttnn.multiply"(%262, %264, %265) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128x28x28xf32, #layout52>, tensor<128x1x1xf32, #layout49>, tensor<1x128x28x28xf32, #layout52>) -> tensor<1x128x28x28xf32, #layout52> loc(#loc551)
    %267 = "ttnn.to_layout"(%arg26, %0) <{layout = #ttnn.layout<tile>}> : (tensor<128x1x1xf32, #layout3>, !tt.device<#device>) -> tensor<128x1x1xf32, #layout49> loc(#loc552)
    %268 = "ttnn.to_device"(%267, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<128x1x1xf32, #layout49>, !tt.device<#device>) -> tensor<128x1x1xf32, #layout49> loc(#loc552)
    %269 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x128x28x28>}> : (!tt.device<#device>) -> tensor<1x128x28x28xf32, #layout52> loc(#loc552)
    %270 = "ttnn.add"(%266, %268, %269) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128x28x28xf32, #layout52>, tensor<128x1x1xf32, #layout49>, tensor<1x128x28x28xf32, #layout52>) -> tensor<1x128x28x28xf32, #layout52> loc(#loc552)
    %271 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x128x28x28>}> : (!tt.device<#device>) -> tensor<1x128x28x28xf32, #layout52> loc(#loc553)
    %272 = "ttnn.relu"(%270, %271) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128x28x28xf32, #layout52>, tensor<1x128x28x28xf32, #layout52>) -> tensor<1x128x28x28xf32, #layout52> loc(#loc553)
    %273 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x28x128x28>}> : (!tt.device<#device>) -> tensor<1x28x128x28xf32, #layout51> loc(#loc554)
    %274 = "ttnn.transpose"(%272, %273) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x128x28x28xf32, #layout52>, tensor<1x28x128x28xf32, #layout51>) -> tensor<1x28x128x28xf32, #layout51> loc(#loc554)
    %275 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x28x28x128>}> : (!tt.device<#device>) -> tensor<1x28x28x128xf32, #layout50> loc(#loc555)
    %276 = "ttnn.transpose"(%274, %275) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x28x128x28xf32, #layout51>, tensor<1x28x28x128xf32, #layout50>) -> tensor<1x28x28x128xf32, #layout50> loc(#loc555)
    %277 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x28x28x512>}> : (!tt.device<#device>) -> tensor<1x28x28x512xf32, #layout53> loc(#loc556)
    %278 = "ttnn.conv2d"(%276, %arg120, %277, %0) <{batch_size = 1 : i32, dilation_height = 1 : i32, dilation_width = 1 : i32, groups = 1 : i32, in_channels = 128 : i32, input_height = 28 : i32, input_width = 28 : i32, kernel_height = 1 : i32, kernel_width = 1 : i32, out_channels = 512 : i32, padding_height = 0 : i32, padding_width = 0 : i32, stride_height = 1 : i32, stride_width = 1 : i32}> : (tensor<1x28x28x128xf32, #layout50>, tensor<512x128x1x1xf32, #layout14>, tensor<1x28x28x512xf32, #layout53>, !tt.device<#device>) -> tensor<1x28x28x512xf32, #layout53> loc(#loc556)
    %279 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x28x512x28>}> : (!tt.device<#device>) -> tensor<1x28x512x28xf32, #layout54> loc(#loc557)
    %280 = "ttnn.transpose"(%278, %279) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x28x28x512xf32, #layout53>, tensor<1x28x512x28xf32, #layout54>) -> tensor<1x28x512x28xf32, #layout54> loc(#loc557)
    %281 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x512x28x28>}> : (!tt.device<#device>) -> tensor<1x512x28x28xf32, #layout55> loc(#loc558)
    %282 = "ttnn.transpose"(%280, %281) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x28x512x28xf32, #layout54>, tensor<1x512x28x28xf32, #layout55>) -> tensor<1x512x28x28xf32, #layout55> loc(#loc558)
    %283 = "ttnn.to_layout"(%arg27, %0) <{layout = #ttnn.layout<tile>}> : (tensor<512x1x1xf32, #layout4>, !tt.device<#device>) -> tensor<512x1x1xf32, #layout56> loc(#loc559)
    %284 = "ttnn.to_device"(%283, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<512x1x1xf32, #layout56>, !tt.device<#device>) -> tensor<512x1x1xf32, #layout56> loc(#loc559)
    %285 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x512x28x28>}> : (!tt.device<#device>) -> tensor<1x512x28x28xf32, #layout55> loc(#loc559)
    %286 = "ttnn.multiply"(%282, %284, %285) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x512x28x28xf32, #layout55>, tensor<512x1x1xf32, #layout56>, tensor<1x512x28x28xf32, #layout55>) -> tensor<1x512x28x28xf32, #layout55> loc(#loc559)
    %287 = "ttnn.to_layout"(%arg28, %0) <{layout = #ttnn.layout<tile>}> : (tensor<512x1x1xf32, #layout4>, !tt.device<#device>) -> tensor<512x1x1xf32, #layout56> loc(#loc560)
    %288 = "ttnn.to_device"(%287, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<512x1x1xf32, #layout56>, !tt.device<#device>) -> tensor<512x1x1xf32, #layout56> loc(#loc560)
    %289 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x512x28x28>}> : (!tt.device<#device>) -> tensor<1x512x28x28xf32, #layout55> loc(#loc560)
    %290 = "ttnn.add"(%286, %288, %289) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x512x28x28xf32, #layout55>, tensor<512x1x1xf32, #layout56>, tensor<1x512x28x28xf32, #layout55>) -> tensor<1x512x28x28xf32, #layout55> loc(#loc560)
    %291 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x56x256x56>}> : (!tt.device<#device>) -> tensor<1x56x256x56xf32, #layout43> loc(#loc561)
    %292 = "ttnn.transpose"(%232, %291) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x256x56x56xf32, #layout44>, tensor<1x56x256x56xf32, #layout43>) -> tensor<1x56x256x56xf32, #layout43> loc(#loc561)
    %293 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x56x56x256>}> : (!tt.device<#device>) -> tensor<1x56x56x256xf32, #layout42> loc(#loc562)
    %294 = "ttnn.transpose"(%292, %293) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x56x256x56xf32, #layout43>, tensor<1x56x56x256xf32, #layout42>) -> tensor<1x56x56x256xf32, #layout42> loc(#loc562)
    %295 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x28x28x512>}> : (!tt.device<#device>) -> tensor<1x28x28x512xf32, #layout53> loc(#loc563)
    %296 = "ttnn.conv2d"(%294, %arg121, %295, %0) <{batch_size = 1 : i32, dilation_height = 1 : i32, dilation_width = 1 : i32, groups = 1 : i32, in_channels = 256 : i32, input_height = 56 : i32, input_width = 56 : i32, kernel_height = 1 : i32, kernel_width = 1 : i32, out_channels = 512 : i32, padding_height = 0 : i32, padding_width = 0 : i32, stride_height = 2 : i32, stride_width = 2 : i32}> : (tensor<1x56x56x256xf32, #layout42>, tensor<512x256x1x1xf32, #layout15>, tensor<1x28x28x512xf32, #layout53>, !tt.device<#device>) -> tensor<1x28x28x512xf32, #layout53> loc(#loc563)
    %297 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x28x512x28>}> : (!tt.device<#device>) -> tensor<1x28x512x28xf32, #layout54> loc(#loc564)
    %298 = "ttnn.transpose"(%296, %297) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x28x28x512xf32, #layout53>, tensor<1x28x512x28xf32, #layout54>) -> tensor<1x28x512x28xf32, #layout54> loc(#loc564)
    %299 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x512x28x28>}> : (!tt.device<#device>) -> tensor<1x512x28x28xf32, #layout55> loc(#loc565)
    %300 = "ttnn.transpose"(%298, %299) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x28x512x28xf32, #layout54>, tensor<1x512x28x28xf32, #layout55>) -> tensor<1x512x28x28xf32, #layout55> loc(#loc565)
    %301 = "ttnn.to_layout"(%arg29, %0) <{layout = #ttnn.layout<tile>}> : (tensor<512x1x1xf32, #layout4>, !tt.device<#device>) -> tensor<512x1x1xf32, #layout56> loc(#loc566)
    %302 = "ttnn.to_device"(%301, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<512x1x1xf32, #layout56>, !tt.device<#device>) -> tensor<512x1x1xf32, #layout56> loc(#loc566)
    %303 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x512x28x28>}> : (!tt.device<#device>) -> tensor<1x512x28x28xf32, #layout55> loc(#loc566)
    %304 = "ttnn.multiply"(%300, %302, %303) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x512x28x28xf32, #layout55>, tensor<512x1x1xf32, #layout56>, tensor<1x512x28x28xf32, #layout55>) -> tensor<1x512x28x28xf32, #layout55> loc(#loc566)
    %305 = "ttnn.to_layout"(%arg30, %0) <{layout = #ttnn.layout<tile>}> : (tensor<512x1x1xf32, #layout4>, !tt.device<#device>) -> tensor<512x1x1xf32, #layout56> loc(#loc567)
    %306 = "ttnn.to_device"(%305, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<512x1x1xf32, #layout56>, !tt.device<#device>) -> tensor<512x1x1xf32, #layout56> loc(#loc567)
    %307 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x512x28x28>}> : (!tt.device<#device>) -> tensor<1x512x28x28xf32, #layout55> loc(#loc567)
    %308 = "ttnn.add"(%304, %306, %307) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x512x28x28xf32, #layout55>, tensor<512x1x1xf32, #layout56>, tensor<1x512x28x28xf32, #layout55>) -> tensor<1x512x28x28xf32, #layout55> loc(#loc567)
    %309 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x512x28x28>}> : (!tt.device<#device>) -> tensor<1x512x28x28xf32, #layout55> loc(#loc568)
    %310 = "ttnn.add"(%290, %308, %309) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x512x28x28xf32, #layout55>, tensor<1x512x28x28xf32, #layout55>, tensor<1x512x28x28xf32, #layout55>) -> tensor<1x512x28x28xf32, #layout55> loc(#loc568)
    %311 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x512x28x28>}> : (!tt.device<#device>) -> tensor<1x512x28x28xf32, #layout55> loc(#loc569)
    %312 = "ttnn.relu"(%310, %311) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x512x28x28xf32, #layout55>, tensor<1x512x28x28xf32, #layout55>) -> tensor<1x512x28x28xf32, #layout55> loc(#loc569)
    %313 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x28x512x28>}> : (!tt.device<#device>) -> tensor<1x28x512x28xf32, #layout54> loc(#loc570)
    %314 = "ttnn.transpose"(%312, %313) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x512x28x28xf32, #layout55>, tensor<1x28x512x28xf32, #layout54>) -> tensor<1x28x512x28xf32, #layout54> loc(#loc570)
    %315 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x28x28x512>}> : (!tt.device<#device>) -> tensor<1x28x28x512xf32, #layout53> loc(#loc571)
    %316 = "ttnn.transpose"(%314, %315) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x28x512x28xf32, #layout54>, tensor<1x28x28x512xf32, #layout53>) -> tensor<1x28x28x512xf32, #layout53> loc(#loc571)
    %317 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x28x28x128>}> : (!tt.device<#device>) -> tensor<1x28x28x128xf32, #layout50> loc(#loc572)
    %318 = "ttnn.conv2d"(%316, %arg122, %317, %0) <{batch_size = 1 : i32, dilation_height = 1 : i32, dilation_width = 1 : i32, groups = 1 : i32, in_channels = 512 : i32, input_height = 28 : i32, input_width = 28 : i32, kernel_height = 1 : i32, kernel_width = 1 : i32, out_channels = 128 : i32, padding_height = 0 : i32, padding_width = 0 : i32, stride_height = 1 : i32, stride_width = 1 : i32}> : (tensor<1x28x28x512xf32, #layout53>, tensor<128x512x1x1xf32, #layout16>, tensor<1x28x28x128xf32, #layout50>, !tt.device<#device>) -> tensor<1x28x28x128xf32, #layout50> loc(#loc572)
    %319 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x28x128x28>}> : (!tt.device<#device>) -> tensor<1x28x128x28xf32, #layout51> loc(#loc573)
    %320 = "ttnn.transpose"(%318, %319) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x28x28x128xf32, #layout50>, tensor<1x28x128x28xf32, #layout51>) -> tensor<1x28x128x28xf32, #layout51> loc(#loc573)
    %321 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x128x28x28>}> : (!tt.device<#device>) -> tensor<1x128x28x28xf32, #layout52> loc(#loc574)
    %322 = "ttnn.transpose"(%320, %321) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x28x128x28xf32, #layout51>, tensor<1x128x28x28xf32, #layout52>) -> tensor<1x128x28x28xf32, #layout52> loc(#loc574)
    %323 = "ttnn.to_layout"(%arg31, %0) <{layout = #ttnn.layout<tile>}> : (tensor<128x1x1xf32, #layout3>, !tt.device<#device>) -> tensor<128x1x1xf32, #layout49> loc(#loc575)
    %324 = "ttnn.to_device"(%323, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<128x1x1xf32, #layout49>, !tt.device<#device>) -> tensor<128x1x1xf32, #layout49> loc(#loc575)
    %325 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x128x28x28>}> : (!tt.device<#device>) -> tensor<1x128x28x28xf32, #layout52> loc(#loc575)
    %326 = "ttnn.multiply"(%322, %324, %325) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128x28x28xf32, #layout52>, tensor<128x1x1xf32, #layout49>, tensor<1x128x28x28xf32, #layout52>) -> tensor<1x128x28x28xf32, #layout52> loc(#loc575)
    %327 = "ttnn.to_layout"(%arg32, %0) <{layout = #ttnn.layout<tile>}> : (tensor<128x1x1xf32, #layout3>, !tt.device<#device>) -> tensor<128x1x1xf32, #layout49> loc(#loc576)
    %328 = "ttnn.to_device"(%327, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<128x1x1xf32, #layout49>, !tt.device<#device>) -> tensor<128x1x1xf32, #layout49> loc(#loc576)
    %329 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x128x28x28>}> : (!tt.device<#device>) -> tensor<1x128x28x28xf32, #layout52> loc(#loc576)
    %330 = "ttnn.add"(%326, %328, %329) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128x28x28xf32, #layout52>, tensor<128x1x1xf32, #layout49>, tensor<1x128x28x28xf32, #layout52>) -> tensor<1x128x28x28xf32, #layout52> loc(#loc576)
    %331 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x128x28x28>}> : (!tt.device<#device>) -> tensor<1x128x28x28xf32, #layout52> loc(#loc577)
    %332 = "ttnn.relu"(%330, %331) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128x28x28xf32, #layout52>, tensor<1x128x28x28xf32, #layout52>) -> tensor<1x128x28x28xf32, #layout52> loc(#loc577)
    %333 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x28x128x28>}> : (!tt.device<#device>) -> tensor<1x28x128x28xf32, #layout51> loc(#loc578)
    %334 = "ttnn.transpose"(%332, %333) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x128x28x28xf32, #layout52>, tensor<1x28x128x28xf32, #layout51>) -> tensor<1x28x128x28xf32, #layout51> loc(#loc578)
    %335 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x28x28x128>}> : (!tt.device<#device>) -> tensor<1x28x28x128xf32, #layout50> loc(#loc579)
    %336 = "ttnn.transpose"(%334, %335) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x28x128x28xf32, #layout51>, tensor<1x28x28x128xf32, #layout50>) -> tensor<1x28x28x128xf32, #layout50> loc(#loc579)
    %337 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x28x28x128>}> : (!tt.device<#device>) -> tensor<1x28x28x128xf32, #layout50> loc(#loc580)
    %338 = "ttnn.conv2d"(%336, %arg123, %337, %0) <{batch_size = 1 : i32, dilation_height = 1 : i32, dilation_width = 1 : i32, groups = 1 : i32, in_channels = 128 : i32, input_height = 28 : i32, input_width = 28 : i32, kernel_height = 3 : i32, kernel_width = 3 : i32, out_channels = 128 : i32, padding_height = 1 : i32, padding_width = 1 : i32, stride_height = 1 : i32, stride_width = 1 : i32}> : (tensor<1x28x28x128xf32, #layout50>, tensor<128x128x3x3xf32, #layout13>, tensor<1x28x28x128xf32, #layout50>, !tt.device<#device>) -> tensor<1x28x28x128xf32, #layout50> loc(#loc580)
    %339 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x28x128x28>}> : (!tt.device<#device>) -> tensor<1x28x128x28xf32, #layout51> loc(#loc581)
    %340 = "ttnn.transpose"(%338, %339) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x28x28x128xf32, #layout50>, tensor<1x28x128x28xf32, #layout51>) -> tensor<1x28x128x28xf32, #layout51> loc(#loc581)
    %341 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x128x28x28>}> : (!tt.device<#device>) -> tensor<1x128x28x28xf32, #layout52> loc(#loc582)
    %342 = "ttnn.transpose"(%340, %341) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x28x128x28xf32, #layout51>, tensor<1x128x28x28xf32, #layout52>) -> tensor<1x128x28x28xf32, #layout52> loc(#loc582)
    %343 = "ttnn.to_layout"(%arg33, %0) <{layout = #ttnn.layout<tile>}> : (tensor<128x1x1xf32, #layout3>, !tt.device<#device>) -> tensor<128x1x1xf32, #layout49> loc(#loc583)
    %344 = "ttnn.to_device"(%343, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<128x1x1xf32, #layout49>, !tt.device<#device>) -> tensor<128x1x1xf32, #layout49> loc(#loc583)
    %345 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x128x28x28>}> : (!tt.device<#device>) -> tensor<1x128x28x28xf32, #layout52> loc(#loc583)
    %346 = "ttnn.multiply"(%342, %344, %345) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128x28x28xf32, #layout52>, tensor<128x1x1xf32, #layout49>, tensor<1x128x28x28xf32, #layout52>) -> tensor<1x128x28x28xf32, #layout52> loc(#loc583)
    %347 = "ttnn.to_layout"(%arg34, %0) <{layout = #ttnn.layout<tile>}> : (tensor<128x1x1xf32, #layout3>, !tt.device<#device>) -> tensor<128x1x1xf32, #layout49> loc(#loc584)
    %348 = "ttnn.to_device"(%347, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<128x1x1xf32, #layout49>, !tt.device<#device>) -> tensor<128x1x1xf32, #layout49> loc(#loc584)
    %349 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x128x28x28>}> : (!tt.device<#device>) -> tensor<1x128x28x28xf32, #layout52> loc(#loc584)
    %350 = "ttnn.add"(%346, %348, %349) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128x28x28xf32, #layout52>, tensor<128x1x1xf32, #layout49>, tensor<1x128x28x28xf32, #layout52>) -> tensor<1x128x28x28xf32, #layout52> loc(#loc584)
    %351 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x128x28x28>}> : (!tt.device<#device>) -> tensor<1x128x28x28xf32, #layout52> loc(#loc585)
    %352 = "ttnn.relu"(%350, %351) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128x28x28xf32, #layout52>, tensor<1x128x28x28xf32, #layout52>) -> tensor<1x128x28x28xf32, #layout52> loc(#loc585)
    %353 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x28x128x28>}> : (!tt.device<#device>) -> tensor<1x28x128x28xf32, #layout51> loc(#loc586)
    %354 = "ttnn.transpose"(%352, %353) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x128x28x28xf32, #layout52>, tensor<1x28x128x28xf32, #layout51>) -> tensor<1x28x128x28xf32, #layout51> loc(#loc586)
    %355 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x28x28x128>}> : (!tt.device<#device>) -> tensor<1x28x28x128xf32, #layout50> loc(#loc587)
    %356 = "ttnn.transpose"(%354, %355) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x28x128x28xf32, #layout51>, tensor<1x28x28x128xf32, #layout50>) -> tensor<1x28x28x128xf32, #layout50> loc(#loc587)
    %357 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x28x28x512>}> : (!tt.device<#device>) -> tensor<1x28x28x512xf32, #layout53> loc(#loc588)
    %358 = "ttnn.conv2d"(%356, %arg124, %357, %0) <{batch_size = 1 : i32, dilation_height = 1 : i32, dilation_width = 1 : i32, groups = 1 : i32, in_channels = 128 : i32, input_height = 28 : i32, input_width = 28 : i32, kernel_height = 1 : i32, kernel_width = 1 : i32, out_channels = 512 : i32, padding_height = 0 : i32, padding_width = 0 : i32, stride_height = 1 : i32, stride_width = 1 : i32}> : (tensor<1x28x28x128xf32, #layout50>, tensor<512x128x1x1xf32, #layout14>, tensor<1x28x28x512xf32, #layout53>, !tt.device<#device>) -> tensor<1x28x28x512xf32, #layout53> loc(#loc588)
    %359 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x28x512x28>}> : (!tt.device<#device>) -> tensor<1x28x512x28xf32, #layout54> loc(#loc589)
    %360 = "ttnn.transpose"(%358, %359) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x28x28x512xf32, #layout53>, tensor<1x28x512x28xf32, #layout54>) -> tensor<1x28x512x28xf32, #layout54> loc(#loc589)
    %361 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x512x28x28>}> : (!tt.device<#device>) -> tensor<1x512x28x28xf32, #layout55> loc(#loc590)
    %362 = "ttnn.transpose"(%360, %361) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x28x512x28xf32, #layout54>, tensor<1x512x28x28xf32, #layout55>) -> tensor<1x512x28x28xf32, #layout55> loc(#loc590)
    %363 = "ttnn.to_layout"(%arg35, %0) <{layout = #ttnn.layout<tile>}> : (tensor<512x1x1xf32, #layout4>, !tt.device<#device>) -> tensor<512x1x1xf32, #layout56> loc(#loc591)
    %364 = "ttnn.to_device"(%363, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<512x1x1xf32, #layout56>, !tt.device<#device>) -> tensor<512x1x1xf32, #layout56> loc(#loc591)
    %365 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x512x28x28>}> : (!tt.device<#device>) -> tensor<1x512x28x28xf32, #layout55> loc(#loc591)
    %366 = "ttnn.multiply"(%362, %364, %365) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x512x28x28xf32, #layout55>, tensor<512x1x1xf32, #layout56>, tensor<1x512x28x28xf32, #layout55>) -> tensor<1x512x28x28xf32, #layout55> loc(#loc591)
    %367 = "ttnn.to_layout"(%arg36, %0) <{layout = #ttnn.layout<tile>}> : (tensor<512x1x1xf32, #layout4>, !tt.device<#device>) -> tensor<512x1x1xf32, #layout56> loc(#loc592)
    %368 = "ttnn.to_device"(%367, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<512x1x1xf32, #layout56>, !tt.device<#device>) -> tensor<512x1x1xf32, #layout56> loc(#loc592)
    %369 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x512x28x28>}> : (!tt.device<#device>) -> tensor<1x512x28x28xf32, #layout55> loc(#loc592)
    %370 = "ttnn.add"(%366, %368, %369) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x512x28x28xf32, #layout55>, tensor<512x1x1xf32, #layout56>, tensor<1x512x28x28xf32, #layout55>) -> tensor<1x512x28x28xf32, #layout55> loc(#loc592)
    %371 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x512x28x28>}> : (!tt.device<#device>) -> tensor<1x512x28x28xf32, #layout55> loc(#loc593)
    %372 = "ttnn.add"(%370, %312, %371) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x512x28x28xf32, #layout55>, tensor<1x512x28x28xf32, #layout55>, tensor<1x512x28x28xf32, #layout55>) -> tensor<1x512x28x28xf32, #layout55> loc(#loc593)
    %373 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x512x28x28>}> : (!tt.device<#device>) -> tensor<1x512x28x28xf32, #layout55> loc(#loc594)
    %374 = "ttnn.relu"(%372, %373) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x512x28x28xf32, #layout55>, tensor<1x512x28x28xf32, #layout55>) -> tensor<1x512x28x28xf32, #layout55> loc(#loc594)
    %375 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x28x512x28>}> : (!tt.device<#device>) -> tensor<1x28x512x28xf32, #layout54> loc(#loc595)
    %376 = "ttnn.transpose"(%374, %375) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x512x28x28xf32, #layout55>, tensor<1x28x512x28xf32, #layout54>) -> tensor<1x28x512x28xf32, #layout54> loc(#loc595)
    %377 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x28x28x512>}> : (!tt.device<#device>) -> tensor<1x28x28x512xf32, #layout53> loc(#loc596)
    %378 = "ttnn.transpose"(%376, %377) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x28x512x28xf32, #layout54>, tensor<1x28x28x512xf32, #layout53>) -> tensor<1x28x28x512xf32, #layout53> loc(#loc596)
    %379 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x28x28x128>}> : (!tt.device<#device>) -> tensor<1x28x28x128xf32, #layout50> loc(#loc597)
    %380 = "ttnn.conv2d"(%378, %arg125, %379, %0) <{batch_size = 1 : i32, dilation_height = 1 : i32, dilation_width = 1 : i32, groups = 1 : i32, in_channels = 512 : i32, input_height = 28 : i32, input_width = 28 : i32, kernel_height = 1 : i32, kernel_width = 1 : i32, out_channels = 128 : i32, padding_height = 0 : i32, padding_width = 0 : i32, stride_height = 1 : i32, stride_width = 1 : i32}> : (tensor<1x28x28x512xf32, #layout53>, tensor<128x512x1x1xf32, #layout16>, tensor<1x28x28x128xf32, #layout50>, !tt.device<#device>) -> tensor<1x28x28x128xf32, #layout50> loc(#loc597)
    %381 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x28x128x28>}> : (!tt.device<#device>) -> tensor<1x28x128x28xf32, #layout51> loc(#loc598)
    %382 = "ttnn.transpose"(%380, %381) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x28x28x128xf32, #layout50>, tensor<1x28x128x28xf32, #layout51>) -> tensor<1x28x128x28xf32, #layout51> loc(#loc598)
    %383 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x128x28x28>}> : (!tt.device<#device>) -> tensor<1x128x28x28xf32, #layout52> loc(#loc599)
    %384 = "ttnn.transpose"(%382, %383) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x28x128x28xf32, #layout51>, tensor<1x128x28x28xf32, #layout52>) -> tensor<1x128x28x28xf32, #layout52> loc(#loc599)
    %385 = "ttnn.to_layout"(%arg37, %0) <{layout = #ttnn.layout<tile>}> : (tensor<128x1x1xf32, #layout3>, !tt.device<#device>) -> tensor<128x1x1xf32, #layout49> loc(#loc600)
    %386 = "ttnn.to_device"(%385, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<128x1x1xf32, #layout49>, !tt.device<#device>) -> tensor<128x1x1xf32, #layout49> loc(#loc600)
    %387 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x128x28x28>}> : (!tt.device<#device>) -> tensor<1x128x28x28xf32, #layout52> loc(#loc600)
    %388 = "ttnn.multiply"(%384, %386, %387) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128x28x28xf32, #layout52>, tensor<128x1x1xf32, #layout49>, tensor<1x128x28x28xf32, #layout52>) -> tensor<1x128x28x28xf32, #layout52> loc(#loc600)
    %389 = "ttnn.to_layout"(%arg38, %0) <{layout = #ttnn.layout<tile>}> : (tensor<128x1x1xf32, #layout3>, !tt.device<#device>) -> tensor<128x1x1xf32, #layout49> loc(#loc601)
    %390 = "ttnn.to_device"(%389, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<128x1x1xf32, #layout49>, !tt.device<#device>) -> tensor<128x1x1xf32, #layout49> loc(#loc601)
    %391 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x128x28x28>}> : (!tt.device<#device>) -> tensor<1x128x28x28xf32, #layout52> loc(#loc601)
    %392 = "ttnn.add"(%388, %390, %391) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128x28x28xf32, #layout52>, tensor<128x1x1xf32, #layout49>, tensor<1x128x28x28xf32, #layout52>) -> tensor<1x128x28x28xf32, #layout52> loc(#loc601)
    %393 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x128x28x28>}> : (!tt.device<#device>) -> tensor<1x128x28x28xf32, #layout52> loc(#loc602)
    %394 = "ttnn.relu"(%392, %393) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128x28x28xf32, #layout52>, tensor<1x128x28x28xf32, #layout52>) -> tensor<1x128x28x28xf32, #layout52> loc(#loc602)
    %395 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x28x128x28>}> : (!tt.device<#device>) -> tensor<1x28x128x28xf32, #layout51> loc(#loc603)
    %396 = "ttnn.transpose"(%394, %395) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x128x28x28xf32, #layout52>, tensor<1x28x128x28xf32, #layout51>) -> tensor<1x28x128x28xf32, #layout51> loc(#loc603)
    %397 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x28x28x128>}> : (!tt.device<#device>) -> tensor<1x28x28x128xf32, #layout50> loc(#loc604)
    %398 = "ttnn.transpose"(%396, %397) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x28x128x28xf32, #layout51>, tensor<1x28x28x128xf32, #layout50>) -> tensor<1x28x28x128xf32, #layout50> loc(#loc604)
    %399 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x28x28x128>}> : (!tt.device<#device>) -> tensor<1x28x28x128xf32, #layout50> loc(#loc605)
    %400 = "ttnn.conv2d"(%398, %arg126, %399, %0) <{batch_size = 1 : i32, dilation_height = 1 : i32, dilation_width = 1 : i32, groups = 1 : i32, in_channels = 128 : i32, input_height = 28 : i32, input_width = 28 : i32, kernel_height = 3 : i32, kernel_width = 3 : i32, out_channels = 128 : i32, padding_height = 1 : i32, padding_width = 1 : i32, stride_height = 1 : i32, stride_width = 1 : i32}> : (tensor<1x28x28x128xf32, #layout50>, tensor<128x128x3x3xf32, #layout13>, tensor<1x28x28x128xf32, #layout50>, !tt.device<#device>) -> tensor<1x28x28x128xf32, #layout50> loc(#loc605)
    %401 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x28x128x28>}> : (!tt.device<#device>) -> tensor<1x28x128x28xf32, #layout51> loc(#loc606)
    %402 = "ttnn.transpose"(%400, %401) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x28x28x128xf32, #layout50>, tensor<1x28x128x28xf32, #layout51>) -> tensor<1x28x128x28xf32, #layout51> loc(#loc606)
    %403 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x128x28x28>}> : (!tt.device<#device>) -> tensor<1x128x28x28xf32, #layout52> loc(#loc607)
    %404 = "ttnn.transpose"(%402, %403) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x28x128x28xf32, #layout51>, tensor<1x128x28x28xf32, #layout52>) -> tensor<1x128x28x28xf32, #layout52> loc(#loc607)
    %405 = "ttnn.to_layout"(%arg39, %0) <{layout = #ttnn.layout<tile>}> : (tensor<128x1x1xf32, #layout3>, !tt.device<#device>) -> tensor<128x1x1xf32, #layout49> loc(#loc608)
    %406 = "ttnn.to_device"(%405, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<128x1x1xf32, #layout49>, !tt.device<#device>) -> tensor<128x1x1xf32, #layout49> loc(#loc608)
    %407 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x128x28x28>}> : (!tt.device<#device>) -> tensor<1x128x28x28xf32, #layout52> loc(#loc608)
    %408 = "ttnn.multiply"(%404, %406, %407) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128x28x28xf32, #layout52>, tensor<128x1x1xf32, #layout49>, tensor<1x128x28x28xf32, #layout52>) -> tensor<1x128x28x28xf32, #layout52> loc(#loc608)
    %409 = "ttnn.to_layout"(%arg40, %0) <{layout = #ttnn.layout<tile>}> : (tensor<128x1x1xf32, #layout3>, !tt.device<#device>) -> tensor<128x1x1xf32, #layout49> loc(#loc609)
    %410 = "ttnn.to_device"(%409, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<128x1x1xf32, #layout49>, !tt.device<#device>) -> tensor<128x1x1xf32, #layout49> loc(#loc609)
    %411 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x128x28x28>}> : (!tt.device<#device>) -> tensor<1x128x28x28xf32, #layout52> loc(#loc609)
    %412 = "ttnn.add"(%408, %410, %411) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128x28x28xf32, #layout52>, tensor<128x1x1xf32, #layout49>, tensor<1x128x28x28xf32, #layout52>) -> tensor<1x128x28x28xf32, #layout52> loc(#loc609)
    %413 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x128x28x28>}> : (!tt.device<#device>) -> tensor<1x128x28x28xf32, #layout52> loc(#loc610)
    %414 = "ttnn.relu"(%412, %413) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128x28x28xf32, #layout52>, tensor<1x128x28x28xf32, #layout52>) -> tensor<1x128x28x28xf32, #layout52> loc(#loc610)
    %415 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x28x128x28>}> : (!tt.device<#device>) -> tensor<1x28x128x28xf32, #layout51> loc(#loc611)
    %416 = "ttnn.transpose"(%414, %415) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x128x28x28xf32, #layout52>, tensor<1x28x128x28xf32, #layout51>) -> tensor<1x28x128x28xf32, #layout51> loc(#loc611)
    %417 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x28x28x128>}> : (!tt.device<#device>) -> tensor<1x28x28x128xf32, #layout50> loc(#loc612)
    %418 = "ttnn.transpose"(%416, %417) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x28x128x28xf32, #layout51>, tensor<1x28x28x128xf32, #layout50>) -> tensor<1x28x28x128xf32, #layout50> loc(#loc612)
    %419 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x28x28x512>}> : (!tt.device<#device>) -> tensor<1x28x28x512xf32, #layout53> loc(#loc613)
    %420 = "ttnn.conv2d"(%418, %arg127, %419, %0) <{batch_size = 1 : i32, dilation_height = 1 : i32, dilation_width = 1 : i32, groups = 1 : i32, in_channels = 128 : i32, input_height = 28 : i32, input_width = 28 : i32, kernel_height = 1 : i32, kernel_width = 1 : i32, out_channels = 512 : i32, padding_height = 0 : i32, padding_width = 0 : i32, stride_height = 1 : i32, stride_width = 1 : i32}> : (tensor<1x28x28x128xf32, #layout50>, tensor<512x128x1x1xf32, #layout14>, tensor<1x28x28x512xf32, #layout53>, !tt.device<#device>) -> tensor<1x28x28x512xf32, #layout53> loc(#loc613)
    %421 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x28x512x28>}> : (!tt.device<#device>) -> tensor<1x28x512x28xf32, #layout54> loc(#loc614)
    %422 = "ttnn.transpose"(%420, %421) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x28x28x512xf32, #layout53>, tensor<1x28x512x28xf32, #layout54>) -> tensor<1x28x512x28xf32, #layout54> loc(#loc614)
    %423 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x512x28x28>}> : (!tt.device<#device>) -> tensor<1x512x28x28xf32, #layout55> loc(#loc615)
    %424 = "ttnn.transpose"(%422, %423) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x28x512x28xf32, #layout54>, tensor<1x512x28x28xf32, #layout55>) -> tensor<1x512x28x28xf32, #layout55> loc(#loc615)
    %425 = "ttnn.to_layout"(%arg41, %0) <{layout = #ttnn.layout<tile>}> : (tensor<512x1x1xf32, #layout4>, !tt.device<#device>) -> tensor<512x1x1xf32, #layout56> loc(#loc616)
    %426 = "ttnn.to_device"(%425, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<512x1x1xf32, #layout56>, !tt.device<#device>) -> tensor<512x1x1xf32, #layout56> loc(#loc616)
    %427 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x512x28x28>}> : (!tt.device<#device>) -> tensor<1x512x28x28xf32, #layout55> loc(#loc616)
    %428 = "ttnn.multiply"(%424, %426, %427) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x512x28x28xf32, #layout55>, tensor<512x1x1xf32, #layout56>, tensor<1x512x28x28xf32, #layout55>) -> tensor<1x512x28x28xf32, #layout55> loc(#loc616)
    %429 = "ttnn.to_layout"(%arg42, %0) <{layout = #ttnn.layout<tile>}> : (tensor<512x1x1xf32, #layout4>, !tt.device<#device>) -> tensor<512x1x1xf32, #layout56> loc(#loc617)
    %430 = "ttnn.to_device"(%429, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<512x1x1xf32, #layout56>, !tt.device<#device>) -> tensor<512x1x1xf32, #layout56> loc(#loc617)
    %431 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x512x28x28>}> : (!tt.device<#device>) -> tensor<1x512x28x28xf32, #layout55> loc(#loc617)
    %432 = "ttnn.add"(%428, %430, %431) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x512x28x28xf32, #layout55>, tensor<512x1x1xf32, #layout56>, tensor<1x512x28x28xf32, #layout55>) -> tensor<1x512x28x28xf32, #layout55> loc(#loc617)
    %433 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x512x28x28>}> : (!tt.device<#device>) -> tensor<1x512x28x28xf32, #layout55> loc(#loc618)
    %434 = "ttnn.add"(%432, %374, %433) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x512x28x28xf32, #layout55>, tensor<1x512x28x28xf32, #layout55>, tensor<1x512x28x28xf32, #layout55>) -> tensor<1x512x28x28xf32, #layout55> loc(#loc618)
    %435 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x512x28x28>}> : (!tt.device<#device>) -> tensor<1x512x28x28xf32, #layout55> loc(#loc619)
    %436 = "ttnn.relu"(%434, %435) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x512x28x28xf32, #layout55>, tensor<1x512x28x28xf32, #layout55>) -> tensor<1x512x28x28xf32, #layout55> loc(#loc619)
    %437 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x28x512x28>}> : (!tt.device<#device>) -> tensor<1x28x512x28xf32, #layout54> loc(#loc620)
    %438 = "ttnn.transpose"(%436, %437) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x512x28x28xf32, #layout55>, tensor<1x28x512x28xf32, #layout54>) -> tensor<1x28x512x28xf32, #layout54> loc(#loc620)
    %439 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x28x28x512>}> : (!tt.device<#device>) -> tensor<1x28x28x512xf32, #layout53> loc(#loc621)
    %440 = "ttnn.transpose"(%438, %439) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x28x512x28xf32, #layout54>, tensor<1x28x28x512xf32, #layout53>) -> tensor<1x28x28x512xf32, #layout53> loc(#loc621)
    %441 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x28x28x128>}> : (!tt.device<#device>) -> tensor<1x28x28x128xf32, #layout50> loc(#loc622)
    %442 = "ttnn.conv2d"(%440, %arg128, %441, %0) <{batch_size = 1 : i32, dilation_height = 1 : i32, dilation_width = 1 : i32, groups = 1 : i32, in_channels = 512 : i32, input_height = 28 : i32, input_width = 28 : i32, kernel_height = 1 : i32, kernel_width = 1 : i32, out_channels = 128 : i32, padding_height = 0 : i32, padding_width = 0 : i32, stride_height = 1 : i32, stride_width = 1 : i32}> : (tensor<1x28x28x512xf32, #layout53>, tensor<128x512x1x1xf32, #layout16>, tensor<1x28x28x128xf32, #layout50>, !tt.device<#device>) -> tensor<1x28x28x128xf32, #layout50> loc(#loc622)
    %443 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x28x128x28>}> : (!tt.device<#device>) -> tensor<1x28x128x28xf32, #layout51> loc(#loc623)
    %444 = "ttnn.transpose"(%442, %443) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x28x28x128xf32, #layout50>, tensor<1x28x128x28xf32, #layout51>) -> tensor<1x28x128x28xf32, #layout51> loc(#loc623)
    %445 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x128x28x28>}> : (!tt.device<#device>) -> tensor<1x128x28x28xf32, #layout52> loc(#loc624)
    %446 = "ttnn.transpose"(%444, %445) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x28x128x28xf32, #layout51>, tensor<1x128x28x28xf32, #layout52>) -> tensor<1x128x28x28xf32, #layout52> loc(#loc624)
    %447 = "ttnn.to_layout"(%arg43, %0) <{layout = #ttnn.layout<tile>}> : (tensor<128x1x1xf32, #layout3>, !tt.device<#device>) -> tensor<128x1x1xf32, #layout49> loc(#loc625)
    %448 = "ttnn.to_device"(%447, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<128x1x1xf32, #layout49>, !tt.device<#device>) -> tensor<128x1x1xf32, #layout49> loc(#loc625)
    %449 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x128x28x28>}> : (!tt.device<#device>) -> tensor<1x128x28x28xf32, #layout52> loc(#loc625)
    %450 = "ttnn.multiply"(%446, %448, %449) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128x28x28xf32, #layout52>, tensor<128x1x1xf32, #layout49>, tensor<1x128x28x28xf32, #layout52>) -> tensor<1x128x28x28xf32, #layout52> loc(#loc625)
    %451 = "ttnn.to_layout"(%arg44, %0) <{layout = #ttnn.layout<tile>}> : (tensor<128x1x1xf32, #layout3>, !tt.device<#device>) -> tensor<128x1x1xf32, #layout49> loc(#loc626)
    %452 = "ttnn.to_device"(%451, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<128x1x1xf32, #layout49>, !tt.device<#device>) -> tensor<128x1x1xf32, #layout49> loc(#loc626)
    %453 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x128x28x28>}> : (!tt.device<#device>) -> tensor<1x128x28x28xf32, #layout52> loc(#loc626)
    %454 = "ttnn.add"(%450, %452, %453) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128x28x28xf32, #layout52>, tensor<128x1x1xf32, #layout49>, tensor<1x128x28x28xf32, #layout52>) -> tensor<1x128x28x28xf32, #layout52> loc(#loc626)
    %455 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x128x28x28>}> : (!tt.device<#device>) -> tensor<1x128x28x28xf32, #layout52> loc(#loc627)
    %456 = "ttnn.relu"(%454, %455) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128x28x28xf32, #layout52>, tensor<1x128x28x28xf32, #layout52>) -> tensor<1x128x28x28xf32, #layout52> loc(#loc627)
    %457 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x28x128x28>}> : (!tt.device<#device>) -> tensor<1x28x128x28xf32, #layout51> loc(#loc628)
    %458 = "ttnn.transpose"(%456, %457) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x128x28x28xf32, #layout52>, tensor<1x28x128x28xf32, #layout51>) -> tensor<1x28x128x28xf32, #layout51> loc(#loc628)
    %459 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x28x28x128>}> : (!tt.device<#device>) -> tensor<1x28x28x128xf32, #layout50> loc(#loc629)
    %460 = "ttnn.transpose"(%458, %459) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x28x128x28xf32, #layout51>, tensor<1x28x28x128xf32, #layout50>) -> tensor<1x28x28x128xf32, #layout50> loc(#loc629)
    %461 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x28x28x128>}> : (!tt.device<#device>) -> tensor<1x28x28x128xf32, #layout50> loc(#loc630)
    %462 = "ttnn.conv2d"(%460, %arg129, %461, %0) <{batch_size = 1 : i32, dilation_height = 1 : i32, dilation_width = 1 : i32, groups = 1 : i32, in_channels = 128 : i32, input_height = 28 : i32, input_width = 28 : i32, kernel_height = 3 : i32, kernel_width = 3 : i32, out_channels = 128 : i32, padding_height = 1 : i32, padding_width = 1 : i32, stride_height = 1 : i32, stride_width = 1 : i32}> : (tensor<1x28x28x128xf32, #layout50>, tensor<128x128x3x3xf32, #layout13>, tensor<1x28x28x128xf32, #layout50>, !tt.device<#device>) -> tensor<1x28x28x128xf32, #layout50> loc(#loc630)
    %463 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x28x128x28>}> : (!tt.device<#device>) -> tensor<1x28x128x28xf32, #layout51> loc(#loc631)
    %464 = "ttnn.transpose"(%462, %463) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x28x28x128xf32, #layout50>, tensor<1x28x128x28xf32, #layout51>) -> tensor<1x28x128x28xf32, #layout51> loc(#loc631)
    %465 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x128x28x28>}> : (!tt.device<#device>) -> tensor<1x128x28x28xf32, #layout52> loc(#loc632)
    %466 = "ttnn.transpose"(%464, %465) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x28x128x28xf32, #layout51>, tensor<1x128x28x28xf32, #layout52>) -> tensor<1x128x28x28xf32, #layout52> loc(#loc632)
    %467 = "ttnn.to_layout"(%arg45, %0) <{layout = #ttnn.layout<tile>}> : (tensor<128x1x1xf32, #layout3>, !tt.device<#device>) -> tensor<128x1x1xf32, #layout49> loc(#loc633)
    %468 = "ttnn.to_device"(%467, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<128x1x1xf32, #layout49>, !tt.device<#device>) -> tensor<128x1x1xf32, #layout49> loc(#loc633)
    %469 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x128x28x28>}> : (!tt.device<#device>) -> tensor<1x128x28x28xf32, #layout52> loc(#loc633)
    %470 = "ttnn.multiply"(%466, %468, %469) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128x28x28xf32, #layout52>, tensor<128x1x1xf32, #layout49>, tensor<1x128x28x28xf32, #layout52>) -> tensor<1x128x28x28xf32, #layout52> loc(#loc633)
    %471 = "ttnn.to_layout"(%arg46, %0) <{layout = #ttnn.layout<tile>}> : (tensor<128x1x1xf32, #layout3>, !tt.device<#device>) -> tensor<128x1x1xf32, #layout49> loc(#loc634)
    %472 = "ttnn.to_device"(%471, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<128x1x1xf32, #layout49>, !tt.device<#device>) -> tensor<128x1x1xf32, #layout49> loc(#loc634)
    %473 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x128x28x28>}> : (!tt.device<#device>) -> tensor<1x128x28x28xf32, #layout52> loc(#loc634)
    %474 = "ttnn.add"(%470, %472, %473) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x128x28x28xf32, #layout52>, tensor<128x1x1xf32, #layout49>, tensor<1x128x28x28xf32, #layout52>) -> tensor<1x128x28x28xf32, #layout52> loc(#loc634)
    %475 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x128x28x28>}> : (!tt.device<#device>) -> tensor<1x128x28x28xf32, #layout52> loc(#loc635)
    %476 = "ttnn.relu"(%474, %475) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x128x28x28xf32, #layout52>, tensor<1x128x28x28xf32, #layout52>) -> tensor<1x128x28x28xf32, #layout52> loc(#loc635)
    %477 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x28x128x28>}> : (!tt.device<#device>) -> tensor<1x28x128x28xf32, #layout51> loc(#loc636)
    %478 = "ttnn.transpose"(%476, %477) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x128x28x28xf32, #layout52>, tensor<1x28x128x28xf32, #layout51>) -> tensor<1x28x128x28xf32, #layout51> loc(#loc636)
    %479 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x28x28x128>}> : (!tt.device<#device>) -> tensor<1x28x28x128xf32, #layout50> loc(#loc637)
    %480 = "ttnn.transpose"(%478, %479) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x28x128x28xf32, #layout51>, tensor<1x28x28x128xf32, #layout50>) -> tensor<1x28x28x128xf32, #layout50> loc(#loc637)
    %481 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x28x28x512>}> : (!tt.device<#device>) -> tensor<1x28x28x512xf32, #layout53> loc(#loc638)
    %482 = "ttnn.conv2d"(%480, %arg130, %481, %0) <{batch_size = 1 : i32, dilation_height = 1 : i32, dilation_width = 1 : i32, groups = 1 : i32, in_channels = 128 : i32, input_height = 28 : i32, input_width = 28 : i32, kernel_height = 1 : i32, kernel_width = 1 : i32, out_channels = 512 : i32, padding_height = 0 : i32, padding_width = 0 : i32, stride_height = 1 : i32, stride_width = 1 : i32}> : (tensor<1x28x28x128xf32, #layout50>, tensor<512x128x1x1xf32, #layout14>, tensor<1x28x28x512xf32, #layout53>, !tt.device<#device>) -> tensor<1x28x28x512xf32, #layout53> loc(#loc638)
    %483 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x28x512x28>}> : (!tt.device<#device>) -> tensor<1x28x512x28xf32, #layout54> loc(#loc639)
    %484 = "ttnn.transpose"(%482, %483) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x28x28x512xf32, #layout53>, tensor<1x28x512x28xf32, #layout54>) -> tensor<1x28x512x28xf32, #layout54> loc(#loc639)
    %485 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x512x28x28>}> : (!tt.device<#device>) -> tensor<1x512x28x28xf32, #layout55> loc(#loc640)
    %486 = "ttnn.transpose"(%484, %485) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x28x512x28xf32, #layout54>, tensor<1x512x28x28xf32, #layout55>) -> tensor<1x512x28x28xf32, #layout55> loc(#loc640)
    %487 = "ttnn.to_layout"(%arg47, %0) <{layout = #ttnn.layout<tile>}> : (tensor<512x1x1xf32, #layout4>, !tt.device<#device>) -> tensor<512x1x1xf32, #layout56> loc(#loc641)
    %488 = "ttnn.to_device"(%487, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<512x1x1xf32, #layout56>, !tt.device<#device>) -> tensor<512x1x1xf32, #layout56> loc(#loc641)
    %489 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x512x28x28>}> : (!tt.device<#device>) -> tensor<1x512x28x28xf32, #layout55> loc(#loc641)
    %490 = "ttnn.multiply"(%486, %488, %489) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x512x28x28xf32, #layout55>, tensor<512x1x1xf32, #layout56>, tensor<1x512x28x28xf32, #layout55>) -> tensor<1x512x28x28xf32, #layout55> loc(#loc641)
    %491 = "ttnn.to_layout"(%arg48, %0) <{layout = #ttnn.layout<tile>}> : (tensor<512x1x1xf32, #layout4>, !tt.device<#device>) -> tensor<512x1x1xf32, #layout56> loc(#loc642)
    %492 = "ttnn.to_device"(%491, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<512x1x1xf32, #layout56>, !tt.device<#device>) -> tensor<512x1x1xf32, #layout56> loc(#loc642)
    %493 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x512x28x28>}> : (!tt.device<#device>) -> tensor<1x512x28x28xf32, #layout55> loc(#loc642)
    %494 = "ttnn.add"(%490, %492, %493) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x512x28x28xf32, #layout55>, tensor<512x1x1xf32, #layout56>, tensor<1x512x28x28xf32, #layout55>) -> tensor<1x512x28x28xf32, #layout55> loc(#loc642)
    %495 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x512x28x28>}> : (!tt.device<#device>) -> tensor<1x512x28x28xf32, #layout55> loc(#loc643)
    %496 = "ttnn.add"(%494, %436, %495) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x512x28x28xf32, #layout55>, tensor<1x512x28x28xf32, #layout55>, tensor<1x512x28x28xf32, #layout55>) -> tensor<1x512x28x28xf32, #layout55> loc(#loc643)
    %497 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x512x28x28>}> : (!tt.device<#device>) -> tensor<1x512x28x28xf32, #layout55> loc(#loc644)
    %498 = "ttnn.relu"(%496, %497) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x512x28x28xf32, #layout55>, tensor<1x512x28x28xf32, #layout55>) -> tensor<1x512x28x28xf32, #layout55> loc(#loc644)
    %499 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x28x512x28>}> : (!tt.device<#device>) -> tensor<1x28x512x28xf32, #layout54> loc(#loc645)
    %500 = "ttnn.transpose"(%498, %499) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x512x28x28xf32, #layout55>, tensor<1x28x512x28xf32, #layout54>) -> tensor<1x28x512x28xf32, #layout54> loc(#loc645)
    %501 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x28x28x512>}> : (!tt.device<#device>) -> tensor<1x28x28x512xf32, #layout53> loc(#loc646)
    %502 = "ttnn.transpose"(%500, %501) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x28x512x28xf32, #layout54>, tensor<1x28x28x512xf32, #layout53>) -> tensor<1x28x28x512xf32, #layout53> loc(#loc646)
    %503 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x28x28x256>}> : (!tt.device<#device>) -> tensor<1x28x28x256xf32, #layout57> loc(#loc647)
    %504 = "ttnn.conv2d"(%502, %arg131, %503, %0) <{batch_size = 1 : i32, dilation_height = 1 : i32, dilation_width = 1 : i32, groups = 1 : i32, in_channels = 512 : i32, input_height = 28 : i32, input_width = 28 : i32, kernel_height = 1 : i32, kernel_width = 1 : i32, out_channels = 256 : i32, padding_height = 0 : i32, padding_width = 0 : i32, stride_height = 1 : i32, stride_width = 1 : i32}> : (tensor<1x28x28x512xf32, #layout53>, tensor<256x512x1x1xf32, #layout17>, tensor<1x28x28x256xf32, #layout57>, !tt.device<#device>) -> tensor<1x28x28x256xf32, #layout57> loc(#loc647)
    %505 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x28x256x28>}> : (!tt.device<#device>) -> tensor<1x28x256x28xf32, #layout58> loc(#loc648)
    %506 = "ttnn.transpose"(%504, %505) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x28x28x256xf32, #layout57>, tensor<1x28x256x28xf32, #layout58>) -> tensor<1x28x256x28xf32, #layout58> loc(#loc648)
    %507 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x256x28x28>}> : (!tt.device<#device>) -> tensor<1x256x28x28xf32, #layout59> loc(#loc649)
    %508 = "ttnn.transpose"(%506, %507) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x28x256x28xf32, #layout58>, tensor<1x256x28x28xf32, #layout59>) -> tensor<1x256x28x28xf32, #layout59> loc(#loc649)
    %509 = "ttnn.to_layout"(%arg49, %0) <{layout = #ttnn.layout<tile>}> : (tensor<256x1x1xf32, #layout2>, !tt.device<#device>) -> tensor<256x1x1xf32, #layout45> loc(#loc650)
    %510 = "ttnn.to_device"(%509, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<256x1x1xf32, #layout45>, !tt.device<#device>) -> tensor<256x1x1xf32, #layout45> loc(#loc650)
    %511 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x256x28x28>}> : (!tt.device<#device>) -> tensor<1x256x28x28xf32, #layout59> loc(#loc650)
    %512 = "ttnn.multiply"(%508, %510, %511) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x256x28x28xf32, #layout59>, tensor<256x1x1xf32, #layout45>, tensor<1x256x28x28xf32, #layout59>) -> tensor<1x256x28x28xf32, #layout59> loc(#loc650)
    %513 = "ttnn.to_layout"(%arg50, %0) <{layout = #ttnn.layout<tile>}> : (tensor<256x1x1xf32, #layout2>, !tt.device<#device>) -> tensor<256x1x1xf32, #layout45> loc(#loc651)
    %514 = "ttnn.to_device"(%513, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<256x1x1xf32, #layout45>, !tt.device<#device>) -> tensor<256x1x1xf32, #layout45> loc(#loc651)
    %515 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x256x28x28>}> : (!tt.device<#device>) -> tensor<1x256x28x28xf32, #layout59> loc(#loc651)
    %516 = "ttnn.add"(%512, %514, %515) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x256x28x28xf32, #layout59>, tensor<256x1x1xf32, #layout45>, tensor<1x256x28x28xf32, #layout59>) -> tensor<1x256x28x28xf32, #layout59> loc(#loc651)
    %517 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x256x28x28>}> : (!tt.device<#device>) -> tensor<1x256x28x28xf32, #layout59> loc(#loc652)
    %518 = "ttnn.relu"(%516, %517) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x256x28x28xf32, #layout59>, tensor<1x256x28x28xf32, #layout59>) -> tensor<1x256x28x28xf32, #layout59> loc(#loc652)
    %519 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x28x256x28>}> : (!tt.device<#device>) -> tensor<1x28x256x28xf32, #layout58> loc(#loc653)
    %520 = "ttnn.transpose"(%518, %519) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x256x28x28xf32, #layout59>, tensor<1x28x256x28xf32, #layout58>) -> tensor<1x28x256x28xf32, #layout58> loc(#loc653)
    %521 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x28x28x256>}> : (!tt.device<#device>) -> tensor<1x28x28x256xf32, #layout57> loc(#loc654)
    %522 = "ttnn.transpose"(%520, %521) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x28x256x28xf32, #layout58>, tensor<1x28x28x256xf32, #layout57>) -> tensor<1x28x28x256xf32, #layout57> loc(#loc654)
    %523 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x14x14x256>}> : (!tt.device<#device>) -> tensor<1x14x14x256xf32, #layout60> loc(#loc655)
    %524 = "ttnn.conv2d"(%522, %arg132, %523, %0) <{batch_size = 1 : i32, dilation_height = 1 : i32, dilation_width = 1 : i32, groups = 1 : i32, in_channels = 256 : i32, input_height = 28 : i32, input_width = 28 : i32, kernel_height = 3 : i32, kernel_width = 3 : i32, out_channels = 256 : i32, padding_height = 1 : i32, padding_width = 1 : i32, stride_height = 2 : i32, stride_width = 2 : i32}> : (tensor<1x28x28x256xf32, #layout57>, tensor<256x256x3x3xf32, #layout18>, tensor<1x14x14x256xf32, #layout60>, !tt.device<#device>) -> tensor<1x14x14x256xf32, #layout60> loc(#loc655)
    %525 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x14x256x14>}> : (!tt.device<#device>) -> tensor<1x14x256x14xf32, #layout61> loc(#loc656)
    %526 = "ttnn.transpose"(%524, %525) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x14x14x256xf32, #layout60>, tensor<1x14x256x14xf32, #layout61>) -> tensor<1x14x256x14xf32, #layout61> loc(#loc656)
    %527 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x256x14x14>}> : (!tt.device<#device>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc657)
    %528 = "ttnn.transpose"(%526, %527) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x14x256x14xf32, #layout61>, tensor<1x256x14x14xf32, #layout62>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc657)
    %529 = "ttnn.to_layout"(%arg51, %0) <{layout = #ttnn.layout<tile>}> : (tensor<256x1x1xf32, #layout2>, !tt.device<#device>) -> tensor<256x1x1xf32, #layout45> loc(#loc658)
    %530 = "ttnn.to_device"(%529, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<256x1x1xf32, #layout45>, !tt.device<#device>) -> tensor<256x1x1xf32, #layout45> loc(#loc658)
    %531 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x256x14x14>}> : (!tt.device<#device>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc658)
    %532 = "ttnn.multiply"(%528, %530, %531) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x256x14x14xf32, #layout62>, tensor<256x1x1xf32, #layout45>, tensor<1x256x14x14xf32, #layout62>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc658)
    %533 = "ttnn.to_layout"(%arg52, %0) <{layout = #ttnn.layout<tile>}> : (tensor<256x1x1xf32, #layout2>, !tt.device<#device>) -> tensor<256x1x1xf32, #layout45> loc(#loc659)
    %534 = "ttnn.to_device"(%533, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<256x1x1xf32, #layout45>, !tt.device<#device>) -> tensor<256x1x1xf32, #layout45> loc(#loc659)
    %535 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x256x14x14>}> : (!tt.device<#device>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc659)
    %536 = "ttnn.add"(%532, %534, %535) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x256x14x14xf32, #layout62>, tensor<256x1x1xf32, #layout45>, tensor<1x256x14x14xf32, #layout62>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc659)
    %537 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x256x14x14>}> : (!tt.device<#device>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc660)
    %538 = "ttnn.relu"(%536, %537) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x256x14x14xf32, #layout62>, tensor<1x256x14x14xf32, #layout62>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc660)
    %539 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x14x256x14>}> : (!tt.device<#device>) -> tensor<1x14x256x14xf32, #layout61> loc(#loc661)
    %540 = "ttnn.transpose"(%538, %539) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x256x14x14xf32, #layout62>, tensor<1x14x256x14xf32, #layout61>) -> tensor<1x14x256x14xf32, #layout61> loc(#loc661)
    %541 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x14x14x256>}> : (!tt.device<#device>) -> tensor<1x14x14x256xf32, #layout60> loc(#loc662)
    %542 = "ttnn.transpose"(%540, %541) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x14x256x14xf32, #layout61>, tensor<1x14x14x256xf32, #layout60>) -> tensor<1x14x14x256xf32, #layout60> loc(#loc662)
    %543 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x14x14x1024>}> : (!tt.device<#device>) -> tensor<1x14x14x1024xf32, #layout63> loc(#loc663)
    %544 = "ttnn.conv2d"(%542, %arg133, %543, %0) <{batch_size = 1 : i32, dilation_height = 1 : i32, dilation_width = 1 : i32, groups = 1 : i32, in_channels = 256 : i32, input_height = 14 : i32, input_width = 14 : i32, kernel_height = 1 : i32, kernel_width = 1 : i32, out_channels = 1024 : i32, padding_height = 0 : i32, padding_width = 0 : i32, stride_height = 1 : i32, stride_width = 1 : i32}> : (tensor<1x14x14x256xf32, #layout60>, tensor<1024x256x1x1xf32, #layout19>, tensor<1x14x14x1024xf32, #layout63>, !tt.device<#device>) -> tensor<1x14x14x1024xf32, #layout63> loc(#loc663)
    %545 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x14x1024x14>}> : (!tt.device<#device>) -> tensor<1x14x1024x14xf32, #layout64> loc(#loc664)
    %546 = "ttnn.transpose"(%544, %545) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x14x14x1024xf32, #layout63>, tensor<1x14x1024x14xf32, #layout64>) -> tensor<1x14x1024x14xf32, #layout64> loc(#loc664)
    %547 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x1024x14x14>}> : (!tt.device<#device>) -> tensor<1x1024x14x14xf32, #layout65> loc(#loc665)
    %548 = "ttnn.transpose"(%546, %547) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x14x1024x14xf32, #layout64>, tensor<1x1024x14x14xf32, #layout65>) -> tensor<1x1024x14x14xf32, #layout65> loc(#loc665)
    %549 = "ttnn.to_layout"(%arg53, %0) <{layout = #ttnn.layout<tile>}> : (tensor<1024x1x1xf32, #layout5>, !tt.device<#device>) -> tensor<1024x1x1xf32, #layout66> loc(#loc666)
    %550 = "ttnn.to_device"(%549, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<1024x1x1xf32, #layout66>, !tt.device<#device>) -> tensor<1024x1x1xf32, #layout66> loc(#loc666)
    %551 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x1024x14x14>}> : (!tt.device<#device>) -> tensor<1x1024x14x14xf32, #layout65> loc(#loc666)
    %552 = "ttnn.multiply"(%548, %550, %551) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x1024x14x14xf32, #layout65>, tensor<1024x1x1xf32, #layout66>, tensor<1x1024x14x14xf32, #layout65>) -> tensor<1x1024x14x14xf32, #layout65> loc(#loc666)
    %553 = "ttnn.to_layout"(%arg54, %0) <{layout = #ttnn.layout<tile>}> : (tensor<1024x1x1xf32, #layout5>, !tt.device<#device>) -> tensor<1024x1x1xf32, #layout66> loc(#loc667)
    %554 = "ttnn.to_device"(%553, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<1024x1x1xf32, #layout66>, !tt.device<#device>) -> tensor<1024x1x1xf32, #layout66> loc(#loc667)
    %555 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x1024x14x14>}> : (!tt.device<#device>) -> tensor<1x1024x14x14xf32, #layout65> loc(#loc667)
    %556 = "ttnn.add"(%552, %554, %555) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x1024x14x14xf32, #layout65>, tensor<1024x1x1xf32, #layout66>, tensor<1x1024x14x14xf32, #layout65>) -> tensor<1x1024x14x14xf32, #layout65> loc(#loc667)
    %557 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x28x512x28>}> : (!tt.device<#device>) -> tensor<1x28x512x28xf32, #layout54> loc(#loc668)
    %558 = "ttnn.transpose"(%498, %557) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x512x28x28xf32, #layout55>, tensor<1x28x512x28xf32, #layout54>) -> tensor<1x28x512x28xf32, #layout54> loc(#loc668)
    %559 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x28x28x512>}> : (!tt.device<#device>) -> tensor<1x28x28x512xf32, #layout53> loc(#loc669)
    %560 = "ttnn.transpose"(%558, %559) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x28x512x28xf32, #layout54>, tensor<1x28x28x512xf32, #layout53>) -> tensor<1x28x28x512xf32, #layout53> loc(#loc669)
    %561 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x14x14x1024>}> : (!tt.device<#device>) -> tensor<1x14x14x1024xf32, #layout63> loc(#loc670)
    %562 = "ttnn.conv2d"(%560, %arg134, %561, %0) <{batch_size = 1 : i32, dilation_height = 1 : i32, dilation_width = 1 : i32, groups = 1 : i32, in_channels = 512 : i32, input_height = 28 : i32, input_width = 28 : i32, kernel_height = 1 : i32, kernel_width = 1 : i32, out_channels = 1024 : i32, padding_height = 0 : i32, padding_width = 0 : i32, stride_height = 2 : i32, stride_width = 2 : i32}> : (tensor<1x28x28x512xf32, #layout53>, tensor<1024x512x1x1xf32, #layout20>, tensor<1x14x14x1024xf32, #layout63>, !tt.device<#device>) -> tensor<1x14x14x1024xf32, #layout63> loc(#loc670)
    %563 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x14x1024x14>}> : (!tt.device<#device>) -> tensor<1x14x1024x14xf32, #layout64> loc(#loc671)
    %564 = "ttnn.transpose"(%562, %563) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x14x14x1024xf32, #layout63>, tensor<1x14x1024x14xf32, #layout64>) -> tensor<1x14x1024x14xf32, #layout64> loc(#loc671)
    %565 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x1024x14x14>}> : (!tt.device<#device>) -> tensor<1x1024x14x14xf32, #layout65> loc(#loc672)
    %566 = "ttnn.transpose"(%564, %565) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x14x1024x14xf32, #layout64>, tensor<1x1024x14x14xf32, #layout65>) -> tensor<1x1024x14x14xf32, #layout65> loc(#loc672)
    %567 = "ttnn.to_layout"(%arg55, %0) <{layout = #ttnn.layout<tile>}> : (tensor<1024x1x1xf32, #layout5>, !tt.device<#device>) -> tensor<1024x1x1xf32, #layout66> loc(#loc673)
    %568 = "ttnn.to_device"(%567, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<1024x1x1xf32, #layout66>, !tt.device<#device>) -> tensor<1024x1x1xf32, #layout66> loc(#loc673)
    %569 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x1024x14x14>}> : (!tt.device<#device>) -> tensor<1x1024x14x14xf32, #layout65> loc(#loc673)
    %570 = "ttnn.multiply"(%566, %568, %569) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x1024x14x14xf32, #layout65>, tensor<1024x1x1xf32, #layout66>, tensor<1x1024x14x14xf32, #layout65>) -> tensor<1x1024x14x14xf32, #layout65> loc(#loc673)
    %571 = "ttnn.to_layout"(%arg56, %0) <{layout = #ttnn.layout<tile>}> : (tensor<1024x1x1xf32, #layout5>, !tt.device<#device>) -> tensor<1024x1x1xf32, #layout66> loc(#loc674)
    %572 = "ttnn.to_device"(%571, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<1024x1x1xf32, #layout66>, !tt.device<#device>) -> tensor<1024x1x1xf32, #layout66> loc(#loc674)
    %573 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x1024x14x14>}> : (!tt.device<#device>) -> tensor<1x1024x14x14xf32, #layout65> loc(#loc674)
    %574 = "ttnn.add"(%570, %572, %573) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x1024x14x14xf32, #layout65>, tensor<1024x1x1xf32, #layout66>, tensor<1x1024x14x14xf32, #layout65>) -> tensor<1x1024x14x14xf32, #layout65> loc(#loc674)
    %575 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x1024x14x14>}> : (!tt.device<#device>) -> tensor<1x1024x14x14xf32, #layout65> loc(#loc675)
    %576 = "ttnn.add"(%556, %574, %575) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x1024x14x14xf32, #layout65>, tensor<1x1024x14x14xf32, #layout65>, tensor<1x1024x14x14xf32, #layout65>) -> tensor<1x1024x14x14xf32, #layout65> loc(#loc675)
    %577 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x1024x14x14>}> : (!tt.device<#device>) -> tensor<1x1024x14x14xf32, #layout65> loc(#loc676)
    %578 = "ttnn.relu"(%576, %577) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x1024x14x14xf32, #layout65>, tensor<1x1024x14x14xf32, #layout65>) -> tensor<1x1024x14x14xf32, #layout65> loc(#loc676)
    %579 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x14x1024x14>}> : (!tt.device<#device>) -> tensor<1x14x1024x14xf32, #layout64> loc(#loc677)
    %580 = "ttnn.transpose"(%578, %579) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x1024x14x14xf32, #layout65>, tensor<1x14x1024x14xf32, #layout64>) -> tensor<1x14x1024x14xf32, #layout64> loc(#loc677)
    %581 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x14x14x1024>}> : (!tt.device<#device>) -> tensor<1x14x14x1024xf32, #layout63> loc(#loc678)
    %582 = "ttnn.transpose"(%580, %581) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x14x1024x14xf32, #layout64>, tensor<1x14x14x1024xf32, #layout63>) -> tensor<1x14x14x1024xf32, #layout63> loc(#loc678)
    %583 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x14x14x256>}> : (!tt.device<#device>) -> tensor<1x14x14x256xf32, #layout60> loc(#loc679)
    %584 = "ttnn.conv2d"(%582, %arg135, %583, %0) <{batch_size = 1 : i32, dilation_height = 1 : i32, dilation_width = 1 : i32, groups = 1 : i32, in_channels = 1024 : i32, input_height = 14 : i32, input_width = 14 : i32, kernel_height = 1 : i32, kernel_width = 1 : i32, out_channels = 256 : i32, padding_height = 0 : i32, padding_width = 0 : i32, stride_height = 1 : i32, stride_width = 1 : i32}> : (tensor<1x14x14x1024xf32, #layout63>, tensor<256x1024x1x1xf32, #layout21>, tensor<1x14x14x256xf32, #layout60>, !tt.device<#device>) -> tensor<1x14x14x256xf32, #layout60> loc(#loc679)
    %585 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x14x256x14>}> : (!tt.device<#device>) -> tensor<1x14x256x14xf32, #layout61> loc(#loc680)
    %586 = "ttnn.transpose"(%584, %585) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x14x14x256xf32, #layout60>, tensor<1x14x256x14xf32, #layout61>) -> tensor<1x14x256x14xf32, #layout61> loc(#loc680)
    %587 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x256x14x14>}> : (!tt.device<#device>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc681)
    %588 = "ttnn.transpose"(%586, %587) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x14x256x14xf32, #layout61>, tensor<1x256x14x14xf32, #layout62>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc681)
    %589 = "ttnn.to_layout"(%arg57, %0) <{layout = #ttnn.layout<tile>}> : (tensor<256x1x1xf32, #layout2>, !tt.device<#device>) -> tensor<256x1x1xf32, #layout45> loc(#loc682)
    %590 = "ttnn.to_device"(%589, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<256x1x1xf32, #layout45>, !tt.device<#device>) -> tensor<256x1x1xf32, #layout45> loc(#loc682)
    %591 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x256x14x14>}> : (!tt.device<#device>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc682)
    %592 = "ttnn.multiply"(%588, %590, %591) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x256x14x14xf32, #layout62>, tensor<256x1x1xf32, #layout45>, tensor<1x256x14x14xf32, #layout62>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc682)
    %593 = "ttnn.to_layout"(%arg58, %0) <{layout = #ttnn.layout<tile>}> : (tensor<256x1x1xf32, #layout2>, !tt.device<#device>) -> tensor<256x1x1xf32, #layout45> loc(#loc683)
    %594 = "ttnn.to_device"(%593, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<256x1x1xf32, #layout45>, !tt.device<#device>) -> tensor<256x1x1xf32, #layout45> loc(#loc683)
    %595 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x256x14x14>}> : (!tt.device<#device>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc683)
    %596 = "ttnn.add"(%592, %594, %595) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x256x14x14xf32, #layout62>, tensor<256x1x1xf32, #layout45>, tensor<1x256x14x14xf32, #layout62>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc683)
    %597 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x256x14x14>}> : (!tt.device<#device>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc684)
    %598 = "ttnn.relu"(%596, %597) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x256x14x14xf32, #layout62>, tensor<1x256x14x14xf32, #layout62>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc684)
    %599 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x14x256x14>}> : (!tt.device<#device>) -> tensor<1x14x256x14xf32, #layout61> loc(#loc685)
    %600 = "ttnn.transpose"(%598, %599) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x256x14x14xf32, #layout62>, tensor<1x14x256x14xf32, #layout61>) -> tensor<1x14x256x14xf32, #layout61> loc(#loc685)
    %601 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x14x14x256>}> : (!tt.device<#device>) -> tensor<1x14x14x256xf32, #layout60> loc(#loc686)
    %602 = "ttnn.transpose"(%600, %601) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x14x256x14xf32, #layout61>, tensor<1x14x14x256xf32, #layout60>) -> tensor<1x14x14x256xf32, #layout60> loc(#loc686)
    %603 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x14x14x256>}> : (!tt.device<#device>) -> tensor<1x14x14x256xf32, #layout60> loc(#loc687)
    %604 = "ttnn.conv2d"(%602, %arg136, %603, %0) <{batch_size = 1 : i32, dilation_height = 1 : i32, dilation_width = 1 : i32, groups = 1 : i32, in_channels = 256 : i32, input_height = 14 : i32, input_width = 14 : i32, kernel_height = 3 : i32, kernel_width = 3 : i32, out_channels = 256 : i32, padding_height = 1 : i32, padding_width = 1 : i32, stride_height = 1 : i32, stride_width = 1 : i32}> : (tensor<1x14x14x256xf32, #layout60>, tensor<256x256x3x3xf32, #layout18>, tensor<1x14x14x256xf32, #layout60>, !tt.device<#device>) -> tensor<1x14x14x256xf32, #layout60> loc(#loc687)
    %605 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x14x256x14>}> : (!tt.device<#device>) -> tensor<1x14x256x14xf32, #layout61> loc(#loc688)
    %606 = "ttnn.transpose"(%604, %605) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x14x14x256xf32, #layout60>, tensor<1x14x256x14xf32, #layout61>) -> tensor<1x14x256x14xf32, #layout61> loc(#loc688)
    %607 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x256x14x14>}> : (!tt.device<#device>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc689)
    %608 = "ttnn.transpose"(%606, %607) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x14x256x14xf32, #layout61>, tensor<1x256x14x14xf32, #layout62>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc689)
    %609 = "ttnn.to_layout"(%arg59, %0) <{layout = #ttnn.layout<tile>}> : (tensor<256x1x1xf32, #layout2>, !tt.device<#device>) -> tensor<256x1x1xf32, #layout45> loc(#loc690)
    %610 = "ttnn.to_device"(%609, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<256x1x1xf32, #layout45>, !tt.device<#device>) -> tensor<256x1x1xf32, #layout45> loc(#loc690)
    %611 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x256x14x14>}> : (!tt.device<#device>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc690)
    %612 = "ttnn.multiply"(%608, %610, %611) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x256x14x14xf32, #layout62>, tensor<256x1x1xf32, #layout45>, tensor<1x256x14x14xf32, #layout62>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc690)
    %613 = "ttnn.to_layout"(%arg60, %0) <{layout = #ttnn.layout<tile>}> : (tensor<256x1x1xf32, #layout2>, !tt.device<#device>) -> tensor<256x1x1xf32, #layout45> loc(#loc691)
    %614 = "ttnn.to_device"(%613, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<256x1x1xf32, #layout45>, !tt.device<#device>) -> tensor<256x1x1xf32, #layout45> loc(#loc691)
    %615 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x256x14x14>}> : (!tt.device<#device>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc691)
    %616 = "ttnn.add"(%612, %614, %615) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x256x14x14xf32, #layout62>, tensor<256x1x1xf32, #layout45>, tensor<1x256x14x14xf32, #layout62>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc691)
    %617 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x256x14x14>}> : (!tt.device<#device>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc692)
    %618 = "ttnn.relu"(%616, %617) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x256x14x14xf32, #layout62>, tensor<1x256x14x14xf32, #layout62>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc692)
    %619 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x14x256x14>}> : (!tt.device<#device>) -> tensor<1x14x256x14xf32, #layout61> loc(#loc693)
    %620 = "ttnn.transpose"(%618, %619) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x256x14x14xf32, #layout62>, tensor<1x14x256x14xf32, #layout61>) -> tensor<1x14x256x14xf32, #layout61> loc(#loc693)
    %621 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x14x14x256>}> : (!tt.device<#device>) -> tensor<1x14x14x256xf32, #layout60> loc(#loc694)
    %622 = "ttnn.transpose"(%620, %621) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x14x256x14xf32, #layout61>, tensor<1x14x14x256xf32, #layout60>) -> tensor<1x14x14x256xf32, #layout60> loc(#loc694)
    %623 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x14x14x1024>}> : (!tt.device<#device>) -> tensor<1x14x14x1024xf32, #layout63> loc(#loc695)
    %624 = "ttnn.conv2d"(%622, %arg137, %623, %0) <{batch_size = 1 : i32, dilation_height = 1 : i32, dilation_width = 1 : i32, groups = 1 : i32, in_channels = 256 : i32, input_height = 14 : i32, input_width = 14 : i32, kernel_height = 1 : i32, kernel_width = 1 : i32, out_channels = 1024 : i32, padding_height = 0 : i32, padding_width = 0 : i32, stride_height = 1 : i32, stride_width = 1 : i32}> : (tensor<1x14x14x256xf32, #layout60>, tensor<1024x256x1x1xf32, #layout19>, tensor<1x14x14x1024xf32, #layout63>, !tt.device<#device>) -> tensor<1x14x14x1024xf32, #layout63> loc(#loc695)
    %625 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x14x1024x14>}> : (!tt.device<#device>) -> tensor<1x14x1024x14xf32, #layout64> loc(#loc696)
    %626 = "ttnn.transpose"(%624, %625) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x14x14x1024xf32, #layout63>, tensor<1x14x1024x14xf32, #layout64>) -> tensor<1x14x1024x14xf32, #layout64> loc(#loc696)
    %627 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x1024x14x14>}> : (!tt.device<#device>) -> tensor<1x1024x14x14xf32, #layout65> loc(#loc697)
    %628 = "ttnn.transpose"(%626, %627) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x14x1024x14xf32, #layout64>, tensor<1x1024x14x14xf32, #layout65>) -> tensor<1x1024x14x14xf32, #layout65> loc(#loc697)
    %629 = "ttnn.to_layout"(%arg61, %0) <{layout = #ttnn.layout<tile>}> : (tensor<1024x1x1xf32, #layout5>, !tt.device<#device>) -> tensor<1024x1x1xf32, #layout66> loc(#loc698)
    %630 = "ttnn.to_device"(%629, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<1024x1x1xf32, #layout66>, !tt.device<#device>) -> tensor<1024x1x1xf32, #layout66> loc(#loc698)
    %631 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x1024x14x14>}> : (!tt.device<#device>) -> tensor<1x1024x14x14xf32, #layout65> loc(#loc698)
    %632 = "ttnn.multiply"(%628, %630, %631) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x1024x14x14xf32, #layout65>, tensor<1024x1x1xf32, #layout66>, tensor<1x1024x14x14xf32, #layout65>) -> tensor<1x1024x14x14xf32, #layout65> loc(#loc698)
    %633 = "ttnn.to_layout"(%arg62, %0) <{layout = #ttnn.layout<tile>}> : (tensor<1024x1x1xf32, #layout5>, !tt.device<#device>) -> tensor<1024x1x1xf32, #layout66> loc(#loc699)
    %634 = "ttnn.to_device"(%633, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<1024x1x1xf32, #layout66>, !tt.device<#device>) -> tensor<1024x1x1xf32, #layout66> loc(#loc699)
    %635 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x1024x14x14>}> : (!tt.device<#device>) -> tensor<1x1024x14x14xf32, #layout65> loc(#loc699)
    %636 = "ttnn.add"(%632, %634, %635) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x1024x14x14xf32, #layout65>, tensor<1024x1x1xf32, #layout66>, tensor<1x1024x14x14xf32, #layout65>) -> tensor<1x1024x14x14xf32, #layout65> loc(#loc699)
    %637 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x1024x14x14>}> : (!tt.device<#device>) -> tensor<1x1024x14x14xf32, #layout65> loc(#loc700)
    %638 = "ttnn.add"(%636, %578, %637) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x1024x14x14xf32, #layout65>, tensor<1x1024x14x14xf32, #layout65>, tensor<1x1024x14x14xf32, #layout65>) -> tensor<1x1024x14x14xf32, #layout65> loc(#loc700)
    %639 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x1024x14x14>}> : (!tt.device<#device>) -> tensor<1x1024x14x14xf32, #layout65> loc(#loc701)
    %640 = "ttnn.relu"(%638, %639) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x1024x14x14xf32, #layout65>, tensor<1x1024x14x14xf32, #layout65>) -> tensor<1x1024x14x14xf32, #layout65> loc(#loc701)
    %641 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x14x1024x14>}> : (!tt.device<#device>) -> tensor<1x14x1024x14xf32, #layout64> loc(#loc702)
    %642 = "ttnn.transpose"(%640, %641) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x1024x14x14xf32, #layout65>, tensor<1x14x1024x14xf32, #layout64>) -> tensor<1x14x1024x14xf32, #layout64> loc(#loc702)
    %643 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x14x14x1024>}> : (!tt.device<#device>) -> tensor<1x14x14x1024xf32, #layout63> loc(#loc703)
    %644 = "ttnn.transpose"(%642, %643) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x14x1024x14xf32, #layout64>, tensor<1x14x14x1024xf32, #layout63>) -> tensor<1x14x14x1024xf32, #layout63> loc(#loc703)
    %645 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x14x14x256>}> : (!tt.device<#device>) -> tensor<1x14x14x256xf32, #layout60> loc(#loc704)
    %646 = "ttnn.conv2d"(%644, %arg138, %645, %0) <{batch_size = 1 : i32, dilation_height = 1 : i32, dilation_width = 1 : i32, groups = 1 : i32, in_channels = 1024 : i32, input_height = 14 : i32, input_width = 14 : i32, kernel_height = 1 : i32, kernel_width = 1 : i32, out_channels = 256 : i32, padding_height = 0 : i32, padding_width = 0 : i32, stride_height = 1 : i32, stride_width = 1 : i32}> : (tensor<1x14x14x1024xf32, #layout63>, tensor<256x1024x1x1xf32, #layout21>, tensor<1x14x14x256xf32, #layout60>, !tt.device<#device>) -> tensor<1x14x14x256xf32, #layout60> loc(#loc704)
    %647 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x14x256x14>}> : (!tt.device<#device>) -> tensor<1x14x256x14xf32, #layout61> loc(#loc705)
    %648 = "ttnn.transpose"(%646, %647) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x14x14x256xf32, #layout60>, tensor<1x14x256x14xf32, #layout61>) -> tensor<1x14x256x14xf32, #layout61> loc(#loc705)
    %649 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x256x14x14>}> : (!tt.device<#device>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc706)
    %650 = "ttnn.transpose"(%648, %649) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x14x256x14xf32, #layout61>, tensor<1x256x14x14xf32, #layout62>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc706)
    %651 = "ttnn.to_layout"(%arg63, %0) <{layout = #ttnn.layout<tile>}> : (tensor<256x1x1xf32, #layout2>, !tt.device<#device>) -> tensor<256x1x1xf32, #layout45> loc(#loc707)
    %652 = "ttnn.to_device"(%651, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<256x1x1xf32, #layout45>, !tt.device<#device>) -> tensor<256x1x1xf32, #layout45> loc(#loc707)
    %653 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x256x14x14>}> : (!tt.device<#device>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc707)
    %654 = "ttnn.multiply"(%650, %652, %653) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x256x14x14xf32, #layout62>, tensor<256x1x1xf32, #layout45>, tensor<1x256x14x14xf32, #layout62>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc707)
    %655 = "ttnn.to_layout"(%arg64, %0) <{layout = #ttnn.layout<tile>}> : (tensor<256x1x1xf32, #layout2>, !tt.device<#device>) -> tensor<256x1x1xf32, #layout45> loc(#loc708)
    %656 = "ttnn.to_device"(%655, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<256x1x1xf32, #layout45>, !tt.device<#device>) -> tensor<256x1x1xf32, #layout45> loc(#loc708)
    %657 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x256x14x14>}> : (!tt.device<#device>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc708)
    %658 = "ttnn.add"(%654, %656, %657) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x256x14x14xf32, #layout62>, tensor<256x1x1xf32, #layout45>, tensor<1x256x14x14xf32, #layout62>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc708)
    %659 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x256x14x14>}> : (!tt.device<#device>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc709)
    %660 = "ttnn.relu"(%658, %659) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x256x14x14xf32, #layout62>, tensor<1x256x14x14xf32, #layout62>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc709)
    %661 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x14x256x14>}> : (!tt.device<#device>) -> tensor<1x14x256x14xf32, #layout61> loc(#loc710)
    %662 = "ttnn.transpose"(%660, %661) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x256x14x14xf32, #layout62>, tensor<1x14x256x14xf32, #layout61>) -> tensor<1x14x256x14xf32, #layout61> loc(#loc710)
    %663 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x14x14x256>}> : (!tt.device<#device>) -> tensor<1x14x14x256xf32, #layout60> loc(#loc711)
    %664 = "ttnn.transpose"(%662, %663) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x14x256x14xf32, #layout61>, tensor<1x14x14x256xf32, #layout60>) -> tensor<1x14x14x256xf32, #layout60> loc(#loc711)
    %665 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x14x14x256>}> : (!tt.device<#device>) -> tensor<1x14x14x256xf32, #layout60> loc(#loc712)
    %666 = "ttnn.conv2d"(%664, %arg139, %665, %0) <{batch_size = 1 : i32, dilation_height = 1 : i32, dilation_width = 1 : i32, groups = 1 : i32, in_channels = 256 : i32, input_height = 14 : i32, input_width = 14 : i32, kernel_height = 3 : i32, kernel_width = 3 : i32, out_channels = 256 : i32, padding_height = 1 : i32, padding_width = 1 : i32, stride_height = 1 : i32, stride_width = 1 : i32}> : (tensor<1x14x14x256xf32, #layout60>, tensor<256x256x3x3xf32, #layout18>, tensor<1x14x14x256xf32, #layout60>, !tt.device<#device>) -> tensor<1x14x14x256xf32, #layout60> loc(#loc712)
    %667 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x14x256x14>}> : (!tt.device<#device>) -> tensor<1x14x256x14xf32, #layout61> loc(#loc713)
    %668 = "ttnn.transpose"(%666, %667) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x14x14x256xf32, #layout60>, tensor<1x14x256x14xf32, #layout61>) -> tensor<1x14x256x14xf32, #layout61> loc(#loc713)
    %669 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x256x14x14>}> : (!tt.device<#device>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc714)
    %670 = "ttnn.transpose"(%668, %669) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x14x256x14xf32, #layout61>, tensor<1x256x14x14xf32, #layout62>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc714)
    %671 = "ttnn.to_layout"(%arg65, %0) <{layout = #ttnn.layout<tile>}> : (tensor<256x1x1xf32, #layout2>, !tt.device<#device>) -> tensor<256x1x1xf32, #layout45> loc(#loc715)
    %672 = "ttnn.to_device"(%671, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<256x1x1xf32, #layout45>, !tt.device<#device>) -> tensor<256x1x1xf32, #layout45> loc(#loc715)
    %673 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x256x14x14>}> : (!tt.device<#device>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc715)
    %674 = "ttnn.multiply"(%670, %672, %673) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x256x14x14xf32, #layout62>, tensor<256x1x1xf32, #layout45>, tensor<1x256x14x14xf32, #layout62>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc715)
    %675 = "ttnn.to_layout"(%arg66, %0) <{layout = #ttnn.layout<tile>}> : (tensor<256x1x1xf32, #layout2>, !tt.device<#device>) -> tensor<256x1x1xf32, #layout45> loc(#loc716)
    %676 = "ttnn.to_device"(%675, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<256x1x1xf32, #layout45>, !tt.device<#device>) -> tensor<256x1x1xf32, #layout45> loc(#loc716)
    %677 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x256x14x14>}> : (!tt.device<#device>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc716)
    %678 = "ttnn.add"(%674, %676, %677) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x256x14x14xf32, #layout62>, tensor<256x1x1xf32, #layout45>, tensor<1x256x14x14xf32, #layout62>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc716)
    %679 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x256x14x14>}> : (!tt.device<#device>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc717)
    %680 = "ttnn.relu"(%678, %679) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x256x14x14xf32, #layout62>, tensor<1x256x14x14xf32, #layout62>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc717)
    %681 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x14x256x14>}> : (!tt.device<#device>) -> tensor<1x14x256x14xf32, #layout61> loc(#loc718)
    %682 = "ttnn.transpose"(%680, %681) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x256x14x14xf32, #layout62>, tensor<1x14x256x14xf32, #layout61>) -> tensor<1x14x256x14xf32, #layout61> loc(#loc718)
    %683 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x14x14x256>}> : (!tt.device<#device>) -> tensor<1x14x14x256xf32, #layout60> loc(#loc719)
    %684 = "ttnn.transpose"(%682, %683) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x14x256x14xf32, #layout61>, tensor<1x14x14x256xf32, #layout60>) -> tensor<1x14x14x256xf32, #layout60> loc(#loc719)
    %685 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x14x14x1024>}> : (!tt.device<#device>) -> tensor<1x14x14x1024xf32, #layout63> loc(#loc720)
    %686 = "ttnn.conv2d"(%684, %arg140, %685, %0) <{batch_size = 1 : i32, dilation_height = 1 : i32, dilation_width = 1 : i32, groups = 1 : i32, in_channels = 256 : i32, input_height = 14 : i32, input_width = 14 : i32, kernel_height = 1 : i32, kernel_width = 1 : i32, out_channels = 1024 : i32, padding_height = 0 : i32, padding_width = 0 : i32, stride_height = 1 : i32, stride_width = 1 : i32}> : (tensor<1x14x14x256xf32, #layout60>, tensor<1024x256x1x1xf32, #layout19>, tensor<1x14x14x1024xf32, #layout63>, !tt.device<#device>) -> tensor<1x14x14x1024xf32, #layout63> loc(#loc720)
    %687 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x14x1024x14>}> : (!tt.device<#device>) -> tensor<1x14x1024x14xf32, #layout64> loc(#loc721)
    %688 = "ttnn.transpose"(%686, %687) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x14x14x1024xf32, #layout63>, tensor<1x14x1024x14xf32, #layout64>) -> tensor<1x14x1024x14xf32, #layout64> loc(#loc721)
    %689 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x1024x14x14>}> : (!tt.device<#device>) -> tensor<1x1024x14x14xf32, #layout65> loc(#loc722)
    %690 = "ttnn.transpose"(%688, %689) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x14x1024x14xf32, #layout64>, tensor<1x1024x14x14xf32, #layout65>) -> tensor<1x1024x14x14xf32, #layout65> loc(#loc722)
    %691 = "ttnn.to_layout"(%arg67, %0) <{layout = #ttnn.layout<tile>}> : (tensor<1024x1x1xf32, #layout5>, !tt.device<#device>) -> tensor<1024x1x1xf32, #layout66> loc(#loc723)
    %692 = "ttnn.to_device"(%691, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<1024x1x1xf32, #layout66>, !tt.device<#device>) -> tensor<1024x1x1xf32, #layout66> loc(#loc723)
    %693 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x1024x14x14>}> : (!tt.device<#device>) -> tensor<1x1024x14x14xf32, #layout65> loc(#loc723)
    %694 = "ttnn.multiply"(%690, %692, %693) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x1024x14x14xf32, #layout65>, tensor<1024x1x1xf32, #layout66>, tensor<1x1024x14x14xf32, #layout65>) -> tensor<1x1024x14x14xf32, #layout65> loc(#loc723)
    %695 = "ttnn.to_layout"(%arg68, %0) <{layout = #ttnn.layout<tile>}> : (tensor<1024x1x1xf32, #layout5>, !tt.device<#device>) -> tensor<1024x1x1xf32, #layout66> loc(#loc724)
    %696 = "ttnn.to_device"(%695, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<1024x1x1xf32, #layout66>, !tt.device<#device>) -> tensor<1024x1x1xf32, #layout66> loc(#loc724)
    %697 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x1024x14x14>}> : (!tt.device<#device>) -> tensor<1x1024x14x14xf32, #layout65> loc(#loc724)
    %698 = "ttnn.add"(%694, %696, %697) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x1024x14x14xf32, #layout65>, tensor<1024x1x1xf32, #layout66>, tensor<1x1024x14x14xf32, #layout65>) -> tensor<1x1024x14x14xf32, #layout65> loc(#loc724)
    %699 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x1024x14x14>}> : (!tt.device<#device>) -> tensor<1x1024x14x14xf32, #layout65> loc(#loc725)
    %700 = "ttnn.add"(%698, %640, %699) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x1024x14x14xf32, #layout65>, tensor<1x1024x14x14xf32, #layout65>, tensor<1x1024x14x14xf32, #layout65>) -> tensor<1x1024x14x14xf32, #layout65> loc(#loc725)
    %701 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x1024x14x14>}> : (!tt.device<#device>) -> tensor<1x1024x14x14xf32, #layout65> loc(#loc726)
    %702 = "ttnn.relu"(%700, %701) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x1024x14x14xf32, #layout65>, tensor<1x1024x14x14xf32, #layout65>) -> tensor<1x1024x14x14xf32, #layout65> loc(#loc726)
    %703 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x14x1024x14>}> : (!tt.device<#device>) -> tensor<1x14x1024x14xf32, #layout64> loc(#loc727)
    %704 = "ttnn.transpose"(%702, %703) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x1024x14x14xf32, #layout65>, tensor<1x14x1024x14xf32, #layout64>) -> tensor<1x14x1024x14xf32, #layout64> loc(#loc727)
    %705 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x14x14x1024>}> : (!tt.device<#device>) -> tensor<1x14x14x1024xf32, #layout63> loc(#loc728)
    %706 = "ttnn.transpose"(%704, %705) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x14x1024x14xf32, #layout64>, tensor<1x14x14x1024xf32, #layout63>) -> tensor<1x14x14x1024xf32, #layout63> loc(#loc728)
    %707 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x14x14x256>}> : (!tt.device<#device>) -> tensor<1x14x14x256xf32, #layout60> loc(#loc729)
    %708 = "ttnn.conv2d"(%706, %arg141, %707, %0) <{batch_size = 1 : i32, dilation_height = 1 : i32, dilation_width = 1 : i32, groups = 1 : i32, in_channels = 1024 : i32, input_height = 14 : i32, input_width = 14 : i32, kernel_height = 1 : i32, kernel_width = 1 : i32, out_channels = 256 : i32, padding_height = 0 : i32, padding_width = 0 : i32, stride_height = 1 : i32, stride_width = 1 : i32}> : (tensor<1x14x14x1024xf32, #layout63>, tensor<256x1024x1x1xf32, #layout21>, tensor<1x14x14x256xf32, #layout60>, !tt.device<#device>) -> tensor<1x14x14x256xf32, #layout60> loc(#loc729)
    %709 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x14x256x14>}> : (!tt.device<#device>) -> tensor<1x14x256x14xf32, #layout61> loc(#loc730)
    %710 = "ttnn.transpose"(%708, %709) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x14x14x256xf32, #layout60>, tensor<1x14x256x14xf32, #layout61>) -> tensor<1x14x256x14xf32, #layout61> loc(#loc730)
    %711 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x256x14x14>}> : (!tt.device<#device>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc731)
    %712 = "ttnn.transpose"(%710, %711) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x14x256x14xf32, #layout61>, tensor<1x256x14x14xf32, #layout62>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc731)
    %713 = "ttnn.to_layout"(%arg69, %0) <{layout = #ttnn.layout<tile>}> : (tensor<256x1x1xf32, #layout2>, !tt.device<#device>) -> tensor<256x1x1xf32, #layout45> loc(#loc732)
    %714 = "ttnn.to_device"(%713, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<256x1x1xf32, #layout45>, !tt.device<#device>) -> tensor<256x1x1xf32, #layout45> loc(#loc732)
    %715 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x256x14x14>}> : (!tt.device<#device>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc732)
    %716 = "ttnn.multiply"(%712, %714, %715) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x256x14x14xf32, #layout62>, tensor<256x1x1xf32, #layout45>, tensor<1x256x14x14xf32, #layout62>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc732)
    %717 = "ttnn.to_layout"(%arg70, %0) <{layout = #ttnn.layout<tile>}> : (tensor<256x1x1xf32, #layout2>, !tt.device<#device>) -> tensor<256x1x1xf32, #layout45> loc(#loc733)
    %718 = "ttnn.to_device"(%717, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<256x1x1xf32, #layout45>, !tt.device<#device>) -> tensor<256x1x1xf32, #layout45> loc(#loc733)
    %719 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x256x14x14>}> : (!tt.device<#device>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc733)
    %720 = "ttnn.add"(%716, %718, %719) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x256x14x14xf32, #layout62>, tensor<256x1x1xf32, #layout45>, tensor<1x256x14x14xf32, #layout62>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc733)
    %721 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x256x14x14>}> : (!tt.device<#device>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc734)
    %722 = "ttnn.relu"(%720, %721) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x256x14x14xf32, #layout62>, tensor<1x256x14x14xf32, #layout62>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc734)
    %723 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x14x256x14>}> : (!tt.device<#device>) -> tensor<1x14x256x14xf32, #layout61> loc(#loc735)
    %724 = "ttnn.transpose"(%722, %723) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x256x14x14xf32, #layout62>, tensor<1x14x256x14xf32, #layout61>) -> tensor<1x14x256x14xf32, #layout61> loc(#loc735)
    %725 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x14x14x256>}> : (!tt.device<#device>) -> tensor<1x14x14x256xf32, #layout60> loc(#loc736)
    %726 = "ttnn.transpose"(%724, %725) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x14x256x14xf32, #layout61>, tensor<1x14x14x256xf32, #layout60>) -> tensor<1x14x14x256xf32, #layout60> loc(#loc736)
    %727 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x14x14x256>}> : (!tt.device<#device>) -> tensor<1x14x14x256xf32, #layout60> loc(#loc737)
    %728 = "ttnn.conv2d"(%726, %arg142, %727, %0) <{batch_size = 1 : i32, dilation_height = 1 : i32, dilation_width = 1 : i32, groups = 1 : i32, in_channels = 256 : i32, input_height = 14 : i32, input_width = 14 : i32, kernel_height = 3 : i32, kernel_width = 3 : i32, out_channels = 256 : i32, padding_height = 1 : i32, padding_width = 1 : i32, stride_height = 1 : i32, stride_width = 1 : i32}> : (tensor<1x14x14x256xf32, #layout60>, tensor<256x256x3x3xf32, #layout18>, tensor<1x14x14x256xf32, #layout60>, !tt.device<#device>) -> tensor<1x14x14x256xf32, #layout60> loc(#loc737)
    %729 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x14x256x14>}> : (!tt.device<#device>) -> tensor<1x14x256x14xf32, #layout61> loc(#loc738)
    %730 = "ttnn.transpose"(%728, %729) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x14x14x256xf32, #layout60>, tensor<1x14x256x14xf32, #layout61>) -> tensor<1x14x256x14xf32, #layout61> loc(#loc738)
    %731 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x256x14x14>}> : (!tt.device<#device>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc739)
    %732 = "ttnn.transpose"(%730, %731) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x14x256x14xf32, #layout61>, tensor<1x256x14x14xf32, #layout62>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc739)
    %733 = "ttnn.to_layout"(%arg71, %0) <{layout = #ttnn.layout<tile>}> : (tensor<256x1x1xf32, #layout2>, !tt.device<#device>) -> tensor<256x1x1xf32, #layout45> loc(#loc740)
    %734 = "ttnn.to_device"(%733, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<256x1x1xf32, #layout45>, !tt.device<#device>) -> tensor<256x1x1xf32, #layout45> loc(#loc740)
    %735 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x256x14x14>}> : (!tt.device<#device>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc740)
    %736 = "ttnn.multiply"(%732, %734, %735) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x256x14x14xf32, #layout62>, tensor<256x1x1xf32, #layout45>, tensor<1x256x14x14xf32, #layout62>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc740)
    %737 = "ttnn.to_layout"(%arg72, %0) <{layout = #ttnn.layout<tile>}> : (tensor<256x1x1xf32, #layout2>, !tt.device<#device>) -> tensor<256x1x1xf32, #layout45> loc(#loc741)
    %738 = "ttnn.to_device"(%737, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<256x1x1xf32, #layout45>, !tt.device<#device>) -> tensor<256x1x1xf32, #layout45> loc(#loc741)
    %739 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x256x14x14>}> : (!tt.device<#device>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc741)
    %740 = "ttnn.add"(%736, %738, %739) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x256x14x14xf32, #layout62>, tensor<256x1x1xf32, #layout45>, tensor<1x256x14x14xf32, #layout62>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc741)
    %741 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x256x14x14>}> : (!tt.device<#device>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc742)
    %742 = "ttnn.relu"(%740, %741) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x256x14x14xf32, #layout62>, tensor<1x256x14x14xf32, #layout62>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc742)
    %743 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x14x256x14>}> : (!tt.device<#device>) -> tensor<1x14x256x14xf32, #layout61> loc(#loc743)
    %744 = "ttnn.transpose"(%742, %743) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x256x14x14xf32, #layout62>, tensor<1x14x256x14xf32, #layout61>) -> tensor<1x14x256x14xf32, #layout61> loc(#loc743)
    %745 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x14x14x256>}> : (!tt.device<#device>) -> tensor<1x14x14x256xf32, #layout60> loc(#loc744)
    %746 = "ttnn.transpose"(%744, %745) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x14x256x14xf32, #layout61>, tensor<1x14x14x256xf32, #layout60>) -> tensor<1x14x14x256xf32, #layout60> loc(#loc744)
    %747 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x14x14x1024>}> : (!tt.device<#device>) -> tensor<1x14x14x1024xf32, #layout63> loc(#loc745)
    %748 = "ttnn.conv2d"(%746, %arg143, %747, %0) <{batch_size = 1 : i32, dilation_height = 1 : i32, dilation_width = 1 : i32, groups = 1 : i32, in_channels = 256 : i32, input_height = 14 : i32, input_width = 14 : i32, kernel_height = 1 : i32, kernel_width = 1 : i32, out_channels = 1024 : i32, padding_height = 0 : i32, padding_width = 0 : i32, stride_height = 1 : i32, stride_width = 1 : i32}> : (tensor<1x14x14x256xf32, #layout60>, tensor<1024x256x1x1xf32, #layout19>, tensor<1x14x14x1024xf32, #layout63>, !tt.device<#device>) -> tensor<1x14x14x1024xf32, #layout63> loc(#loc745)
    %749 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x14x1024x14>}> : (!tt.device<#device>) -> tensor<1x14x1024x14xf32, #layout64> loc(#loc746)
    %750 = "ttnn.transpose"(%748, %749) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x14x14x1024xf32, #layout63>, tensor<1x14x1024x14xf32, #layout64>) -> tensor<1x14x1024x14xf32, #layout64> loc(#loc746)
    %751 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x1024x14x14>}> : (!tt.device<#device>) -> tensor<1x1024x14x14xf32, #layout65> loc(#loc747)
    %752 = "ttnn.transpose"(%750, %751) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x14x1024x14xf32, #layout64>, tensor<1x1024x14x14xf32, #layout65>) -> tensor<1x1024x14x14xf32, #layout65> loc(#loc747)
    %753 = "ttnn.to_layout"(%arg73, %0) <{layout = #ttnn.layout<tile>}> : (tensor<1024x1x1xf32, #layout5>, !tt.device<#device>) -> tensor<1024x1x1xf32, #layout66> loc(#loc748)
    %754 = "ttnn.to_device"(%753, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<1024x1x1xf32, #layout66>, !tt.device<#device>) -> tensor<1024x1x1xf32, #layout66> loc(#loc748)
    %755 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x1024x14x14>}> : (!tt.device<#device>) -> tensor<1x1024x14x14xf32, #layout65> loc(#loc748)
    %756 = "ttnn.multiply"(%752, %754, %755) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x1024x14x14xf32, #layout65>, tensor<1024x1x1xf32, #layout66>, tensor<1x1024x14x14xf32, #layout65>) -> tensor<1x1024x14x14xf32, #layout65> loc(#loc748)
    %757 = "ttnn.to_layout"(%arg74, %0) <{layout = #ttnn.layout<tile>}> : (tensor<1024x1x1xf32, #layout5>, !tt.device<#device>) -> tensor<1024x1x1xf32, #layout66> loc(#loc749)
    %758 = "ttnn.to_device"(%757, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<1024x1x1xf32, #layout66>, !tt.device<#device>) -> tensor<1024x1x1xf32, #layout66> loc(#loc749)
    %759 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x1024x14x14>}> : (!tt.device<#device>) -> tensor<1x1024x14x14xf32, #layout65> loc(#loc749)
    %760 = "ttnn.add"(%756, %758, %759) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x1024x14x14xf32, #layout65>, tensor<1024x1x1xf32, #layout66>, tensor<1x1024x14x14xf32, #layout65>) -> tensor<1x1024x14x14xf32, #layout65> loc(#loc749)
    %761 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x1024x14x14>}> : (!tt.device<#device>) -> tensor<1x1024x14x14xf32, #layout65> loc(#loc750)
    %762 = "ttnn.add"(%760, %702, %761) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x1024x14x14xf32, #layout65>, tensor<1x1024x14x14xf32, #layout65>, tensor<1x1024x14x14xf32, #layout65>) -> tensor<1x1024x14x14xf32, #layout65> loc(#loc750)
    %763 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x1024x14x14>}> : (!tt.device<#device>) -> tensor<1x1024x14x14xf32, #layout65> loc(#loc751)
    %764 = "ttnn.relu"(%762, %763) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x1024x14x14xf32, #layout65>, tensor<1x1024x14x14xf32, #layout65>) -> tensor<1x1024x14x14xf32, #layout65> loc(#loc751)
    %765 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x14x1024x14>}> : (!tt.device<#device>) -> tensor<1x14x1024x14xf32, #layout64> loc(#loc752)
    %766 = "ttnn.transpose"(%764, %765) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x1024x14x14xf32, #layout65>, tensor<1x14x1024x14xf32, #layout64>) -> tensor<1x14x1024x14xf32, #layout64> loc(#loc752)
    %767 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x14x14x1024>}> : (!tt.device<#device>) -> tensor<1x14x14x1024xf32, #layout63> loc(#loc753)
    %768 = "ttnn.transpose"(%766, %767) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x14x1024x14xf32, #layout64>, tensor<1x14x14x1024xf32, #layout63>) -> tensor<1x14x14x1024xf32, #layout63> loc(#loc753)
    %769 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x14x14x256>}> : (!tt.device<#device>) -> tensor<1x14x14x256xf32, #layout60> loc(#loc754)
    %770 = "ttnn.conv2d"(%768, %arg144, %769, %0) <{batch_size = 1 : i32, dilation_height = 1 : i32, dilation_width = 1 : i32, groups = 1 : i32, in_channels = 1024 : i32, input_height = 14 : i32, input_width = 14 : i32, kernel_height = 1 : i32, kernel_width = 1 : i32, out_channels = 256 : i32, padding_height = 0 : i32, padding_width = 0 : i32, stride_height = 1 : i32, stride_width = 1 : i32}> : (tensor<1x14x14x1024xf32, #layout63>, tensor<256x1024x1x1xf32, #layout21>, tensor<1x14x14x256xf32, #layout60>, !tt.device<#device>) -> tensor<1x14x14x256xf32, #layout60> loc(#loc754)
    %771 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x14x256x14>}> : (!tt.device<#device>) -> tensor<1x14x256x14xf32, #layout61> loc(#loc755)
    %772 = "ttnn.transpose"(%770, %771) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x14x14x256xf32, #layout60>, tensor<1x14x256x14xf32, #layout61>) -> tensor<1x14x256x14xf32, #layout61> loc(#loc755)
    %773 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x256x14x14>}> : (!tt.device<#device>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc756)
    %774 = "ttnn.transpose"(%772, %773) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x14x256x14xf32, #layout61>, tensor<1x256x14x14xf32, #layout62>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc756)
    %775 = "ttnn.to_layout"(%arg75, %0) <{layout = #ttnn.layout<tile>}> : (tensor<256x1x1xf32, #layout2>, !tt.device<#device>) -> tensor<256x1x1xf32, #layout45> loc(#loc757)
    %776 = "ttnn.to_device"(%775, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<256x1x1xf32, #layout45>, !tt.device<#device>) -> tensor<256x1x1xf32, #layout45> loc(#loc757)
    %777 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x256x14x14>}> : (!tt.device<#device>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc757)
    %778 = "ttnn.multiply"(%774, %776, %777) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x256x14x14xf32, #layout62>, tensor<256x1x1xf32, #layout45>, tensor<1x256x14x14xf32, #layout62>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc757)
    %779 = "ttnn.to_layout"(%arg76, %0) <{layout = #ttnn.layout<tile>}> : (tensor<256x1x1xf32, #layout2>, !tt.device<#device>) -> tensor<256x1x1xf32, #layout45> loc(#loc758)
    %780 = "ttnn.to_device"(%779, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<256x1x1xf32, #layout45>, !tt.device<#device>) -> tensor<256x1x1xf32, #layout45> loc(#loc758)
    %781 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x256x14x14>}> : (!tt.device<#device>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc758)
    %782 = "ttnn.add"(%778, %780, %781) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x256x14x14xf32, #layout62>, tensor<256x1x1xf32, #layout45>, tensor<1x256x14x14xf32, #layout62>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc758)
    %783 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x256x14x14>}> : (!tt.device<#device>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc759)
    %784 = "ttnn.relu"(%782, %783) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x256x14x14xf32, #layout62>, tensor<1x256x14x14xf32, #layout62>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc759)
    %785 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x14x256x14>}> : (!tt.device<#device>) -> tensor<1x14x256x14xf32, #layout61> loc(#loc760)
    %786 = "ttnn.transpose"(%784, %785) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x256x14x14xf32, #layout62>, tensor<1x14x256x14xf32, #layout61>) -> tensor<1x14x256x14xf32, #layout61> loc(#loc760)
    %787 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x14x14x256>}> : (!tt.device<#device>) -> tensor<1x14x14x256xf32, #layout60> loc(#loc761)
    %788 = "ttnn.transpose"(%786, %787) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x14x256x14xf32, #layout61>, tensor<1x14x14x256xf32, #layout60>) -> tensor<1x14x14x256xf32, #layout60> loc(#loc761)
    %789 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x14x14x256>}> : (!tt.device<#device>) -> tensor<1x14x14x256xf32, #layout60> loc(#loc762)
    %790 = "ttnn.conv2d"(%788, %arg145, %789, %0) <{batch_size = 1 : i32, dilation_height = 1 : i32, dilation_width = 1 : i32, groups = 1 : i32, in_channels = 256 : i32, input_height = 14 : i32, input_width = 14 : i32, kernel_height = 3 : i32, kernel_width = 3 : i32, out_channels = 256 : i32, padding_height = 1 : i32, padding_width = 1 : i32, stride_height = 1 : i32, stride_width = 1 : i32}> : (tensor<1x14x14x256xf32, #layout60>, tensor<256x256x3x3xf32, #layout18>, tensor<1x14x14x256xf32, #layout60>, !tt.device<#device>) -> tensor<1x14x14x256xf32, #layout60> loc(#loc762)
    %791 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x14x256x14>}> : (!tt.device<#device>) -> tensor<1x14x256x14xf32, #layout61> loc(#loc763)
    %792 = "ttnn.transpose"(%790, %791) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x14x14x256xf32, #layout60>, tensor<1x14x256x14xf32, #layout61>) -> tensor<1x14x256x14xf32, #layout61> loc(#loc763)
    %793 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x256x14x14>}> : (!tt.device<#device>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc764)
    %794 = "ttnn.transpose"(%792, %793) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x14x256x14xf32, #layout61>, tensor<1x256x14x14xf32, #layout62>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc764)
    %795 = "ttnn.to_layout"(%arg77, %0) <{layout = #ttnn.layout<tile>}> : (tensor<256x1x1xf32, #layout2>, !tt.device<#device>) -> tensor<256x1x1xf32, #layout45> loc(#loc765)
    %796 = "ttnn.to_device"(%795, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<256x1x1xf32, #layout45>, !tt.device<#device>) -> tensor<256x1x1xf32, #layout45> loc(#loc765)
    %797 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x256x14x14>}> : (!tt.device<#device>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc765)
    %798 = "ttnn.multiply"(%794, %796, %797) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x256x14x14xf32, #layout62>, tensor<256x1x1xf32, #layout45>, tensor<1x256x14x14xf32, #layout62>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc765)
    %799 = "ttnn.to_layout"(%arg78, %0) <{layout = #ttnn.layout<tile>}> : (tensor<256x1x1xf32, #layout2>, !tt.device<#device>) -> tensor<256x1x1xf32, #layout45> loc(#loc766)
    %800 = "ttnn.to_device"(%799, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<256x1x1xf32, #layout45>, !tt.device<#device>) -> tensor<256x1x1xf32, #layout45> loc(#loc766)
    %801 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x256x14x14>}> : (!tt.device<#device>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc766)
    %802 = "ttnn.add"(%798, %800, %801) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x256x14x14xf32, #layout62>, tensor<256x1x1xf32, #layout45>, tensor<1x256x14x14xf32, #layout62>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc766)
    %803 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x256x14x14>}> : (!tt.device<#device>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc767)
    %804 = "ttnn.relu"(%802, %803) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x256x14x14xf32, #layout62>, tensor<1x256x14x14xf32, #layout62>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc767)
    %805 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x14x256x14>}> : (!tt.device<#device>) -> tensor<1x14x256x14xf32, #layout61> loc(#loc768)
    %806 = "ttnn.transpose"(%804, %805) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x256x14x14xf32, #layout62>, tensor<1x14x256x14xf32, #layout61>) -> tensor<1x14x256x14xf32, #layout61> loc(#loc768)
    %807 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x14x14x256>}> : (!tt.device<#device>) -> tensor<1x14x14x256xf32, #layout60> loc(#loc769)
    %808 = "ttnn.transpose"(%806, %807) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x14x256x14xf32, #layout61>, tensor<1x14x14x256xf32, #layout60>) -> tensor<1x14x14x256xf32, #layout60> loc(#loc769)
    %809 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x14x14x1024>}> : (!tt.device<#device>) -> tensor<1x14x14x1024xf32, #layout63> loc(#loc770)
    %810 = "ttnn.conv2d"(%808, %arg146, %809, %0) <{batch_size = 1 : i32, dilation_height = 1 : i32, dilation_width = 1 : i32, groups = 1 : i32, in_channels = 256 : i32, input_height = 14 : i32, input_width = 14 : i32, kernel_height = 1 : i32, kernel_width = 1 : i32, out_channels = 1024 : i32, padding_height = 0 : i32, padding_width = 0 : i32, stride_height = 1 : i32, stride_width = 1 : i32}> : (tensor<1x14x14x256xf32, #layout60>, tensor<1024x256x1x1xf32, #layout19>, tensor<1x14x14x1024xf32, #layout63>, !tt.device<#device>) -> tensor<1x14x14x1024xf32, #layout63> loc(#loc770)
    %811 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x14x1024x14>}> : (!tt.device<#device>) -> tensor<1x14x1024x14xf32, #layout64> loc(#loc771)
    %812 = "ttnn.transpose"(%810, %811) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x14x14x1024xf32, #layout63>, tensor<1x14x1024x14xf32, #layout64>) -> tensor<1x14x1024x14xf32, #layout64> loc(#loc771)
    %813 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x1024x14x14>}> : (!tt.device<#device>) -> tensor<1x1024x14x14xf32, #layout65> loc(#loc772)
    %814 = "ttnn.transpose"(%812, %813) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x14x1024x14xf32, #layout64>, tensor<1x1024x14x14xf32, #layout65>) -> tensor<1x1024x14x14xf32, #layout65> loc(#loc772)
    %815 = "ttnn.to_layout"(%arg79, %0) <{layout = #ttnn.layout<tile>}> : (tensor<1024x1x1xf32, #layout5>, !tt.device<#device>) -> tensor<1024x1x1xf32, #layout66> loc(#loc773)
    %816 = "ttnn.to_device"(%815, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<1024x1x1xf32, #layout66>, !tt.device<#device>) -> tensor<1024x1x1xf32, #layout66> loc(#loc773)
    %817 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x1024x14x14>}> : (!tt.device<#device>) -> tensor<1x1024x14x14xf32, #layout65> loc(#loc773)
    %818 = "ttnn.multiply"(%814, %816, %817) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x1024x14x14xf32, #layout65>, tensor<1024x1x1xf32, #layout66>, tensor<1x1024x14x14xf32, #layout65>) -> tensor<1x1024x14x14xf32, #layout65> loc(#loc773)
    %819 = "ttnn.to_layout"(%arg80, %0) <{layout = #ttnn.layout<tile>}> : (tensor<1024x1x1xf32, #layout5>, !tt.device<#device>) -> tensor<1024x1x1xf32, #layout66> loc(#loc774)
    %820 = "ttnn.to_device"(%819, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<1024x1x1xf32, #layout66>, !tt.device<#device>) -> tensor<1024x1x1xf32, #layout66> loc(#loc774)
    %821 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x1024x14x14>}> : (!tt.device<#device>) -> tensor<1x1024x14x14xf32, #layout65> loc(#loc774)
    %822 = "ttnn.add"(%818, %820, %821) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x1024x14x14xf32, #layout65>, tensor<1024x1x1xf32, #layout66>, tensor<1x1024x14x14xf32, #layout65>) -> tensor<1x1024x14x14xf32, #layout65> loc(#loc774)
    %823 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x1024x14x14>}> : (!tt.device<#device>) -> tensor<1x1024x14x14xf32, #layout65> loc(#loc775)
    %824 = "ttnn.add"(%822, %764, %823) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x1024x14x14xf32, #layout65>, tensor<1x1024x14x14xf32, #layout65>, tensor<1x1024x14x14xf32, #layout65>) -> tensor<1x1024x14x14xf32, #layout65> loc(#loc775)
    %825 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x1024x14x14>}> : (!tt.device<#device>) -> tensor<1x1024x14x14xf32, #layout65> loc(#loc776)
    %826 = "ttnn.relu"(%824, %825) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x1024x14x14xf32, #layout65>, tensor<1x1024x14x14xf32, #layout65>) -> tensor<1x1024x14x14xf32, #layout65> loc(#loc776)
    %827 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x14x1024x14>}> : (!tt.device<#device>) -> tensor<1x14x1024x14xf32, #layout64> loc(#loc777)
    %828 = "ttnn.transpose"(%826, %827) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x1024x14x14xf32, #layout65>, tensor<1x14x1024x14xf32, #layout64>) -> tensor<1x14x1024x14xf32, #layout64> loc(#loc777)
    %829 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x14x14x1024>}> : (!tt.device<#device>) -> tensor<1x14x14x1024xf32, #layout63> loc(#loc778)
    %830 = "ttnn.transpose"(%828, %829) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x14x1024x14xf32, #layout64>, tensor<1x14x14x1024xf32, #layout63>) -> tensor<1x14x14x1024xf32, #layout63> loc(#loc778)
    %831 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x14x14x256>}> : (!tt.device<#device>) -> tensor<1x14x14x256xf32, #layout60> loc(#loc779)
    %832 = "ttnn.conv2d"(%830, %arg147, %831, %0) <{batch_size = 1 : i32, dilation_height = 1 : i32, dilation_width = 1 : i32, groups = 1 : i32, in_channels = 1024 : i32, input_height = 14 : i32, input_width = 14 : i32, kernel_height = 1 : i32, kernel_width = 1 : i32, out_channels = 256 : i32, padding_height = 0 : i32, padding_width = 0 : i32, stride_height = 1 : i32, stride_width = 1 : i32}> : (tensor<1x14x14x1024xf32, #layout63>, tensor<256x1024x1x1xf32, #layout21>, tensor<1x14x14x256xf32, #layout60>, !tt.device<#device>) -> tensor<1x14x14x256xf32, #layout60> loc(#loc779)
    %833 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x14x256x14>}> : (!tt.device<#device>) -> tensor<1x14x256x14xf32, #layout61> loc(#loc780)
    %834 = "ttnn.transpose"(%832, %833) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x14x14x256xf32, #layout60>, tensor<1x14x256x14xf32, #layout61>) -> tensor<1x14x256x14xf32, #layout61> loc(#loc780)
    %835 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x256x14x14>}> : (!tt.device<#device>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc781)
    %836 = "ttnn.transpose"(%834, %835) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x14x256x14xf32, #layout61>, tensor<1x256x14x14xf32, #layout62>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc781)
    %837 = "ttnn.to_layout"(%arg81, %0) <{layout = #ttnn.layout<tile>}> : (tensor<256x1x1xf32, #layout2>, !tt.device<#device>) -> tensor<256x1x1xf32, #layout45> loc(#loc782)
    %838 = "ttnn.to_device"(%837, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<256x1x1xf32, #layout45>, !tt.device<#device>) -> tensor<256x1x1xf32, #layout45> loc(#loc782)
    %839 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x256x14x14>}> : (!tt.device<#device>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc782)
    %840 = "ttnn.multiply"(%836, %838, %839) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x256x14x14xf32, #layout62>, tensor<256x1x1xf32, #layout45>, tensor<1x256x14x14xf32, #layout62>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc782)
    %841 = "ttnn.to_layout"(%arg82, %0) <{layout = #ttnn.layout<tile>}> : (tensor<256x1x1xf32, #layout2>, !tt.device<#device>) -> tensor<256x1x1xf32, #layout45> loc(#loc783)
    %842 = "ttnn.to_device"(%841, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<256x1x1xf32, #layout45>, !tt.device<#device>) -> tensor<256x1x1xf32, #layout45> loc(#loc783)
    %843 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x256x14x14>}> : (!tt.device<#device>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc783)
    %844 = "ttnn.add"(%840, %842, %843) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x256x14x14xf32, #layout62>, tensor<256x1x1xf32, #layout45>, tensor<1x256x14x14xf32, #layout62>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc783)
    %845 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x256x14x14>}> : (!tt.device<#device>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc784)
    %846 = "ttnn.relu"(%844, %845) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x256x14x14xf32, #layout62>, tensor<1x256x14x14xf32, #layout62>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc784)
    %847 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x14x256x14>}> : (!tt.device<#device>) -> tensor<1x14x256x14xf32, #layout61> loc(#loc785)
    %848 = "ttnn.transpose"(%846, %847) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x256x14x14xf32, #layout62>, tensor<1x14x256x14xf32, #layout61>) -> tensor<1x14x256x14xf32, #layout61> loc(#loc785)
    %849 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x14x14x256>}> : (!tt.device<#device>) -> tensor<1x14x14x256xf32, #layout60> loc(#loc786)
    %850 = "ttnn.transpose"(%848, %849) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x14x256x14xf32, #layout61>, tensor<1x14x14x256xf32, #layout60>) -> tensor<1x14x14x256xf32, #layout60> loc(#loc786)
    %851 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x14x14x256>}> : (!tt.device<#device>) -> tensor<1x14x14x256xf32, #layout60> loc(#loc787)
    %852 = "ttnn.conv2d"(%850, %arg148, %851, %0) <{batch_size = 1 : i32, dilation_height = 1 : i32, dilation_width = 1 : i32, groups = 1 : i32, in_channels = 256 : i32, input_height = 14 : i32, input_width = 14 : i32, kernel_height = 3 : i32, kernel_width = 3 : i32, out_channels = 256 : i32, padding_height = 1 : i32, padding_width = 1 : i32, stride_height = 1 : i32, stride_width = 1 : i32}> : (tensor<1x14x14x256xf32, #layout60>, tensor<256x256x3x3xf32, #layout18>, tensor<1x14x14x256xf32, #layout60>, !tt.device<#device>) -> tensor<1x14x14x256xf32, #layout60> loc(#loc787)
    %853 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x14x256x14>}> : (!tt.device<#device>) -> tensor<1x14x256x14xf32, #layout61> loc(#loc788)
    %854 = "ttnn.transpose"(%852, %853) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x14x14x256xf32, #layout60>, tensor<1x14x256x14xf32, #layout61>) -> tensor<1x14x256x14xf32, #layout61> loc(#loc788)
    %855 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x256x14x14>}> : (!tt.device<#device>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc789)
    %856 = "ttnn.transpose"(%854, %855) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x14x256x14xf32, #layout61>, tensor<1x256x14x14xf32, #layout62>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc789)
    %857 = "ttnn.to_layout"(%arg83, %0) <{layout = #ttnn.layout<tile>}> : (tensor<256x1x1xf32, #layout2>, !tt.device<#device>) -> tensor<256x1x1xf32, #layout45> loc(#loc790)
    %858 = "ttnn.to_device"(%857, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<256x1x1xf32, #layout45>, !tt.device<#device>) -> tensor<256x1x1xf32, #layout45> loc(#loc790)
    %859 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x256x14x14>}> : (!tt.device<#device>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc790)
    %860 = "ttnn.multiply"(%856, %858, %859) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x256x14x14xf32, #layout62>, tensor<256x1x1xf32, #layout45>, tensor<1x256x14x14xf32, #layout62>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc790)
    %861 = "ttnn.to_layout"(%arg84, %0) <{layout = #ttnn.layout<tile>}> : (tensor<256x1x1xf32, #layout2>, !tt.device<#device>) -> tensor<256x1x1xf32, #layout45> loc(#loc791)
    %862 = "ttnn.to_device"(%861, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<256x1x1xf32, #layout45>, !tt.device<#device>) -> tensor<256x1x1xf32, #layout45> loc(#loc791)
    %863 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x256x14x14>}> : (!tt.device<#device>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc791)
    %864 = "ttnn.add"(%860, %862, %863) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x256x14x14xf32, #layout62>, tensor<256x1x1xf32, #layout45>, tensor<1x256x14x14xf32, #layout62>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc791)
    %865 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x256x14x14>}> : (!tt.device<#device>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc792)
    %866 = "ttnn.relu"(%864, %865) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x256x14x14xf32, #layout62>, tensor<1x256x14x14xf32, #layout62>) -> tensor<1x256x14x14xf32, #layout62> loc(#loc792)
    %867 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x14x256x14>}> : (!tt.device<#device>) -> tensor<1x14x256x14xf32, #layout61> loc(#loc793)
    %868 = "ttnn.transpose"(%866, %867) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x256x14x14xf32, #layout62>, tensor<1x14x256x14xf32, #layout61>) -> tensor<1x14x256x14xf32, #layout61> loc(#loc793)
    %869 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x14x14x256>}> : (!tt.device<#device>) -> tensor<1x14x14x256xf32, #layout60> loc(#loc794)
    %870 = "ttnn.transpose"(%868, %869) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x14x256x14xf32, #layout61>, tensor<1x14x14x256xf32, #layout60>) -> tensor<1x14x14x256xf32, #layout60> loc(#loc794)
    %871 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x14x14x1024>}> : (!tt.device<#device>) -> tensor<1x14x14x1024xf32, #layout63> loc(#loc795)
    %872 = "ttnn.conv2d"(%870, %arg149, %871, %0) <{batch_size = 1 : i32, dilation_height = 1 : i32, dilation_width = 1 : i32, groups = 1 : i32, in_channels = 256 : i32, input_height = 14 : i32, input_width = 14 : i32, kernel_height = 1 : i32, kernel_width = 1 : i32, out_channels = 1024 : i32, padding_height = 0 : i32, padding_width = 0 : i32, stride_height = 1 : i32, stride_width = 1 : i32}> : (tensor<1x14x14x256xf32, #layout60>, tensor<1024x256x1x1xf32, #layout19>, tensor<1x14x14x1024xf32, #layout63>, !tt.device<#device>) -> tensor<1x14x14x1024xf32, #layout63> loc(#loc795)
    %873 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x14x1024x14>}> : (!tt.device<#device>) -> tensor<1x14x1024x14xf32, #layout64> loc(#loc796)
    %874 = "ttnn.transpose"(%872, %873) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x14x14x1024xf32, #layout63>, tensor<1x14x1024x14xf32, #layout64>) -> tensor<1x14x1024x14xf32, #layout64> loc(#loc796)
    %875 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x1024x14x14>}> : (!tt.device<#device>) -> tensor<1x1024x14x14xf32, #layout65> loc(#loc797)
    %876 = "ttnn.transpose"(%874, %875) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x14x1024x14xf32, #layout64>, tensor<1x1024x14x14xf32, #layout65>) -> tensor<1x1024x14x14xf32, #layout65> loc(#loc797)
    %877 = "ttnn.to_layout"(%arg85, %0) <{layout = #ttnn.layout<tile>}> : (tensor<1024x1x1xf32, #layout5>, !tt.device<#device>) -> tensor<1024x1x1xf32, #layout66> loc(#loc798)
    %878 = "ttnn.to_device"(%877, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<1024x1x1xf32, #layout66>, !tt.device<#device>) -> tensor<1024x1x1xf32, #layout66> loc(#loc798)
    %879 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x1024x14x14>}> : (!tt.device<#device>) -> tensor<1x1024x14x14xf32, #layout65> loc(#loc798)
    %880 = "ttnn.multiply"(%876, %878, %879) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x1024x14x14xf32, #layout65>, tensor<1024x1x1xf32, #layout66>, tensor<1x1024x14x14xf32, #layout65>) -> tensor<1x1024x14x14xf32, #layout65> loc(#loc798)
    %881 = "ttnn.to_layout"(%arg86, %0) <{layout = #ttnn.layout<tile>}> : (tensor<1024x1x1xf32, #layout5>, !tt.device<#device>) -> tensor<1024x1x1xf32, #layout66> loc(#loc799)
    %882 = "ttnn.to_device"(%881, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<1024x1x1xf32, #layout66>, !tt.device<#device>) -> tensor<1024x1x1xf32, #layout66> loc(#loc799)
    %883 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x1024x14x14>}> : (!tt.device<#device>) -> tensor<1x1024x14x14xf32, #layout65> loc(#loc799)
    %884 = "ttnn.add"(%880, %882, %883) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x1024x14x14xf32, #layout65>, tensor<1024x1x1xf32, #layout66>, tensor<1x1024x14x14xf32, #layout65>) -> tensor<1x1024x14x14xf32, #layout65> loc(#loc799)
    %885 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x1024x14x14>}> : (!tt.device<#device>) -> tensor<1x1024x14x14xf32, #layout65> loc(#loc800)
    %886 = "ttnn.add"(%884, %826, %885) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x1024x14x14xf32, #layout65>, tensor<1x1024x14x14xf32, #layout65>, tensor<1x1024x14x14xf32, #layout65>) -> tensor<1x1024x14x14xf32, #layout65> loc(#loc800)
    %887 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x1024x14x14>}> : (!tt.device<#device>) -> tensor<1x1024x14x14xf32, #layout65> loc(#loc801)
    %888 = "ttnn.relu"(%886, %887) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x1024x14x14xf32, #layout65>, tensor<1x1024x14x14xf32, #layout65>) -> tensor<1x1024x14x14xf32, #layout65> loc(#loc801)
    %889 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x14x1024x14>}> : (!tt.device<#device>) -> tensor<1x14x1024x14xf32, #layout64> loc(#loc802)
    %890 = "ttnn.transpose"(%888, %889) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x1024x14x14xf32, #layout65>, tensor<1x14x1024x14xf32, #layout64>) -> tensor<1x14x1024x14xf32, #layout64> loc(#loc802)
    %891 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x14x14x1024>}> : (!tt.device<#device>) -> tensor<1x14x14x1024xf32, #layout63> loc(#loc803)
    %892 = "ttnn.transpose"(%890, %891) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x14x1024x14xf32, #layout64>, tensor<1x14x14x1024xf32, #layout63>) -> tensor<1x14x14x1024xf32, #layout63> loc(#loc803)
    %893 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x14x14x512>}> : (!tt.device<#device>) -> tensor<1x14x14x512xf32, #layout67> loc(#loc804)
    %894 = "ttnn.conv2d"(%892, %arg150, %893, %0) <{batch_size = 1 : i32, dilation_height = 1 : i32, dilation_width = 1 : i32, groups = 1 : i32, in_channels = 1024 : i32, input_height = 14 : i32, input_width = 14 : i32, kernel_height = 1 : i32, kernel_width = 1 : i32, out_channels = 512 : i32, padding_height = 0 : i32, padding_width = 0 : i32, stride_height = 1 : i32, stride_width = 1 : i32}> : (tensor<1x14x14x1024xf32, #layout63>, tensor<512x1024x1x1xf32, #layout22>, tensor<1x14x14x512xf32, #layout67>, !tt.device<#device>) -> tensor<1x14x14x512xf32, #layout67> loc(#loc804)
    %895 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x14x512x14>}> : (!tt.device<#device>) -> tensor<1x14x512x14xf32, #layout68> loc(#loc805)
    %896 = "ttnn.transpose"(%894, %895) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x14x14x512xf32, #layout67>, tensor<1x14x512x14xf32, #layout68>) -> tensor<1x14x512x14xf32, #layout68> loc(#loc805)
    %897 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x512x14x14>}> : (!tt.device<#device>) -> tensor<1x512x14x14xf32, #layout69> loc(#loc806)
    %898 = "ttnn.transpose"(%896, %897) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x14x512x14xf32, #layout68>, tensor<1x512x14x14xf32, #layout69>) -> tensor<1x512x14x14xf32, #layout69> loc(#loc806)
    %899 = "ttnn.to_layout"(%arg87, %0) <{layout = #ttnn.layout<tile>}> : (tensor<512x1x1xf32, #layout4>, !tt.device<#device>) -> tensor<512x1x1xf32, #layout56> loc(#loc807)
    %900 = "ttnn.to_device"(%899, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<512x1x1xf32, #layout56>, !tt.device<#device>) -> tensor<512x1x1xf32, #layout56> loc(#loc807)
    %901 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x512x14x14>}> : (!tt.device<#device>) -> tensor<1x512x14x14xf32, #layout69> loc(#loc807)
    %902 = "ttnn.multiply"(%898, %900, %901) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x512x14x14xf32, #layout69>, tensor<512x1x1xf32, #layout56>, tensor<1x512x14x14xf32, #layout69>) -> tensor<1x512x14x14xf32, #layout69> loc(#loc807)
    %903 = "ttnn.to_layout"(%arg88, %0) <{layout = #ttnn.layout<tile>}> : (tensor<512x1x1xf32, #layout4>, !tt.device<#device>) -> tensor<512x1x1xf32, #layout56> loc(#loc808)
    %904 = "ttnn.to_device"(%903, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<512x1x1xf32, #layout56>, !tt.device<#device>) -> tensor<512x1x1xf32, #layout56> loc(#loc808)
    %905 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x512x14x14>}> : (!tt.device<#device>) -> tensor<1x512x14x14xf32, #layout69> loc(#loc808)
    %906 = "ttnn.add"(%902, %904, %905) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x512x14x14xf32, #layout69>, tensor<512x1x1xf32, #layout56>, tensor<1x512x14x14xf32, #layout69>) -> tensor<1x512x14x14xf32, #layout69> loc(#loc808)
    %907 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x512x14x14>}> : (!tt.device<#device>) -> tensor<1x512x14x14xf32, #layout69> loc(#loc809)
    %908 = "ttnn.relu"(%906, %907) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x512x14x14xf32, #layout69>, tensor<1x512x14x14xf32, #layout69>) -> tensor<1x512x14x14xf32, #layout69> loc(#loc809)
    %909 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x14x512x14>}> : (!tt.device<#device>) -> tensor<1x14x512x14xf32, #layout68> loc(#loc810)
    %910 = "ttnn.transpose"(%908, %909) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x512x14x14xf32, #layout69>, tensor<1x14x512x14xf32, #layout68>) -> tensor<1x14x512x14xf32, #layout68> loc(#loc810)
    %911 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x14x14x512>}> : (!tt.device<#device>) -> tensor<1x14x14x512xf32, #layout67> loc(#loc811)
    %912 = "ttnn.transpose"(%910, %911) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x14x512x14xf32, #layout68>, tensor<1x14x14x512xf32, #layout67>) -> tensor<1x14x14x512xf32, #layout67> loc(#loc811)
    %913 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x7x7x512>}> : (!tt.device<#device>) -> tensor<1x7x7x512xf32, #layout70> loc(#loc812)
    %914 = "ttnn.conv2d"(%912, %arg151, %913, %0) <{batch_size = 1 : i32, dilation_height = 1 : i32, dilation_width = 1 : i32, groups = 1 : i32, in_channels = 512 : i32, input_height = 14 : i32, input_width = 14 : i32, kernel_height = 3 : i32, kernel_width = 3 : i32, out_channels = 512 : i32, padding_height = 1 : i32, padding_width = 1 : i32, stride_height = 2 : i32, stride_width = 2 : i32}> : (tensor<1x14x14x512xf32, #layout67>, tensor<512x512x3x3xf32, #layout23>, tensor<1x7x7x512xf32, #layout70>, !tt.device<#device>) -> tensor<1x7x7x512xf32, #layout70> loc(#loc812)
    %915 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x7x512x7>}> : (!tt.device<#device>) -> tensor<1x7x512x7xf32, #layout71> loc(#loc813)
    %916 = "ttnn.transpose"(%914, %915) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x7x7x512xf32, #layout70>, tensor<1x7x512x7xf32, #layout71>) -> tensor<1x7x512x7xf32, #layout71> loc(#loc813)
    %917 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x512x7x7>}> : (!tt.device<#device>) -> tensor<1x512x7x7xf32, #layout72> loc(#loc814)
    %918 = "ttnn.transpose"(%916, %917) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x7x512x7xf32, #layout71>, tensor<1x512x7x7xf32, #layout72>) -> tensor<1x512x7x7xf32, #layout72> loc(#loc814)
    %919 = "ttnn.to_layout"(%arg89, %0) <{layout = #ttnn.layout<tile>}> : (tensor<512x1x1xf32, #layout4>, !tt.device<#device>) -> tensor<512x1x1xf32, #layout56> loc(#loc815)
    %920 = "ttnn.to_device"(%919, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<512x1x1xf32, #layout56>, !tt.device<#device>) -> tensor<512x1x1xf32, #layout56> loc(#loc815)
    %921 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x512x7x7>}> : (!tt.device<#device>) -> tensor<1x512x7x7xf32, #layout72> loc(#loc815)
    %922 = "ttnn.multiply"(%918, %920, %921) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x512x7x7xf32, #layout72>, tensor<512x1x1xf32, #layout56>, tensor<1x512x7x7xf32, #layout72>) -> tensor<1x512x7x7xf32, #layout72> loc(#loc815)
    %923 = "ttnn.to_layout"(%arg90, %0) <{layout = #ttnn.layout<tile>}> : (tensor<512x1x1xf32, #layout4>, !tt.device<#device>) -> tensor<512x1x1xf32, #layout56> loc(#loc816)
    %924 = "ttnn.to_device"(%923, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<512x1x1xf32, #layout56>, !tt.device<#device>) -> tensor<512x1x1xf32, #layout56> loc(#loc816)
    %925 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x512x7x7>}> : (!tt.device<#device>) -> tensor<1x512x7x7xf32, #layout72> loc(#loc816)
    %926 = "ttnn.add"(%922, %924, %925) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x512x7x7xf32, #layout72>, tensor<512x1x1xf32, #layout56>, tensor<1x512x7x7xf32, #layout72>) -> tensor<1x512x7x7xf32, #layout72> loc(#loc816)
    %927 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x512x7x7>}> : (!tt.device<#device>) -> tensor<1x512x7x7xf32, #layout72> loc(#loc817)
    %928 = "ttnn.relu"(%926, %927) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x512x7x7xf32, #layout72>, tensor<1x512x7x7xf32, #layout72>) -> tensor<1x512x7x7xf32, #layout72> loc(#loc817)
    %929 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x7x512x7>}> : (!tt.device<#device>) -> tensor<1x7x512x7xf32, #layout71> loc(#loc818)
    %930 = "ttnn.transpose"(%928, %929) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x512x7x7xf32, #layout72>, tensor<1x7x512x7xf32, #layout71>) -> tensor<1x7x512x7xf32, #layout71> loc(#loc818)
    %931 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x7x7x512>}> : (!tt.device<#device>) -> tensor<1x7x7x512xf32, #layout70> loc(#loc819)
    %932 = "ttnn.transpose"(%930, %931) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x7x512x7xf32, #layout71>, tensor<1x7x7x512xf32, #layout70>) -> tensor<1x7x7x512xf32, #layout70> loc(#loc819)
    %933 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x7x7x2048>}> : (!tt.device<#device>) -> tensor<1x7x7x2048xf32, #layout73> loc(#loc820)
    %934 = "ttnn.conv2d"(%932, %arg152, %933, %0) <{batch_size = 1 : i32, dilation_height = 1 : i32, dilation_width = 1 : i32, groups = 1 : i32, in_channels = 512 : i32, input_height = 7 : i32, input_width = 7 : i32, kernel_height = 1 : i32, kernel_width = 1 : i32, out_channels = 2048 : i32, padding_height = 0 : i32, padding_width = 0 : i32, stride_height = 1 : i32, stride_width = 1 : i32}> : (tensor<1x7x7x512xf32, #layout70>, tensor<2048x512x1x1xf32, #layout24>, tensor<1x7x7x2048xf32, #layout73>, !tt.device<#device>) -> tensor<1x7x7x2048xf32, #layout73> loc(#loc820)
    %935 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x7x2048x7>}> : (!tt.device<#device>) -> tensor<1x7x2048x7xf32, #layout74> loc(#loc821)
    %936 = "ttnn.transpose"(%934, %935) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x7x7x2048xf32, #layout73>, tensor<1x7x2048x7xf32, #layout74>) -> tensor<1x7x2048x7xf32, #layout74> loc(#loc821)
    %937 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x2048x7x7>}> : (!tt.device<#device>) -> tensor<1x2048x7x7xf32, #layout75> loc(#loc822)
    %938 = "ttnn.transpose"(%936, %937) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x7x2048x7xf32, #layout74>, tensor<1x2048x7x7xf32, #layout75>) -> tensor<1x2048x7x7xf32, #layout75> loc(#loc822)
    %939 = "ttnn.to_layout"(%arg91, %0) <{layout = #ttnn.layout<tile>}> : (tensor<2048x1x1xf32, #layout6>, !tt.device<#device>) -> tensor<2048x1x1xf32, #layout76> loc(#loc823)
    %940 = "ttnn.to_device"(%939, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<2048x1x1xf32, #layout76>, !tt.device<#device>) -> tensor<2048x1x1xf32, #layout76> loc(#loc823)
    %941 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x2048x7x7>}> : (!tt.device<#device>) -> tensor<1x2048x7x7xf32, #layout75> loc(#loc823)
    %942 = "ttnn.multiply"(%938, %940, %941) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x2048x7x7xf32, #layout75>, tensor<2048x1x1xf32, #layout76>, tensor<1x2048x7x7xf32, #layout75>) -> tensor<1x2048x7x7xf32, #layout75> loc(#loc823)
    %943 = "ttnn.to_layout"(%arg92, %0) <{layout = #ttnn.layout<tile>}> : (tensor<2048x1x1xf32, #layout6>, !tt.device<#device>) -> tensor<2048x1x1xf32, #layout76> loc(#loc824)
    %944 = "ttnn.to_device"(%943, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<2048x1x1xf32, #layout76>, !tt.device<#device>) -> tensor<2048x1x1xf32, #layout76> loc(#loc824)
    %945 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x2048x7x7>}> : (!tt.device<#device>) -> tensor<1x2048x7x7xf32, #layout75> loc(#loc824)
    %946 = "ttnn.add"(%942, %944, %945) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x2048x7x7xf32, #layout75>, tensor<2048x1x1xf32, #layout76>, tensor<1x2048x7x7xf32, #layout75>) -> tensor<1x2048x7x7xf32, #layout75> loc(#loc824)
    %947 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x14x1024x14>}> : (!tt.device<#device>) -> tensor<1x14x1024x14xf32, #layout64> loc(#loc825)
    %948 = "ttnn.transpose"(%888, %947) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x1024x14x14xf32, #layout65>, tensor<1x14x1024x14xf32, #layout64>) -> tensor<1x14x1024x14xf32, #layout64> loc(#loc825)
    %949 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x14x14x1024>}> : (!tt.device<#device>) -> tensor<1x14x14x1024xf32, #layout63> loc(#loc826)
    %950 = "ttnn.transpose"(%948, %949) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x14x1024x14xf32, #layout64>, tensor<1x14x14x1024xf32, #layout63>) -> tensor<1x14x14x1024xf32, #layout63> loc(#loc826)
    %951 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x7x7x2048>}> : (!tt.device<#device>) -> tensor<1x7x7x2048xf32, #layout73> loc(#loc827)
    %952 = "ttnn.conv2d"(%950, %arg153, %951, %0) <{batch_size = 1 : i32, dilation_height = 1 : i32, dilation_width = 1 : i32, groups = 1 : i32, in_channels = 1024 : i32, input_height = 14 : i32, input_width = 14 : i32, kernel_height = 1 : i32, kernel_width = 1 : i32, out_channels = 2048 : i32, padding_height = 0 : i32, padding_width = 0 : i32, stride_height = 2 : i32, stride_width = 2 : i32}> : (tensor<1x14x14x1024xf32, #layout63>, tensor<2048x1024x1x1xf32, #layout25>, tensor<1x7x7x2048xf32, #layout73>, !tt.device<#device>) -> tensor<1x7x7x2048xf32, #layout73> loc(#loc827)
    %953 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x7x2048x7>}> : (!tt.device<#device>) -> tensor<1x7x2048x7xf32, #layout74> loc(#loc828)
    %954 = "ttnn.transpose"(%952, %953) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x7x7x2048xf32, #layout73>, tensor<1x7x2048x7xf32, #layout74>) -> tensor<1x7x2048x7xf32, #layout74> loc(#loc828)
    %955 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x2048x7x7>}> : (!tt.device<#device>) -> tensor<1x2048x7x7xf32, #layout75> loc(#loc829)
    %956 = "ttnn.transpose"(%954, %955) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x7x2048x7xf32, #layout74>, tensor<1x2048x7x7xf32, #layout75>) -> tensor<1x2048x7x7xf32, #layout75> loc(#loc829)
    %957 = "ttnn.to_layout"(%arg93, %0) <{layout = #ttnn.layout<tile>}> : (tensor<2048x1x1xf32, #layout6>, !tt.device<#device>) -> tensor<2048x1x1xf32, #layout76> loc(#loc830)
    %958 = "ttnn.to_device"(%957, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<2048x1x1xf32, #layout76>, !tt.device<#device>) -> tensor<2048x1x1xf32, #layout76> loc(#loc830)
    %959 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x2048x7x7>}> : (!tt.device<#device>) -> tensor<1x2048x7x7xf32, #layout75> loc(#loc830)
    %960 = "ttnn.multiply"(%956, %958, %959) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x2048x7x7xf32, #layout75>, tensor<2048x1x1xf32, #layout76>, tensor<1x2048x7x7xf32, #layout75>) -> tensor<1x2048x7x7xf32, #layout75> loc(#loc830)
    %961 = "ttnn.to_layout"(%arg94, %0) <{layout = #ttnn.layout<tile>}> : (tensor<2048x1x1xf32, #layout6>, !tt.device<#device>) -> tensor<2048x1x1xf32, #layout76> loc(#loc831)
    %962 = "ttnn.to_device"(%961, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<2048x1x1xf32, #layout76>, !tt.device<#device>) -> tensor<2048x1x1xf32, #layout76> loc(#loc831)
    %963 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x2048x7x7>}> : (!tt.device<#device>) -> tensor<1x2048x7x7xf32, #layout75> loc(#loc831)
    %964 = "ttnn.add"(%960, %962, %963) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x2048x7x7xf32, #layout75>, tensor<2048x1x1xf32, #layout76>, tensor<1x2048x7x7xf32, #layout75>) -> tensor<1x2048x7x7xf32, #layout75> loc(#loc831)
    %965 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x2048x7x7>}> : (!tt.device<#device>) -> tensor<1x2048x7x7xf32, #layout75> loc(#loc832)
    %966 = "ttnn.add"(%946, %964, %965) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x2048x7x7xf32, #layout75>, tensor<1x2048x7x7xf32, #layout75>, tensor<1x2048x7x7xf32, #layout75>) -> tensor<1x2048x7x7xf32, #layout75> loc(#loc832)
    %967 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x2048x7x7>}> : (!tt.device<#device>) -> tensor<1x2048x7x7xf32, #layout75> loc(#loc833)
    %968 = "ttnn.relu"(%966, %967) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x2048x7x7xf32, #layout75>, tensor<1x2048x7x7xf32, #layout75>) -> tensor<1x2048x7x7xf32, #layout75> loc(#loc833)
    %969 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x7x2048x7>}> : (!tt.device<#device>) -> tensor<1x7x2048x7xf32, #layout74> loc(#loc834)
    %970 = "ttnn.transpose"(%968, %969) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x2048x7x7xf32, #layout75>, tensor<1x7x2048x7xf32, #layout74>) -> tensor<1x7x2048x7xf32, #layout74> loc(#loc834)
    %971 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x7x7x2048>}> : (!tt.device<#device>) -> tensor<1x7x7x2048xf32, #layout73> loc(#loc835)
    %972 = "ttnn.transpose"(%970, %971) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x7x2048x7xf32, #layout74>, tensor<1x7x7x2048xf32, #layout73>) -> tensor<1x7x7x2048xf32, #layout73> loc(#loc835)
    %973 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x7x7x512>}> : (!tt.device<#device>) -> tensor<1x7x7x512xf32, #layout70> loc(#loc836)
    %974 = "ttnn.conv2d"(%972, %arg154, %973, %0) <{batch_size = 1 : i32, dilation_height = 1 : i32, dilation_width = 1 : i32, groups = 1 : i32, in_channels = 2048 : i32, input_height = 7 : i32, input_width = 7 : i32, kernel_height = 1 : i32, kernel_width = 1 : i32, out_channels = 512 : i32, padding_height = 0 : i32, padding_width = 0 : i32, stride_height = 1 : i32, stride_width = 1 : i32}> : (tensor<1x7x7x2048xf32, #layout73>, tensor<512x2048x1x1xf32, #layout26>, tensor<1x7x7x512xf32, #layout70>, !tt.device<#device>) -> tensor<1x7x7x512xf32, #layout70> loc(#loc836)
    %975 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x7x512x7>}> : (!tt.device<#device>) -> tensor<1x7x512x7xf32, #layout71> loc(#loc837)
    %976 = "ttnn.transpose"(%974, %975) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x7x7x512xf32, #layout70>, tensor<1x7x512x7xf32, #layout71>) -> tensor<1x7x512x7xf32, #layout71> loc(#loc837)
    %977 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x512x7x7>}> : (!tt.device<#device>) -> tensor<1x512x7x7xf32, #layout72> loc(#loc838)
    %978 = "ttnn.transpose"(%976, %977) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x7x512x7xf32, #layout71>, tensor<1x512x7x7xf32, #layout72>) -> tensor<1x512x7x7xf32, #layout72> loc(#loc838)
    %979 = "ttnn.to_layout"(%arg95, %0) <{layout = #ttnn.layout<tile>}> : (tensor<512x1x1xf32, #layout4>, !tt.device<#device>) -> tensor<512x1x1xf32, #layout56> loc(#loc839)
    %980 = "ttnn.to_device"(%979, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<512x1x1xf32, #layout56>, !tt.device<#device>) -> tensor<512x1x1xf32, #layout56> loc(#loc839)
    %981 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x512x7x7>}> : (!tt.device<#device>) -> tensor<1x512x7x7xf32, #layout72> loc(#loc839)
    %982 = "ttnn.multiply"(%978, %980, %981) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x512x7x7xf32, #layout72>, tensor<512x1x1xf32, #layout56>, tensor<1x512x7x7xf32, #layout72>) -> tensor<1x512x7x7xf32, #layout72> loc(#loc839)
    %983 = "ttnn.to_layout"(%arg96, %0) <{layout = #ttnn.layout<tile>}> : (tensor<512x1x1xf32, #layout4>, !tt.device<#device>) -> tensor<512x1x1xf32, #layout56> loc(#loc840)
    %984 = "ttnn.to_device"(%983, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<512x1x1xf32, #layout56>, !tt.device<#device>) -> tensor<512x1x1xf32, #layout56> loc(#loc840)
    %985 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x512x7x7>}> : (!tt.device<#device>) -> tensor<1x512x7x7xf32, #layout72> loc(#loc840)
    %986 = "ttnn.add"(%982, %984, %985) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x512x7x7xf32, #layout72>, tensor<512x1x1xf32, #layout56>, tensor<1x512x7x7xf32, #layout72>) -> tensor<1x512x7x7xf32, #layout72> loc(#loc840)
    %987 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x512x7x7>}> : (!tt.device<#device>) -> tensor<1x512x7x7xf32, #layout72> loc(#loc841)
    %988 = "ttnn.relu"(%986, %987) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x512x7x7xf32, #layout72>, tensor<1x512x7x7xf32, #layout72>) -> tensor<1x512x7x7xf32, #layout72> loc(#loc841)
    %989 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x7x512x7>}> : (!tt.device<#device>) -> tensor<1x7x512x7xf32, #layout71> loc(#loc842)
    %990 = "ttnn.transpose"(%988, %989) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x512x7x7xf32, #layout72>, tensor<1x7x512x7xf32, #layout71>) -> tensor<1x7x512x7xf32, #layout71> loc(#loc842)
    %991 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x7x7x512>}> : (!tt.device<#device>) -> tensor<1x7x7x512xf32, #layout70> loc(#loc843)
    %992 = "ttnn.transpose"(%990, %991) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x7x512x7xf32, #layout71>, tensor<1x7x7x512xf32, #layout70>) -> tensor<1x7x7x512xf32, #layout70> loc(#loc843)
    %993 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x7x7x512>}> : (!tt.device<#device>) -> tensor<1x7x7x512xf32, #layout70> loc(#loc844)
    %994 = "ttnn.conv2d"(%992, %arg155, %993, %0) <{batch_size = 1 : i32, dilation_height = 1 : i32, dilation_width = 1 : i32, groups = 1 : i32, in_channels = 512 : i32, input_height = 7 : i32, input_width = 7 : i32, kernel_height = 3 : i32, kernel_width = 3 : i32, out_channels = 512 : i32, padding_height = 1 : i32, padding_width = 1 : i32, stride_height = 1 : i32, stride_width = 1 : i32}> : (tensor<1x7x7x512xf32, #layout70>, tensor<512x512x3x3xf32, #layout23>, tensor<1x7x7x512xf32, #layout70>, !tt.device<#device>) -> tensor<1x7x7x512xf32, #layout70> loc(#loc844)
    %995 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x7x512x7>}> : (!tt.device<#device>) -> tensor<1x7x512x7xf32, #layout71> loc(#loc845)
    %996 = "ttnn.transpose"(%994, %995) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x7x7x512xf32, #layout70>, tensor<1x7x512x7xf32, #layout71>) -> tensor<1x7x512x7xf32, #layout71> loc(#loc845)
    %997 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x512x7x7>}> : (!tt.device<#device>) -> tensor<1x512x7x7xf32, #layout72> loc(#loc846)
    %998 = "ttnn.transpose"(%996, %997) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x7x512x7xf32, #layout71>, tensor<1x512x7x7xf32, #layout72>) -> tensor<1x512x7x7xf32, #layout72> loc(#loc846)
    %999 = "ttnn.to_layout"(%arg97, %0) <{layout = #ttnn.layout<tile>}> : (tensor<512x1x1xf32, #layout4>, !tt.device<#device>) -> tensor<512x1x1xf32, #layout56> loc(#loc847)
    %1000 = "ttnn.to_device"(%999, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<512x1x1xf32, #layout56>, !tt.device<#device>) -> tensor<512x1x1xf32, #layout56> loc(#loc847)
    %1001 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x512x7x7>}> : (!tt.device<#device>) -> tensor<1x512x7x7xf32, #layout72> loc(#loc847)
    %1002 = "ttnn.multiply"(%998, %1000, %1001) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x512x7x7xf32, #layout72>, tensor<512x1x1xf32, #layout56>, tensor<1x512x7x7xf32, #layout72>) -> tensor<1x512x7x7xf32, #layout72> loc(#loc847)
    %1003 = "ttnn.to_layout"(%arg98, %0) <{layout = #ttnn.layout<tile>}> : (tensor<512x1x1xf32, #layout4>, !tt.device<#device>) -> tensor<512x1x1xf32, #layout56> loc(#loc848)
    %1004 = "ttnn.to_device"(%1003, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<512x1x1xf32, #layout56>, !tt.device<#device>) -> tensor<512x1x1xf32, #layout56> loc(#loc848)
    %1005 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x512x7x7>}> : (!tt.device<#device>) -> tensor<1x512x7x7xf32, #layout72> loc(#loc848)
    %1006 = "ttnn.add"(%1002, %1004, %1005) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x512x7x7xf32, #layout72>, tensor<512x1x1xf32, #layout56>, tensor<1x512x7x7xf32, #layout72>) -> tensor<1x512x7x7xf32, #layout72> loc(#loc848)
    %1007 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x512x7x7>}> : (!tt.device<#device>) -> tensor<1x512x7x7xf32, #layout72> loc(#loc849)
    %1008 = "ttnn.relu"(%1006, %1007) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x512x7x7xf32, #layout72>, tensor<1x512x7x7xf32, #layout72>) -> tensor<1x512x7x7xf32, #layout72> loc(#loc849)
    %1009 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x7x512x7>}> : (!tt.device<#device>) -> tensor<1x7x512x7xf32, #layout71> loc(#loc850)
    %1010 = "ttnn.transpose"(%1008, %1009) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x512x7x7xf32, #layout72>, tensor<1x7x512x7xf32, #layout71>) -> tensor<1x7x512x7xf32, #layout71> loc(#loc850)
    %1011 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x7x7x512>}> : (!tt.device<#device>) -> tensor<1x7x7x512xf32, #layout70> loc(#loc851)
    %1012 = "ttnn.transpose"(%1010, %1011) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x7x512x7xf32, #layout71>, tensor<1x7x7x512xf32, #layout70>) -> tensor<1x7x7x512xf32, #layout70> loc(#loc851)
    %1013 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x7x7x2048>}> : (!tt.device<#device>) -> tensor<1x7x7x2048xf32, #layout73> loc(#loc852)
    %1014 = "ttnn.conv2d"(%1012, %arg156, %1013, %0) <{batch_size = 1 : i32, dilation_height = 1 : i32, dilation_width = 1 : i32, groups = 1 : i32, in_channels = 512 : i32, input_height = 7 : i32, input_width = 7 : i32, kernel_height = 1 : i32, kernel_width = 1 : i32, out_channels = 2048 : i32, padding_height = 0 : i32, padding_width = 0 : i32, stride_height = 1 : i32, stride_width = 1 : i32}> : (tensor<1x7x7x512xf32, #layout70>, tensor<2048x512x1x1xf32, #layout24>, tensor<1x7x7x2048xf32, #layout73>, !tt.device<#device>) -> tensor<1x7x7x2048xf32, #layout73> loc(#loc852)
    %1015 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x7x2048x7>}> : (!tt.device<#device>) -> tensor<1x7x2048x7xf32, #layout74> loc(#loc853)
    %1016 = "ttnn.transpose"(%1014, %1015) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x7x7x2048xf32, #layout73>, tensor<1x7x2048x7xf32, #layout74>) -> tensor<1x7x2048x7xf32, #layout74> loc(#loc853)
    %1017 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x2048x7x7>}> : (!tt.device<#device>) -> tensor<1x2048x7x7xf32, #layout75> loc(#loc854)
    %1018 = "ttnn.transpose"(%1016, %1017) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x7x2048x7xf32, #layout74>, tensor<1x2048x7x7xf32, #layout75>) -> tensor<1x2048x7x7xf32, #layout75> loc(#loc854)
    %1019 = "ttnn.to_layout"(%arg99, %0) <{layout = #ttnn.layout<tile>}> : (tensor<2048x1x1xf32, #layout6>, !tt.device<#device>) -> tensor<2048x1x1xf32, #layout76> loc(#loc855)
    %1020 = "ttnn.to_device"(%1019, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<2048x1x1xf32, #layout76>, !tt.device<#device>) -> tensor<2048x1x1xf32, #layout76> loc(#loc855)
    %1021 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x2048x7x7>}> : (!tt.device<#device>) -> tensor<1x2048x7x7xf32, #layout75> loc(#loc855)
    %1022 = "ttnn.multiply"(%1018, %1020, %1021) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x2048x7x7xf32, #layout75>, tensor<2048x1x1xf32, #layout76>, tensor<1x2048x7x7xf32, #layout75>) -> tensor<1x2048x7x7xf32, #layout75> loc(#loc855)
    %1023 = "ttnn.to_layout"(%arg100, %0) <{layout = #ttnn.layout<tile>}> : (tensor<2048x1x1xf32, #layout6>, !tt.device<#device>) -> tensor<2048x1x1xf32, #layout76> loc(#loc856)
    %1024 = "ttnn.to_device"(%1023, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<2048x1x1xf32, #layout76>, !tt.device<#device>) -> tensor<2048x1x1xf32, #layout76> loc(#loc856)
    %1025 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x2048x7x7>}> : (!tt.device<#device>) -> tensor<1x2048x7x7xf32, #layout75> loc(#loc856)
    %1026 = "ttnn.add"(%1022, %1024, %1025) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x2048x7x7xf32, #layout75>, tensor<2048x1x1xf32, #layout76>, tensor<1x2048x7x7xf32, #layout75>) -> tensor<1x2048x7x7xf32, #layout75> loc(#loc856)
    %1027 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x2048x7x7>}> : (!tt.device<#device>) -> tensor<1x2048x7x7xf32, #layout75> loc(#loc857)
    %1028 = "ttnn.add"(%1026, %968, %1027) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x2048x7x7xf32, #layout75>, tensor<1x2048x7x7xf32, #layout75>, tensor<1x2048x7x7xf32, #layout75>) -> tensor<1x2048x7x7xf32, #layout75> loc(#loc857)
    %1029 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x2048x7x7>}> : (!tt.device<#device>) -> tensor<1x2048x7x7xf32, #layout75> loc(#loc858)
    %1030 = "ttnn.relu"(%1028, %1029) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x2048x7x7xf32, #layout75>, tensor<1x2048x7x7xf32, #layout75>) -> tensor<1x2048x7x7xf32, #layout75> loc(#loc858)
    %1031 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x7x2048x7>}> : (!tt.device<#device>) -> tensor<1x7x2048x7xf32, #layout74> loc(#loc859)
    %1032 = "ttnn.transpose"(%1030, %1031) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x2048x7x7xf32, #layout75>, tensor<1x7x2048x7xf32, #layout74>) -> tensor<1x7x2048x7xf32, #layout74> loc(#loc859)
    %1033 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x7x7x2048>}> : (!tt.device<#device>) -> tensor<1x7x7x2048xf32, #layout73> loc(#loc860)
    %1034 = "ttnn.transpose"(%1032, %1033) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x7x2048x7xf32, #layout74>, tensor<1x7x7x2048xf32, #layout73>) -> tensor<1x7x7x2048xf32, #layout73> loc(#loc860)
    %1035 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x7x7x512>}> : (!tt.device<#device>) -> tensor<1x7x7x512xf32, #layout70> loc(#loc861)
    %1036 = "ttnn.conv2d"(%1034, %arg157, %1035, %0) <{batch_size = 1 : i32, dilation_height = 1 : i32, dilation_width = 1 : i32, groups = 1 : i32, in_channels = 2048 : i32, input_height = 7 : i32, input_width = 7 : i32, kernel_height = 1 : i32, kernel_width = 1 : i32, out_channels = 512 : i32, padding_height = 0 : i32, padding_width = 0 : i32, stride_height = 1 : i32, stride_width = 1 : i32}> : (tensor<1x7x7x2048xf32, #layout73>, tensor<512x2048x1x1xf32, #layout26>, tensor<1x7x7x512xf32, #layout70>, !tt.device<#device>) -> tensor<1x7x7x512xf32, #layout70> loc(#loc861)
    %1037 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x7x512x7>}> : (!tt.device<#device>) -> tensor<1x7x512x7xf32, #layout71> loc(#loc862)
    %1038 = "ttnn.transpose"(%1036, %1037) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x7x7x512xf32, #layout70>, tensor<1x7x512x7xf32, #layout71>) -> tensor<1x7x512x7xf32, #layout71> loc(#loc862)
    %1039 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x512x7x7>}> : (!tt.device<#device>) -> tensor<1x512x7x7xf32, #layout72> loc(#loc863)
    %1040 = "ttnn.transpose"(%1038, %1039) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x7x512x7xf32, #layout71>, tensor<1x512x7x7xf32, #layout72>) -> tensor<1x512x7x7xf32, #layout72> loc(#loc863)
    %1041 = "ttnn.to_layout"(%arg101, %0) <{layout = #ttnn.layout<tile>}> : (tensor<512x1x1xf32, #layout4>, !tt.device<#device>) -> tensor<512x1x1xf32, #layout56> loc(#loc864)
    %1042 = "ttnn.to_device"(%1041, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<512x1x1xf32, #layout56>, !tt.device<#device>) -> tensor<512x1x1xf32, #layout56> loc(#loc864)
    %1043 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x512x7x7>}> : (!tt.device<#device>) -> tensor<1x512x7x7xf32, #layout72> loc(#loc864)
    %1044 = "ttnn.multiply"(%1040, %1042, %1043) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x512x7x7xf32, #layout72>, tensor<512x1x1xf32, #layout56>, tensor<1x512x7x7xf32, #layout72>) -> tensor<1x512x7x7xf32, #layout72> loc(#loc864)
    %1045 = "ttnn.to_layout"(%arg102, %0) <{layout = #ttnn.layout<tile>}> : (tensor<512x1x1xf32, #layout4>, !tt.device<#device>) -> tensor<512x1x1xf32, #layout56> loc(#loc865)
    %1046 = "ttnn.to_device"(%1045, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<512x1x1xf32, #layout56>, !tt.device<#device>) -> tensor<512x1x1xf32, #layout56> loc(#loc865)
    %1047 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x512x7x7>}> : (!tt.device<#device>) -> tensor<1x512x7x7xf32, #layout72> loc(#loc865)
    %1048 = "ttnn.add"(%1044, %1046, %1047) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x512x7x7xf32, #layout72>, tensor<512x1x1xf32, #layout56>, tensor<1x512x7x7xf32, #layout72>) -> tensor<1x512x7x7xf32, #layout72> loc(#loc865)
    %1049 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x512x7x7>}> : (!tt.device<#device>) -> tensor<1x512x7x7xf32, #layout72> loc(#loc866)
    %1050 = "ttnn.relu"(%1048, %1049) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x512x7x7xf32, #layout72>, tensor<1x512x7x7xf32, #layout72>) -> tensor<1x512x7x7xf32, #layout72> loc(#loc866)
    %1051 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x7x512x7>}> : (!tt.device<#device>) -> tensor<1x7x512x7xf32, #layout71> loc(#loc867)
    %1052 = "ttnn.transpose"(%1050, %1051) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x512x7x7xf32, #layout72>, tensor<1x7x512x7xf32, #layout71>) -> tensor<1x7x512x7xf32, #layout71> loc(#loc867)
    %1053 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x7x7x512>}> : (!tt.device<#device>) -> tensor<1x7x7x512xf32, #layout70> loc(#loc868)
    %1054 = "ttnn.transpose"(%1052, %1053) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x7x512x7xf32, #layout71>, tensor<1x7x7x512xf32, #layout70>) -> tensor<1x7x7x512xf32, #layout70> loc(#loc868)
    %1055 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x7x7x512>}> : (!tt.device<#device>) -> tensor<1x7x7x512xf32, #layout70> loc(#loc869)
    %1056 = "ttnn.conv2d"(%1054, %arg158, %1055, %0) <{batch_size = 1 : i32, dilation_height = 1 : i32, dilation_width = 1 : i32, groups = 1 : i32, in_channels = 512 : i32, input_height = 7 : i32, input_width = 7 : i32, kernel_height = 3 : i32, kernel_width = 3 : i32, out_channels = 512 : i32, padding_height = 1 : i32, padding_width = 1 : i32, stride_height = 1 : i32, stride_width = 1 : i32}> : (tensor<1x7x7x512xf32, #layout70>, tensor<512x512x3x3xf32, #layout23>, tensor<1x7x7x512xf32, #layout70>, !tt.device<#device>) -> tensor<1x7x7x512xf32, #layout70> loc(#loc869)
    %1057 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x7x512x7>}> : (!tt.device<#device>) -> tensor<1x7x512x7xf32, #layout71> loc(#loc870)
    %1058 = "ttnn.transpose"(%1056, %1057) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x7x7x512xf32, #layout70>, tensor<1x7x512x7xf32, #layout71>) -> tensor<1x7x512x7xf32, #layout71> loc(#loc870)
    %1059 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x512x7x7>}> : (!tt.device<#device>) -> tensor<1x512x7x7xf32, #layout72> loc(#loc871)
    %1060 = "ttnn.transpose"(%1058, %1059) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x7x512x7xf32, #layout71>, tensor<1x512x7x7xf32, #layout72>) -> tensor<1x512x7x7xf32, #layout72> loc(#loc871)
    %1061 = "ttnn.to_layout"(%arg103, %0) <{layout = #ttnn.layout<tile>}> : (tensor<512x1x1xf32, #layout4>, !tt.device<#device>) -> tensor<512x1x1xf32, #layout56> loc(#loc872)
    %1062 = "ttnn.to_device"(%1061, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<512x1x1xf32, #layout56>, !tt.device<#device>) -> tensor<512x1x1xf32, #layout56> loc(#loc872)
    %1063 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x512x7x7>}> : (!tt.device<#device>) -> tensor<1x512x7x7xf32, #layout72> loc(#loc872)
    %1064 = "ttnn.multiply"(%1060, %1062, %1063) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x512x7x7xf32, #layout72>, tensor<512x1x1xf32, #layout56>, tensor<1x512x7x7xf32, #layout72>) -> tensor<1x512x7x7xf32, #layout72> loc(#loc872)
    %1065 = "ttnn.to_layout"(%arg104, %0) <{layout = #ttnn.layout<tile>}> : (tensor<512x1x1xf32, #layout4>, !tt.device<#device>) -> tensor<512x1x1xf32, #layout56> loc(#loc873)
    %1066 = "ttnn.to_device"(%1065, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<512x1x1xf32, #layout56>, !tt.device<#device>) -> tensor<512x1x1xf32, #layout56> loc(#loc873)
    %1067 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x512x7x7>}> : (!tt.device<#device>) -> tensor<1x512x7x7xf32, #layout72> loc(#loc873)
    %1068 = "ttnn.add"(%1064, %1066, %1067) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x512x7x7xf32, #layout72>, tensor<512x1x1xf32, #layout56>, tensor<1x512x7x7xf32, #layout72>) -> tensor<1x512x7x7xf32, #layout72> loc(#loc873)
    %1069 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x512x7x7>}> : (!tt.device<#device>) -> tensor<1x512x7x7xf32, #layout72> loc(#loc874)
    %1070 = "ttnn.relu"(%1068, %1069) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x512x7x7xf32, #layout72>, tensor<1x512x7x7xf32, #layout72>) -> tensor<1x512x7x7xf32, #layout72> loc(#loc874)
    %1071 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x7x512x7>}> : (!tt.device<#device>) -> tensor<1x7x512x7xf32, #layout71> loc(#loc875)
    %1072 = "ttnn.transpose"(%1070, %1071) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x512x7x7xf32, #layout72>, tensor<1x7x512x7xf32, #layout71>) -> tensor<1x7x512x7xf32, #layout71> loc(#loc875)
    %1073 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x7x7x512>}> : (!tt.device<#device>) -> tensor<1x7x7x512xf32, #layout70> loc(#loc876)
    %1074 = "ttnn.transpose"(%1072, %1073) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x7x512x7xf32, #layout71>, tensor<1x7x7x512xf32, #layout70>) -> tensor<1x7x7x512xf32, #layout70> loc(#loc876)
    %1075 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x7x7x2048>}> : (!tt.device<#device>) -> tensor<1x7x7x2048xf32, #layout73> loc(#loc877)
    %1076 = "ttnn.conv2d"(%1074, %arg159, %1075, %0) <{batch_size = 1 : i32, dilation_height = 1 : i32, dilation_width = 1 : i32, groups = 1 : i32, in_channels = 512 : i32, input_height = 7 : i32, input_width = 7 : i32, kernel_height = 1 : i32, kernel_width = 1 : i32, out_channels = 2048 : i32, padding_height = 0 : i32, padding_width = 0 : i32, stride_height = 1 : i32, stride_width = 1 : i32}> : (tensor<1x7x7x512xf32, #layout70>, tensor<2048x512x1x1xf32, #layout24>, tensor<1x7x7x2048xf32, #layout73>, !tt.device<#device>) -> tensor<1x7x7x2048xf32, #layout73> loc(#loc877)
    %1077 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x7x2048x7>}> : (!tt.device<#device>) -> tensor<1x7x2048x7xf32, #layout74> loc(#loc878)
    %1078 = "ttnn.transpose"(%1076, %1077) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x7x7x2048xf32, #layout73>, tensor<1x7x2048x7xf32, #layout74>) -> tensor<1x7x2048x7xf32, #layout74> loc(#loc878)
    %1079 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x2048x7x7>}> : (!tt.device<#device>) -> tensor<1x2048x7x7xf32, #layout75> loc(#loc879)
    %1080 = "ttnn.transpose"(%1078, %1079) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<1x7x2048x7xf32, #layout74>, tensor<1x2048x7x7xf32, #layout75>) -> tensor<1x2048x7x7xf32, #layout75> loc(#loc879)
    %1081 = "ttnn.to_layout"(%arg105, %0) <{layout = #ttnn.layout<tile>}> : (tensor<2048x1x1xf32, #layout6>, !tt.device<#device>) -> tensor<2048x1x1xf32, #layout76> loc(#loc880)
    %1082 = "ttnn.to_device"(%1081, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<2048x1x1xf32, #layout76>, !tt.device<#device>) -> tensor<2048x1x1xf32, #layout76> loc(#loc880)
    %1083 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x2048x7x7>}> : (!tt.device<#device>) -> tensor<1x2048x7x7xf32, #layout75> loc(#loc880)
    %1084 = "ttnn.multiply"(%1080, %1082, %1083) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x2048x7x7xf32, #layout75>, tensor<2048x1x1xf32, #layout76>, tensor<1x2048x7x7xf32, #layout75>) -> tensor<1x2048x7x7xf32, #layout75> loc(#loc880)
    %1085 = "ttnn.to_layout"(%arg106, %0) <{layout = #ttnn.layout<tile>}> : (tensor<2048x1x1xf32, #layout6>, !tt.device<#device>) -> tensor<2048x1x1xf32, #layout76> loc(#loc881)
    %1086 = "ttnn.to_device"(%1085, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<2048x1x1xf32, #layout76>, !tt.device<#device>) -> tensor<2048x1x1xf32, #layout76> loc(#loc881)
    %1087 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x2048x7x7>}> : (!tt.device<#device>) -> tensor<1x2048x7x7xf32, #layout75> loc(#loc881)
    %1088 = "ttnn.add"(%1084, %1086, %1087) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x2048x7x7xf32, #layout75>, tensor<2048x1x1xf32, #layout76>, tensor<1x2048x7x7xf32, #layout75>) -> tensor<1x2048x7x7xf32, #layout75> loc(#loc881)
    %1089 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x2048x7x7>}> : (!tt.device<#device>) -> tensor<1x2048x7x7xf32, #layout75> loc(#loc882)
    %1090 = "ttnn.add"(%1088, %1030, %1089) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x2048x7x7xf32, #layout75>, tensor<1x2048x7x7xf32, #layout75>, tensor<1x2048x7x7xf32, #layout75>) -> tensor<1x2048x7x7xf32, #layout75> loc(#loc882)
    %1091 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x2048x7x7>}> : (!tt.device<#device>) -> tensor<1x2048x7x7xf32, #layout75> loc(#loc883)
    %1092 = "ttnn.relu"(%1090, %1091) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x2048x7x7xf32, #layout75>, tensor<1x2048x7x7xf32, #layout75>) -> tensor<1x2048x7x7xf32, #layout75> loc(#loc883)
    %1093 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x1x2048x49>}> : (!tt.device<#device>) -> tensor<1x1x2048x49xf32, #layout77> loc(#loc884)
    %1094 = "ttnn.reshape"(%1092, %1093) <{shape = [1 : i32, 1 : i32, 2048 : i32, 49 : i32]}> : (tensor<1x2048x7x7xf32, #layout75>, tensor<1x1x2048x49xf32, #layout77>) -> tensor<1x1x2048x49xf32, #layout77> loc(#loc884)
    %1095 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x1x49x2048>}> : (!tt.device<#device>) -> tensor<1x1x49x2048xf32, #layout78> loc(#loc885)
    %1096 = "ttnn.transpose"(%1094, %1095) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<1x1x2048x49xf32, #layout77>, tensor<1x1x49x2048xf32, #layout78>) -> tensor<1x1x49x2048xf32, #layout78> loc(#loc885)
    %1097 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x1x1x2048>}> : (!tt.device<#device>) -> tensor<1x1x1x2048xf32, #layout79> loc(#loc886)
    %1098 = "ttnn.mean"(%1096, %1097) <{keep_dim = true}> : (tensor<1x1x49x2048xf32, #layout78>, tensor<1x1x1x2048xf32, #layout79>) -> tensor<1x1x1x2048xf32, #layout79> loc(#loc886)
    %1099 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x2048x1x1>}> : (!tt.device<#device>) -> tensor<1x2048x1x1xf32, #layout80> loc(#loc887)
    %1100 = "ttnn.reshape"(%1098, %1099) <{shape = [1 : i32, 2048 : i32, 1 : i32, 1 : i32]}> : (tensor<1x1x1x2048xf32, #layout79>, tensor<1x2048x1x1xf32, #layout80>) -> tensor<1x2048x1x1xf32, #layout80> loc(#loc887)
    %1101 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x2048x1>}> : (!tt.device<#device>) -> tensor<1x2048x1xf32, #layout81> loc(#loc888)
    %1102 = "ttnn.reshape"(%1100, %1101) <{shape = [1 : i32, 2048 : i32, 1 : i32]}> : (tensor<1x2048x1x1xf32, #layout80>, tensor<1x2048x1xf32, #layout81>) -> tensor<1x2048x1xf32, #layout81> loc(#loc888)
    %1103 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x2048>}> : (!tt.device<#device>) -> tensor<1x2048xf32, #layout82> loc(#loc889)
    %1104 = "ttnn.reshape"(%1102, %1103) <{shape = [1 : i32, 2048 : i32]}> : (tensor<1x2048x1xf32, #layout81>, tensor<1x2048xf32, #layout82>) -> tensor<1x2048xf32, #layout82> loc(#loc889)
    %1105 = "ttnn.to_layout"(%arg160, %0) <{layout = #ttnn.layout<tile>}> : (tensor<2048x1000xf32, #layout27>, !tt.device<#device>) -> tensor<2048x1000xf32, #layout83> loc(#loc890)
    %1106 = "ttnn.to_device"(%1105, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<2048x1000xf32, #layout83>, !tt.device<#device>) -> tensor<2048x1000xf32, #layout83> loc(#loc890)
    %1107 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x1000>}> : (!tt.device<#device>) -> tensor<1x1000xf32, #layout84> loc(#loc890)
    %1108 = "ttnn.matmul"(%1104, %1106, %1107) : (tensor<1x2048xf32, #layout82>, tensor<2048x1000xf32, #layout83>, tensor<1x1000xf32, #layout84>) -> tensor<1x1000xf32, #layout84> loc(#loc890)
    %1109 = "ttnn.to_layout"(%arg161, %0) <{layout = #ttnn.layout<tile>}> : (tensor<1000xf32, #layout28>, !tt.device<#device>) -> tensor<1000xf32, #layout85> loc(#loc891)
    %1110 = "ttnn.to_device"(%1109, %0) <{memory_config = #ttnn.memory_config<<interleaved>, <dram>>}> : (tensor<1000xf32, #layout85>, !tt.device<#device>) -> tensor<1000xf32, #layout85> loc(#loc891)
    %1111 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<row_major>, memory_config = #ttnn.memory_config<<interleaved>, <dram>>, shape = #ttnn.shape<1x1000>}> : (!tt.device<#device>) -> tensor<1x1000xf32, #layout84> loc(#loc891)
    %1112 = "ttnn.add"(%1108, %1110, %1111) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x1000xf32, #layout84>, tensor<1000xf32, #layout85>, tensor<1x1000xf32, #layout84>) -> tensor<1x1000xf32, #layout84> loc(#loc891)
    %1113 = "ttnn.to_memory_config"(%1112, %0) : (tensor<1x1000xf32, #layout84>, !tt.device<#device>) -> tensor<1x1000xf32, #layout29> loc(#loc446)
    return %1113 : tensor<1x1000xf32, #layout29> loc(#loc446)
  } loc(#loc)
} loc(#loc)
#loc1 = loc("forward":4294967295:2951)
#loc2 = loc("forward":4294967295:2952)
#loc3 = loc("forward":4294967295:2954)
#loc4 = loc("forward":4294967295:2955)
#loc5 = loc("forward":4294967295:2956)
#loc6 = loc("forward":4294967295:2958)
#loc7 = loc("forward":4294967295:2960)
#loc8 = loc("forward":4294967295:2961)
#loc9 = loc("forward":4294967295:2962)
#loc10 = loc("forward":4294967295:2963)
#loc11 = loc("forward":4294967295:2964)
#loc12 = loc("forward":4294967295:2966)
#loc13 = loc("forward":4294967295:2967)
#loc14 = loc("forward":4294967295:2968)
#loc15 = loc("forward":4294967295:2970)
#loc16 = loc("forward":4294967295:2972)
#loc17 = loc("forward":4294967295:2973)
#loc18 = loc("forward":4294967295:2974)
#loc19 = loc("forward":4294967295:2975)
#loc20 = loc("forward":4294967295:2977)
#loc21 = loc("forward":4294967295:2978)
#loc22 = loc("forward":4294967295:2979)
#loc23 = loc("forward":4294967295:2981)
#loc24 = loc("forward":4294967295:2983)
#loc25 = loc("forward":4294967295:2984)
#loc26 = loc("forward":4294967295:2985)
#loc27 = loc("forward":4294967295:2986)
#loc28 = loc("forward":4294967295:2988)
#loc29 = loc("forward":4294967295:2989)
#loc30 = loc("forward":4294967295:2990)
#loc31 = loc("forward":4294967295:2992)
#loc32 = loc("forward":4294967295:2994)
#loc33 = loc("forward":4294967295:2995)
#loc34 = loc("forward":4294967295:2996)
#loc35 = loc("forward":4294967295:2998)
#loc36 = loc("forward":4294967295:2999)
#loc37 = loc("forward":4294967295:3000)
#loc38 = loc("forward":4294967295:3002)
#loc39 = loc("forward":4294967295:3004)
#loc40 = loc("forward":4294967295:3005)
#loc41 = loc("forward":4294967295:3006)
#loc42 = loc("forward":4294967295:3007)
#loc43 = loc("forward":4294967295:3008)
#loc44 = loc("forward":4294967295:3010)
#loc45 = loc("forward":4294967295:3011)
#loc46 = loc("forward":4294967295:3012)
#loc47 = loc("forward":4294967295:3014)
#loc48 = loc("forward":4294967295:3016)
#loc49 = loc("forward":4294967295:3017)
#loc50 = loc("forward":4294967295:3018)
#loc51 = loc("forward":4294967295:3019)
#loc52 = loc("forward":4294967295:3021)
#loc53 = loc("forward":4294967295:3022)
#loc54 = loc("forward":4294967295:3023)
#loc55 = loc("forward":4294967295:3025)
#loc56 = loc("forward":4294967295:3027)
#loc57 = loc("forward":4294967295:3028)
#loc58 = loc("forward":4294967295:3029)
#loc59 = loc("forward":4294967295:3030)
#loc60 = loc("forward":4294967295:3032)
#loc61 = loc("forward":4294967295:3033)
#loc62 = loc("forward":4294967295:3034)
#loc63 = loc("forward":4294967295:3036)
#loc64 = loc("forward":4294967295:3038)
#loc65 = loc("forward":4294967295:3039)
#loc66 = loc("forward":4294967295:3040)
#loc67 = loc("forward":4294967295:3041)
#loc68 = loc("forward":4294967295:3042)
#loc69 = loc("forward":4294967295:3044)
#loc70 = loc("forward":4294967295:3045)
#loc71 = loc("forward":4294967295:3046)
#loc72 = loc("forward":4294967295:3048)
#loc73 = loc("forward":4294967295:3050)
#loc74 = loc("forward":4294967295:3051)
#loc75 = loc("forward":4294967295:3052)
#loc76 = loc("forward":4294967295:3053)
#loc77 = loc("forward":4294967295:3055)
#loc78 = loc("forward":4294967295:3056)
#loc79 = loc("forward":4294967295:3057)
#loc80 = loc("forward":4294967295:3059)
#loc81 = loc("forward":4294967295:3061)
#loc82 = loc("forward":4294967295:3062)
#loc83 = loc("forward":4294967295:3063)
#loc84 = loc("forward":4294967295:3064)
#loc85 = loc("forward":4294967295:3066)
#loc86 = loc("forward":4294967295:3067)
#loc87 = loc("forward":4294967295:3068)
#loc88 = loc("forward":4294967295:3070)
#loc89 = loc("forward":4294967295:3072)
#loc90 = loc("forward":4294967295:3073)
#loc91 = loc("forward":4294967295:3074)
#loc92 = loc("forward":4294967295:3075)
#loc93 = loc("forward":4294967295:3076)
#loc94 = loc("forward":4294967295:3078)
#loc95 = loc("forward":4294967295:3079)
#loc96 = loc("forward":4294967295:3080)
#loc97 = loc("forward":4294967295:3082)
#loc98 = loc("forward":4294967295:3084)
#loc99 = loc("forward":4294967295:3085)
#loc100 = loc("forward":4294967295:3086)
#loc101 = loc("forward":4294967295:3087)
#loc102 = loc("forward":4294967295:3089)
#loc103 = loc("forward":4294967295:3090)
#loc104 = loc("forward":4294967295:3091)
#loc105 = loc("forward":4294967295:3093)
#loc106 = loc("forward":4294967295:3095)
#loc107 = loc("forward":4294967295:3096)
#loc108 = loc("forward":4294967295:3097)
#loc109 = loc("forward":4294967295:3098)
#loc110 = loc("forward":4294967295:3100)
#loc111 = loc("forward":4294967295:3101)
#loc112 = loc("forward":4294967295:3102)
#loc113 = loc("forward":4294967295:3104)
#loc114 = loc("forward":4294967295:3106)
#loc115 = loc("forward":4294967295:3107)
#loc116 = loc("forward":4294967295:3108)
#loc117 = loc("forward":4294967295:3110)
#loc118 = loc("forward":4294967295:3111)
#loc119 = loc("forward":4294967295:3112)
#loc120 = loc("forward":4294967295:3114)
#loc121 = loc("forward":4294967295:3116)
#loc122 = loc("forward":4294967295:3117)
#loc123 = loc("forward":4294967295:3118)
#loc124 = loc("forward":4294967295:3119)
#loc125 = loc("forward":4294967295:3120)
#loc126 = loc("forward":4294967295:3122)
#loc127 = loc("forward":4294967295:3123)
#loc128 = loc("forward":4294967295:3124)
#loc129 = loc("forward":4294967295:3126)
#loc130 = loc("forward":4294967295:3128)
#loc131 = loc("forward":4294967295:3129)
#loc132 = loc("forward":4294967295:3130)
#loc133 = loc("forward":4294967295:3131)
#loc134 = loc("forward":4294967295:3133)
#loc135 = loc("forward":4294967295:3134)
#loc136 = loc("forward":4294967295:3135)
#loc137 = loc("forward":4294967295:3137)
#loc138 = loc("forward":4294967295:3139)
#loc139 = loc("forward":4294967295:3140)
#loc140 = loc("forward":4294967295:3141)
#loc141 = loc("forward":4294967295:3142)
#loc142 = loc("forward":4294967295:3144)
#loc143 = loc("forward":4294967295:3145)
#loc144 = loc("forward":4294967295:3146)
#loc145 = loc("forward":4294967295:3148)
#loc146 = loc("forward":4294967295:3150)
#loc147 = loc("forward":4294967295:3151)
#loc148 = loc("forward":4294967295:3152)
#loc149 = loc("forward":4294967295:3153)
#loc150 = loc("forward":4294967295:3154)
#loc151 = loc("forward":4294967295:3156)
#loc152 = loc("forward":4294967295:3157)
#loc153 = loc("forward":4294967295:3158)
#loc154 = loc("forward":4294967295:3160)
#loc155 = loc("forward":4294967295:3162)
#loc156 = loc("forward":4294967295:3163)
#loc157 = loc("forward":4294967295:3164)
#loc158 = loc("forward":4294967295:3165)
#loc159 = loc("forward":4294967295:3167)
#loc160 = loc("forward":4294967295:3168)
#loc161 = loc("forward":4294967295:3169)
#loc162 = loc("forward":4294967295:3171)
#loc163 = loc("forward":4294967295:3173)
#loc164 = loc("forward":4294967295:3174)
#loc165 = loc("forward":4294967295:3175)
#loc166 = loc("forward":4294967295:3176)
#loc167 = loc("forward":4294967295:3178)
#loc168 = loc("forward":4294967295:3179)
#loc169 = loc("forward":4294967295:3180)
#loc170 = loc("forward":4294967295:3182)
#loc171 = loc("forward":4294967295:3184)
#loc172 = loc("forward":4294967295:3185)
#loc173 = loc("forward":4294967295:3186)
#loc174 = loc("forward":4294967295:3187)
#loc175 = loc("forward":4294967295:3188)
#loc176 = loc("forward":4294967295:3190)
#loc177 = loc("forward":4294967295:3191)
#loc178 = loc("forward":4294967295:3192)
#loc179 = loc("forward":4294967295:3194)
#loc180 = loc("forward":4294967295:3196)
#loc181 = loc("forward":4294967295:3197)
#loc182 = loc("forward":4294967295:3198)
#loc183 = loc("forward":4294967295:3199)
#loc184 = loc("forward":4294967295:3201)
#loc185 = loc("forward":4294967295:3202)
#loc186 = loc("forward":4294967295:3203)
#loc187 = loc("forward":4294967295:3205)
#loc188 = loc("forward":4294967295:3207)
#loc189 = loc("forward":4294967295:3208)
#loc190 = loc("forward":4294967295:3209)
#loc191 = loc("forward":4294967295:3210)
#loc192 = loc("forward":4294967295:3212)
#loc193 = loc("forward":4294967295:3213)
#loc194 = loc("forward":4294967295:3214)
#loc195 = loc("forward":4294967295:3216)
#loc196 = loc("forward":4294967295:3218)
#loc197 = loc("forward":4294967295:3219)
#loc198 = loc("forward":4294967295:3220)
#loc199 = loc("forward":4294967295:3221)
#loc200 = loc("forward":4294967295:3222)
#loc201 = loc("forward":4294967295:3224)
#loc202 = loc("forward":4294967295:3225)
#loc203 = loc("forward":4294967295:3226)
#loc204 = loc("forward":4294967295:3228)
#loc205 = loc("forward":4294967295:3230)
#loc206 = loc("forward":4294967295:3231)
#loc207 = loc("forward":4294967295:3232)
#loc208 = loc("forward":4294967295:3233)
#loc209 = loc("forward":4294967295:3235)
#loc210 = loc("forward":4294967295:3236)
#loc211 = loc("forward":4294967295:3237)
#loc212 = loc("forward":4294967295:3239)
#loc213 = loc("forward":4294967295:3241)
#loc214 = loc("forward":4294967295:3242)
#loc215 = loc("forward":4294967295:3243)
#loc216 = loc("forward":4294967295:3244)
#loc217 = loc("forward":4294967295:3246)
#loc218 = loc("forward":4294967295:3247)
#loc219 = loc("forward":4294967295:3248)
#loc220 = loc("forward":4294967295:3250)
#loc221 = loc("forward":4294967295:3252)
#loc222 = loc("forward":4294967295:3253)
#loc223 = loc("forward":4294967295:3254)
#loc224 = loc("forward":4294967295:3256)
#loc225 = loc("forward":4294967295:3257)
#loc226 = loc("forward":4294967295:3258)
#loc227 = loc("forward":4294967295:3260)
#loc228 = loc("forward":4294967295:3262)
#loc229 = loc("forward":4294967295:3263)
#loc230 = loc("forward":4294967295:3264)
#loc231 = loc("forward":4294967295:3265)
#loc232 = loc("forward":4294967295:3266)
#loc233 = loc("forward":4294967295:3268)
#loc234 = loc("forward":4294967295:3269)
#loc235 = loc("forward":4294967295:3270)
#loc236 = loc("forward":4294967295:3272)
#loc237 = loc("forward":4294967295:3274)
#loc238 = loc("forward":4294967295:3275)
#loc239 = loc("forward":4294967295:3276)
#loc240 = loc("forward":4294967295:3277)
#loc241 = loc("forward":4294967295:3279)
#loc242 = loc("forward":4294967295:3280)
#loc243 = loc("forward":4294967295:3281)
#loc244 = loc("forward":4294967295:3283)
#loc245 = loc("forward":4294967295:3285)
#loc246 = loc("forward":4294967295:3286)
#loc247 = loc("forward":4294967295:3287)
#loc248 = loc("forward":4294967295:3288)
#loc249 = loc("forward":4294967295:3290)
#loc250 = loc("forward":4294967295:3291)
#loc251 = loc("forward":4294967295:3292)
#loc252 = loc("forward":4294967295:3294)
#loc253 = loc("forward":4294967295:3296)
#loc254 = loc("forward":4294967295:3297)
#loc255 = loc("forward":4294967295:3298)
#loc256 = loc("forward":4294967295:3299)
#loc257 = loc("forward":4294967295:3300)
#loc258 = loc("forward":4294967295:3302)
#loc259 = loc("forward":4294967295:3303)
#loc260 = loc("forward":4294967295:3304)
#loc261 = loc("forward":4294967295:3306)
#loc262 = loc("forward":4294967295:3308)
#loc263 = loc("forward":4294967295:3309)
#loc264 = loc("forward":4294967295:3310)
#loc265 = loc("forward":4294967295:3311)
#loc266 = loc("forward":4294967295:3313)
#loc267 = loc("forward":4294967295:3314)
#loc268 = loc("forward":4294967295:3315)
#loc269 = loc("forward":4294967295:3317)
#loc270 = loc("forward":4294967295:3319)
#loc271 = loc("forward":4294967295:3320)
#loc272 = loc("forward":4294967295:3321)
#loc273 = loc("forward":4294967295:3322)
#loc274 = loc("forward":4294967295:3324)
#loc275 = loc("forward":4294967295:3325)
#loc276 = loc("forward":4294967295:3326)
#loc277 = loc("forward":4294967295:3328)
#loc278 = loc("forward":4294967295:3330)
#loc279 = loc("forward":4294967295:3331)
#loc280 = loc("forward":4294967295:3332)
#loc281 = loc("forward":4294967295:3333)
#loc282 = loc("forward":4294967295:3334)
#loc283 = loc("forward":4294967295:3336)
#loc284 = loc("forward":4294967295:3337)
#loc285 = loc("forward":4294967295:3338)
#loc286 = loc("forward":4294967295:3340)
#loc287 = loc("forward":4294967295:3342)
#loc288 = loc("forward":4294967295:3343)
#loc289 = loc("forward":4294967295:3344)
#loc290 = loc("forward":4294967295:3345)
#loc291 = loc("forward":4294967295:3347)
#loc292 = loc("forward":4294967295:3348)
#loc293 = loc("forward":4294967295:3349)
#loc294 = loc("forward":4294967295:3351)
#loc295 = loc("forward":4294967295:3353)
#loc296 = loc("forward":4294967295:3354)
#loc297 = loc("forward":4294967295:3355)
#loc298 = loc("forward":4294967295:3356)
#loc299 = loc("forward":4294967295:3358)
#loc300 = loc("forward":4294967295:3359)
#loc301 = loc("forward":4294967295:3360)
#loc302 = loc("forward":4294967295:3362)
#loc303 = loc("forward":4294967295:3364)
#loc304 = loc("forward":4294967295:3365)
#loc305 = loc("forward":4294967295:3366)
#loc306 = loc("forward":4294967295:3367)
#loc307 = loc("forward":4294967295:3368)
#loc308 = loc("forward":4294967295:3370)
#loc309 = loc("forward":4294967295:3371)
#loc310 = loc("forward":4294967295:3372)
#loc311 = loc("forward":4294967295:3374)
#loc312 = loc("forward":4294967295:3376)
#loc313 = loc("forward":4294967295:3377)
#loc314 = loc("forward":4294967295:3378)
#loc315 = loc("forward":4294967295:3379)
#loc316 = loc("forward":4294967295:3381)
#loc317 = loc("forward":4294967295:3382)
#loc318 = loc("forward":4294967295:3383)
#loc319 = loc("forward":4294967295:3385)
#loc320 = loc("forward":4294967295:3387)
#loc321 = loc("forward":4294967295:3388)
#loc322 = loc("forward":4294967295:3389)
#loc323 = loc("forward":4294967295:3390)
#loc324 = loc("forward":4294967295:3392)
#loc325 = loc("forward":4294967295:3393)
#loc326 = loc("forward":4294967295:3394)
#loc327 = loc("forward":4294967295:3396)
#loc328 = loc("forward":4294967295:3398)
#loc329 = loc("forward":4294967295:3399)
#loc330 = loc("forward":4294967295:3400)
#loc331 = loc("forward":4294967295:3401)
#loc332 = loc("forward":4294967295:3402)
#loc333 = loc("forward":4294967295:3404)
#loc334 = loc("forward":4294967295:3405)
#loc335 = loc("forward":4294967295:3406)
#loc336 = loc("forward":4294967295:3408)
#loc337 = loc("forward":4294967295:3410)
#loc338 = loc("forward":4294967295:3411)
#loc339 = loc("forward":4294967295:3412)
#loc340 = loc("forward":4294967295:3413)
#loc341 = loc("forward":4294967295:3415)
#loc342 = loc("forward":4294967295:3416)
#loc343 = loc("forward":4294967295:3417)
#loc344 = loc("forward":4294967295:3419)
#loc345 = loc("forward":4294967295:3421)
#loc346 = loc("forward":4294967295:3422)
#loc347 = loc("forward":4294967295:3423)
#loc348 = loc("forward":4294967295:3424)
#loc349 = loc("forward":4294967295:3426)
#loc350 = loc("forward":4294967295:3427)
#loc351 = loc("forward":4294967295:3428)
#loc352 = loc("forward":4294967295:3430)
#loc353 = loc("forward":4294967295:3432)
#loc354 = loc("forward":4294967295:3433)
#loc355 = loc("forward":4294967295:3434)
#loc356 = loc("forward":4294967295:3435)
#loc357 = loc("forward":4294967295:3436)
#loc358 = loc("forward":4294967295:3438)
#loc359 = loc("forward":4294967295:3439)
#loc360 = loc("forward":4294967295:3440)
#loc361 = loc("forward":4294967295:3442)
#loc362 = loc("forward":4294967295:3444)
#loc363 = loc("forward":4294967295:3445)
#loc364 = loc("forward":4294967295:3446)
#loc365 = loc("forward":4294967295:3447)
#loc366 = loc("forward":4294967295:3449)
#loc367 = loc("forward":4294967295:3450)
#loc368 = loc("forward":4294967295:3451)
#loc369 = loc("forward":4294967295:3453)
#loc370 = loc("forward":4294967295:3455)
#loc371 = loc("forward":4294967295:3456)
#loc372 = loc("forward":4294967295:3457)
#loc373 = loc("forward":4294967295:3458)
#loc374 = loc("forward":4294967295:3460)
#loc375 = loc("forward":4294967295:3461)
#loc376 = loc("forward":4294967295:3462)
#loc377 = loc("forward":4294967295:3464)
#loc378 = loc("forward":4294967295:3466)
#loc379 = loc("forward":4294967295:3467)
#loc380 = loc("forward":4294967295:3468)
#loc381 = loc("forward":4294967295:3470)
#loc382 = loc("forward":4294967295:3471)
#loc383 = loc("forward":4294967295:3472)
#loc384 = loc("forward":4294967295:3474)
#loc385 = loc("forward":4294967295:3476)
#loc386 = loc("forward":4294967295:3477)
#loc387 = loc("forward":4294967295:3478)
#loc388 = loc("forward":4294967295:3479)
#loc389 = loc("forward":4294967295:3480)
#loc390 = loc("forward":4294967295:3482)
#loc391 = loc("forward":4294967295:3483)
#loc392 = loc("forward":4294967295:3484)
#loc393 = loc("forward":4294967295:3486)
#loc394 = loc("forward":4294967295:3488)
#loc395 = loc("forward":4294967295:3489)
#loc396 = loc("forward":4294967295:3490)
#loc397 = loc("forward":4294967295:3491)
#loc398 = loc("forward":4294967295:3493)
#loc399 = loc("forward":4294967295:3494)
#loc400 = loc("forward":4294967295:3495)
#loc401 = loc("forward":4294967295:3497)
#loc402 = loc("forward":4294967295:3499)
#loc403 = loc("forward":4294967295:3500)
#loc404 = loc("forward":4294967295:3501)
#loc405 = loc("forward":4294967295:3502)
#loc406 = loc("forward":4294967295:3504)
#loc407 = loc("forward":4294967295:3505)
#loc408 = loc("forward":4294967295:3506)
#loc409 = loc("forward":4294967295:3508)
#loc410 = loc("forward":4294967295:3510)
#loc411 = loc("forward":4294967295:3511)
#loc412 = loc("forward":4294967295:3512)
#loc413 = loc("forward":4294967295:3513)
#loc414 = loc("forward":4294967295:3514)
#loc415 = loc("forward":4294967295:3516)
#loc416 = loc("forward":4294967295:3517)
#loc417 = loc("forward":4294967295:3518)
#loc418 = loc("forward":4294967295:3520)
#loc419 = loc("forward":4294967295:3522)
#loc420 = loc("forward":4294967295:3523)
#loc421 = loc("forward":4294967295:3524)
#loc422 = loc("forward":4294967295:3525)
#loc423 = loc("forward":4294967295:3527)
#loc424 = loc("forward":4294967295:3528)
#loc425 = loc("forward":4294967295:3529)
#loc426 = loc("forward":4294967295:3531)
#loc427 = loc("forward":4294967295:3533)
#loc428 = loc("forward":4294967295:3534)
#loc429 = loc("forward":4294967295:3535)
#loc430 = loc("forward":4294967295:3536)
#loc431 = loc("forward":4294967295:3538)
#loc432 = loc("forward":4294967295:3539)
#loc433 = loc("forward":4294967295:3540)
#loc434 = loc("forward":4294967295:3542)
#loc435 = loc("forward":4294967295:3544)
#loc436 = loc("forward":4294967295:3545)
#loc437 = loc("forward":4294967295:3546)
#loc438 = loc("forward":4294967295:3547)
#loc439 = loc("forward":4294967295:3548)
#loc440 = loc("forward":4294967295:3549)
#loc441 = loc("forward":4294967295:3550)
#loc442 = loc("forward":4294967295:3551)
#loc443 = loc("forward":4294967295:3552)
#loc444 = loc("forward":4294967295:3554)
#loc445 = loc("forward":4294967295:3556)
#loc446 = loc(unknown)
#loc447 = loc("conv2d_0.dc.transpose.0"(#loc1))
#loc448 = loc("conv2d_0.dc.transpose.1"(#loc2))
#loc449 = loc("conv2d_0.dc.conv2d.2"(#loc3))
#loc450 = loc("conv2d_0.dc.transpose.3"(#loc4))
#loc451 = loc("conv2d_0.dc.transpose.4"(#loc5))
#loc452 = loc("multiply_8"(#loc6))
#loc453 = loc("add_14"(#loc7))
#loc454 = loc("relu_15"(#loc8))
#loc455 = loc("max_pool2d_16"(#loc9))
#loc456 = loc("conv2d_17.dc.transpose.0"(#loc10))
#loc457 = loc("conv2d_17.dc.transpose.1"(#loc11))
#loc458 = loc("conv2d_17.dc.conv2d.2"(#loc12))
#loc459 = loc("conv2d_17.dc.transpose.3"(#loc13))
#loc460 = loc("conv2d_17.dc.transpose.4"(#loc14))
#loc461 = loc("multiply_25"(#loc15))
#loc462 = loc("add_31"(#loc16))
#loc463 = loc("relu_32"(#loc17))
#loc464 = loc("conv2d_33.dc.transpose.0"(#loc18))
#loc465 = loc("conv2d_33.dc.transpose.1"(#loc19))
#loc466 = loc("conv2d_33.dc.conv2d.2"(#loc20))
#loc467 = loc("conv2d_33.dc.transpose.3"(#loc21))
#loc468 = loc("conv2d_33.dc.transpose.4"(#loc22))
#loc469 = loc("multiply_41"(#loc23))
#loc470 = loc("add_47"(#loc24))
#loc471 = loc("relu_48"(#loc25))
#loc472 = loc("conv2d_49.dc.transpose.0"(#loc26))
#loc473 = loc("conv2d_49.dc.transpose.1"(#loc27))
#loc474 = loc("conv2d_49.dc.conv2d.2"(#loc28))
#loc475 = loc("conv2d_49.dc.transpose.3"(#loc29))
#loc476 = loc("conv2d_49.dc.transpose.4"(#loc30))
#loc477 = loc("multiply_57"(#loc31))
#loc478 = loc("add_63"(#loc32))
#loc479 = loc("conv2d_64.dc.transpose.0"(#loc33))
#loc480 = loc("conv2d_64.dc.transpose.1"(#loc34))
#loc481 = loc("conv2d_64.dc.conv2d.2"(#loc35))
#loc482 = loc("conv2d_64.dc.transpose.3"(#loc36))
#loc483 = loc("conv2d_64.dc.transpose.4"(#loc37))
#loc484 = loc("multiply_72"(#loc38))
#loc485 = loc("add_78"(#loc39))
#loc486 = loc("add_79"(#loc40))
#loc487 = loc("relu_80"(#loc41))
#loc488 = loc("conv2d_81.dc.transpose.0"(#loc42))
#loc489 = loc("conv2d_81.dc.transpose.1"(#loc43))
#loc490 = loc("conv2d_81.dc.conv2d.2"(#loc44))
#loc491 = loc("conv2d_81.dc.transpose.3"(#loc45))
#loc492 = loc("conv2d_81.dc.transpose.4"(#loc46))
#loc493 = loc("multiply_89"(#loc47))
#loc494 = loc("add_95"(#loc48))
#loc495 = loc("relu_96"(#loc49))
#loc496 = loc("conv2d_97.dc.transpose.0"(#loc50))
#loc497 = loc("conv2d_97.dc.transpose.1"(#loc51))
#loc498 = loc("conv2d_97.dc.conv2d.2"(#loc52))
#loc499 = loc("conv2d_97.dc.transpose.3"(#loc53))
#loc500 = loc("conv2d_97.dc.transpose.4"(#loc54))
#loc501 = loc("multiply_105"(#loc55))
#loc502 = loc("add_111"(#loc56))
#loc503 = loc("relu_112"(#loc57))
#loc504 = loc("conv2d_113.dc.transpose.0"(#loc58))
#loc505 = loc("conv2d_113.dc.transpose.1"(#loc59))
#loc506 = loc("conv2d_113.dc.conv2d.2"(#loc60))
#loc507 = loc("conv2d_113.dc.transpose.3"(#loc61))
#loc508 = loc("conv2d_113.dc.transpose.4"(#loc62))
#loc509 = loc("multiply_121"(#loc63))
#loc510 = loc("add_127"(#loc64))
#loc511 = loc("add_128"(#loc65))
#loc512 = loc("relu_129"(#loc66))
#loc513 = loc("conv2d_130.dc.transpose.0"(#loc67))
#loc514 = loc("conv2d_130.dc.transpose.1"(#loc68))
#loc515 = loc("conv2d_130.dc.conv2d.2"(#loc69))
#loc516 = loc("conv2d_130.dc.transpose.3"(#loc70))
#loc517 = loc("conv2d_130.dc.transpose.4"(#loc71))
#loc518 = loc("multiply_138"(#loc72))
#loc519 = loc("add_144"(#loc73))
#loc520 = loc("relu_145"(#loc74))
#loc521 = loc("conv2d_146.dc.transpose.0"(#loc75))
#loc522 = loc("conv2d_146.dc.transpose.1"(#loc76))
#loc523 = loc("conv2d_146.dc.conv2d.2"(#loc77))
#loc524 = loc("conv2d_146.dc.transpose.3"(#loc78))
#loc525 = loc("conv2d_146.dc.transpose.4"(#loc79))
#loc526 = loc("multiply_154"(#loc80))
#loc527 = loc("add_160"(#loc81))
#loc528 = loc("relu_161"(#loc82))
#loc529 = loc("conv2d_162.dc.transpose.0"(#loc83))
#loc530 = loc("conv2d_162.dc.transpose.1"(#loc84))
#loc531 = loc("conv2d_162.dc.conv2d.2"(#loc85))
#loc532 = loc("conv2d_162.dc.transpose.3"(#loc86))
#loc533 = loc("conv2d_162.dc.transpose.4"(#loc87))
#loc534 = loc("multiply_170"(#loc88))
#loc535 = loc("add_176"(#loc89))
#loc536 = loc("add_177"(#loc90))
#loc537 = loc("relu_178"(#loc91))
#loc538 = loc("conv2d_179.dc.transpose.0"(#loc92))
#loc539 = loc("conv2d_179.dc.transpose.1"(#loc93))
#loc540 = loc("conv2d_179.dc.conv2d.2"(#loc94))
#loc541 = loc("conv2d_179.dc.transpose.3"(#loc95))
#loc542 = loc("conv2d_179.dc.transpose.4"(#loc96))
#loc543 = loc("multiply_187"(#loc97))
#loc544 = loc("add_193"(#loc98))
#loc545 = loc("relu_194"(#loc99))
#loc546 = loc("conv2d_195.dc.transpose.0"(#loc100))
#loc547 = loc("conv2d_195.dc.transpose.1"(#loc101))
#loc548 = loc("conv2d_195.dc.conv2d.2"(#loc102))
#loc549 = loc("conv2d_195.dc.transpose.3"(#loc103))
#loc550 = loc("conv2d_195.dc.transpose.4"(#loc104))
#loc551 = loc("multiply_203"(#loc105))
#loc552 = loc("add_209"(#loc106))
#loc553 = loc("relu_210"(#loc107))
#loc554 = loc("conv2d_211.dc.transpose.0"(#loc108))
#loc555 = loc("conv2d_211.dc.transpose.1"(#loc109))
#loc556 = loc("conv2d_211.dc.conv2d.2"(#loc110))
#loc557 = loc("conv2d_211.dc.transpose.3"(#loc111))
#loc558 = loc("conv2d_211.dc.transpose.4"(#loc112))
#loc559 = loc("multiply_219"(#loc113))
#loc560 = loc("add_225"(#loc114))
#loc561 = loc("conv2d_226.dc.transpose.0"(#loc115))
#loc562 = loc("conv2d_226.dc.transpose.1"(#loc116))
#loc563 = loc("conv2d_226.dc.conv2d.2"(#loc117))
#loc564 = loc("conv2d_226.dc.transpose.3"(#loc118))
#loc565 = loc("conv2d_226.dc.transpose.4"(#loc119))
#loc566 = loc("multiply_234"(#loc120))
#loc567 = loc("add_240"(#loc121))
#loc568 = loc("add_241"(#loc122))
#loc569 = loc("relu_242"(#loc123))
#loc570 = loc("conv2d_243.dc.transpose.0"(#loc124))
#loc571 = loc("conv2d_243.dc.transpose.1"(#loc125))
#loc572 = loc("conv2d_243.dc.conv2d.2"(#loc126))
#loc573 = loc("conv2d_243.dc.transpose.3"(#loc127))
#loc574 = loc("conv2d_243.dc.transpose.4"(#loc128))
#loc575 = loc("multiply_251"(#loc129))
#loc576 = loc("add_257"(#loc130))
#loc577 = loc("relu_258"(#loc131))
#loc578 = loc("conv2d_259.dc.transpose.0"(#loc132))
#loc579 = loc("conv2d_259.dc.transpose.1"(#loc133))
#loc580 = loc("conv2d_259.dc.conv2d.2"(#loc134))
#loc581 = loc("conv2d_259.dc.transpose.3"(#loc135))
#loc582 = loc("conv2d_259.dc.transpose.4"(#loc136))
#loc583 = loc("multiply_267"(#loc137))
#loc584 = loc("add_273"(#loc138))
#loc585 = loc("relu_274"(#loc139))
#loc586 = loc("conv2d_275.dc.transpose.0"(#loc140))
#loc587 = loc("conv2d_275.dc.transpose.1"(#loc141))
#loc588 = loc("conv2d_275.dc.conv2d.2"(#loc142))
#loc589 = loc("conv2d_275.dc.transpose.3"(#loc143))
#loc590 = loc("conv2d_275.dc.transpose.4"(#loc144))
#loc591 = loc("multiply_283"(#loc145))
#loc592 = loc("add_289"(#loc146))
#loc593 = loc("add_290"(#loc147))
#loc594 = loc("relu_291"(#loc148))
#loc595 = loc("conv2d_292.dc.transpose.0"(#loc149))
#loc596 = loc("conv2d_292.dc.transpose.1"(#loc150))
#loc597 = loc("conv2d_292.dc.conv2d.2"(#loc151))
#loc598 = loc("conv2d_292.dc.transpose.3"(#loc152))
#loc599 = loc("conv2d_292.dc.transpose.4"(#loc153))
#loc600 = loc("multiply_300"(#loc154))
#loc601 = loc("add_306"(#loc155))
#loc602 = loc("relu_307"(#loc156))
#loc603 = loc("conv2d_308.dc.transpose.0"(#loc157))
#loc604 = loc("conv2d_308.dc.transpose.1"(#loc158))
#loc605 = loc("conv2d_308.dc.conv2d.2"(#loc159))
#loc606 = loc("conv2d_308.dc.transpose.3"(#loc160))
#loc607 = loc("conv2d_308.dc.transpose.4"(#loc161))
#loc608 = loc("multiply_316"(#loc162))
#loc609 = loc("add_322"(#loc163))
#loc610 = loc("relu_323"(#loc164))
#loc611 = loc("conv2d_324.dc.transpose.0"(#loc165))
#loc612 = loc("conv2d_324.dc.transpose.1"(#loc166))
#loc613 = loc("conv2d_324.dc.conv2d.2"(#loc167))
#loc614 = loc("conv2d_324.dc.transpose.3"(#loc168))
#loc615 = loc("conv2d_324.dc.transpose.4"(#loc169))
#loc616 = loc("multiply_332"(#loc170))
#loc617 = loc("add_338"(#loc171))
#loc618 = loc("add_339"(#loc172))
#loc619 = loc("relu_340"(#loc173))
#loc620 = loc("conv2d_341.dc.transpose.0"(#loc174))
#loc621 = loc("conv2d_341.dc.transpose.1"(#loc175))
#loc622 = loc("conv2d_341.dc.conv2d.2"(#loc176))
#loc623 = loc("conv2d_341.dc.transpose.3"(#loc177))
#loc624 = loc("conv2d_341.dc.transpose.4"(#loc178))
#loc625 = loc("multiply_349"(#loc179))
#loc626 = loc("add_355"(#loc180))
#loc627 = loc("relu_356"(#loc181))
#loc628 = loc("conv2d_357.dc.transpose.0"(#loc182))
#loc629 = loc("conv2d_357.dc.transpose.1"(#loc183))
#loc630 = loc("conv2d_357.dc.conv2d.2"(#loc184))
#loc631 = loc("conv2d_357.dc.transpose.3"(#loc185))
#loc632 = loc("conv2d_357.dc.transpose.4"(#loc186))
#loc633 = loc("multiply_365"(#loc187))
#loc634 = loc("add_371"(#loc188))
#loc635 = loc("relu_372"(#loc189))
#loc636 = loc("conv2d_373.dc.transpose.0"(#loc190))
#loc637 = loc("conv2d_373.dc.transpose.1"(#loc191))
#loc638 = loc("conv2d_373.dc.conv2d.2"(#loc192))
#loc639 = loc("conv2d_373.dc.transpose.3"(#loc193))
#loc640 = loc("conv2d_373.dc.transpose.4"(#loc194))
#loc641 = loc("multiply_381"(#loc195))
#loc642 = loc("add_387"(#loc196))
#loc643 = loc("add_388"(#loc197))
#loc644 = loc("relu_389"(#loc198))
#loc645 = loc("conv2d_390.dc.transpose.0"(#loc199))
#loc646 = loc("conv2d_390.dc.transpose.1"(#loc200))
#loc647 = loc("conv2d_390.dc.conv2d.2"(#loc201))
#loc648 = loc("conv2d_390.dc.transpose.3"(#loc202))
#loc649 = loc("conv2d_390.dc.transpose.4"(#loc203))
#loc650 = loc("multiply_398"(#loc204))
#loc651 = loc("add_404"(#loc205))
#loc652 = loc("relu_405"(#loc206))
#loc653 = loc("conv2d_406.dc.transpose.0"(#loc207))
#loc654 = loc("conv2d_406.dc.transpose.1"(#loc208))
#loc655 = loc("conv2d_406.dc.conv2d.2"(#loc209))
#loc656 = loc("conv2d_406.dc.transpose.3"(#loc210))
#loc657 = loc("conv2d_406.dc.transpose.4"(#loc211))
#loc658 = loc("multiply_414"(#loc212))
#loc659 = loc("add_420"(#loc213))
#loc660 = loc("relu_421"(#loc214))
#loc661 = loc("conv2d_422.dc.transpose.0"(#loc215))
#loc662 = loc("conv2d_422.dc.transpose.1"(#loc216))
#loc663 = loc("conv2d_422.dc.conv2d.2"(#loc217))
#loc664 = loc("conv2d_422.dc.transpose.3"(#loc218))
#loc665 = loc("conv2d_422.dc.transpose.4"(#loc219))
#loc666 = loc("multiply_430"(#loc220))
#loc667 = loc("add_436"(#loc221))
#loc668 = loc("conv2d_437.dc.transpose.0"(#loc222))
#loc669 = loc("conv2d_437.dc.transpose.1"(#loc223))
#loc670 = loc("conv2d_437.dc.conv2d.2"(#loc224))
#loc671 = loc("conv2d_437.dc.transpose.3"(#loc225))
#loc672 = loc("conv2d_437.dc.transpose.4"(#loc226))
#loc673 = loc("multiply_445"(#loc227))
#loc674 = loc("add_451"(#loc228))
#loc675 = loc("add_452"(#loc229))
#loc676 = loc("relu_453"(#loc230))
#loc677 = loc("conv2d_454.dc.transpose.0"(#loc231))
#loc678 = loc("conv2d_454.dc.transpose.1"(#loc232))
#loc679 = loc("conv2d_454.dc.conv2d.2"(#loc233))
#loc680 = loc("conv2d_454.dc.transpose.3"(#loc234))
#loc681 = loc("conv2d_454.dc.transpose.4"(#loc235))
#loc682 = loc("multiply_462"(#loc236))
#loc683 = loc("add_468"(#loc237))
#loc684 = loc("relu_469"(#loc238))
#loc685 = loc("conv2d_470.dc.transpose.0"(#loc239))
#loc686 = loc("conv2d_470.dc.transpose.1"(#loc240))
#loc687 = loc("conv2d_470.dc.conv2d.2"(#loc241))
#loc688 = loc("conv2d_470.dc.transpose.3"(#loc242))
#loc689 = loc("conv2d_470.dc.transpose.4"(#loc243))
#loc690 = loc("multiply_478"(#loc244))
#loc691 = loc("add_484"(#loc245))
#loc692 = loc("relu_485"(#loc246))
#loc693 = loc("conv2d_486.dc.transpose.0"(#loc247))
#loc694 = loc("conv2d_486.dc.transpose.1"(#loc248))
#loc695 = loc("conv2d_486.dc.conv2d.2"(#loc249))
#loc696 = loc("conv2d_486.dc.transpose.3"(#loc250))
#loc697 = loc("conv2d_486.dc.transpose.4"(#loc251))
#loc698 = loc("multiply_494"(#loc252))
#loc699 = loc("add_500"(#loc253))
#loc700 = loc("add_501"(#loc254))
#loc701 = loc("relu_502"(#loc255))
#loc702 = loc("conv2d_503.dc.transpose.0"(#loc256))
#loc703 = loc("conv2d_503.dc.transpose.1"(#loc257))
#loc704 = loc("conv2d_503.dc.conv2d.2"(#loc258))
#loc705 = loc("conv2d_503.dc.transpose.3"(#loc259))
#loc706 = loc("conv2d_503.dc.transpose.4"(#loc260))
#loc707 = loc("multiply_511"(#loc261))
#loc708 = loc("add_517"(#loc262))
#loc709 = loc("relu_518"(#loc263))
#loc710 = loc("conv2d_519.dc.transpose.0"(#loc264))
#loc711 = loc("conv2d_519.dc.transpose.1"(#loc265))
#loc712 = loc("conv2d_519.dc.conv2d.2"(#loc266))
#loc713 = loc("conv2d_519.dc.transpose.3"(#loc267))
#loc714 = loc("conv2d_519.dc.transpose.4"(#loc268))
#loc715 = loc("multiply_527"(#loc269))
#loc716 = loc("add_533"(#loc270))
#loc717 = loc("relu_534"(#loc271))
#loc718 = loc("conv2d_535.dc.transpose.0"(#loc272))
#loc719 = loc("conv2d_535.dc.transpose.1"(#loc273))
#loc720 = loc("conv2d_535.dc.conv2d.2"(#loc274))
#loc721 = loc("conv2d_535.dc.transpose.3"(#loc275))
#loc722 = loc("conv2d_535.dc.transpose.4"(#loc276))
#loc723 = loc("multiply_543"(#loc277))
#loc724 = loc("add_549"(#loc278))
#loc725 = loc("add_550"(#loc279))
#loc726 = loc("relu_551"(#loc280))
#loc727 = loc("conv2d_552.dc.transpose.0"(#loc281))
#loc728 = loc("conv2d_552.dc.transpose.1"(#loc282))
#loc729 = loc("conv2d_552.dc.conv2d.2"(#loc283))
#loc730 = loc("conv2d_552.dc.transpose.3"(#loc284))
#loc731 = loc("conv2d_552.dc.transpose.4"(#loc285))
#loc732 = loc("multiply_560"(#loc286))
#loc733 = loc("add_566"(#loc287))
#loc734 = loc("relu_567"(#loc288))
#loc735 = loc("conv2d_568.dc.transpose.0"(#loc289))
#loc736 = loc("conv2d_568.dc.transpose.1"(#loc290))
#loc737 = loc("conv2d_568.dc.conv2d.2"(#loc291))
#loc738 = loc("conv2d_568.dc.transpose.3"(#loc292))
#loc739 = loc("conv2d_568.dc.transpose.4"(#loc293))
#loc740 = loc("multiply_576"(#loc294))
#loc741 = loc("add_582"(#loc295))
#loc742 = loc("relu_583"(#loc296))
#loc743 = loc("conv2d_584.dc.transpose.0"(#loc297))
#loc744 = loc("conv2d_584.dc.transpose.1"(#loc298))
#loc745 = loc("conv2d_584.dc.conv2d.2"(#loc299))
#loc746 = loc("conv2d_584.dc.transpose.3"(#loc300))
#loc747 = loc("conv2d_584.dc.transpose.4"(#loc301))
#loc748 = loc("multiply_592"(#loc302))
#loc749 = loc("add_598"(#loc303))
#loc750 = loc("add_599"(#loc304))
#loc751 = loc("relu_600"(#loc305))
#loc752 = loc("conv2d_601.dc.transpose.0"(#loc306))
#loc753 = loc("conv2d_601.dc.transpose.1"(#loc307))
#loc754 = loc("conv2d_601.dc.conv2d.2"(#loc308))
#loc755 = loc("conv2d_601.dc.transpose.3"(#loc309))
#loc756 = loc("conv2d_601.dc.transpose.4"(#loc310))
#loc757 = loc("multiply_609"(#loc311))
#loc758 = loc("add_615"(#loc312))
#loc759 = loc("relu_616"(#loc313))
#loc760 = loc("conv2d_617.dc.transpose.0"(#loc314))
#loc761 = loc("conv2d_617.dc.transpose.1"(#loc315))
#loc762 = loc("conv2d_617.dc.conv2d.2"(#loc316))
#loc763 = loc("conv2d_617.dc.transpose.3"(#loc317))
#loc764 = loc("conv2d_617.dc.transpose.4"(#loc318))
#loc765 = loc("multiply_625"(#loc319))
#loc766 = loc("add_631"(#loc320))
#loc767 = loc("relu_632"(#loc321))
#loc768 = loc("conv2d_633.dc.transpose.0"(#loc322))
#loc769 = loc("conv2d_633.dc.transpose.1"(#loc323))
#loc770 = loc("conv2d_633.dc.conv2d.2"(#loc324))
#loc771 = loc("conv2d_633.dc.transpose.3"(#loc325))
#loc772 = loc("conv2d_633.dc.transpose.4"(#loc326))
#loc773 = loc("multiply_641"(#loc327))
#loc774 = loc("add_647"(#loc328))
#loc775 = loc("add_648"(#loc329))
#loc776 = loc("relu_649"(#loc330))
#loc777 = loc("conv2d_650.dc.transpose.0"(#loc331))
#loc778 = loc("conv2d_650.dc.transpose.1"(#loc332))
#loc779 = loc("conv2d_650.dc.conv2d.2"(#loc333))
#loc780 = loc("conv2d_650.dc.transpose.3"(#loc334))
#loc781 = loc("conv2d_650.dc.transpose.4"(#loc335))
#loc782 = loc("multiply_658"(#loc336))
#loc783 = loc("add_664"(#loc337))
#loc784 = loc("relu_665"(#loc338))
#loc785 = loc("conv2d_666.dc.transpose.0"(#loc339))
#loc786 = loc("conv2d_666.dc.transpose.1"(#loc340))
#loc787 = loc("conv2d_666.dc.conv2d.2"(#loc341))
#loc788 = loc("conv2d_666.dc.transpose.3"(#loc342))
#loc789 = loc("conv2d_666.dc.transpose.4"(#loc343))
#loc790 = loc("multiply_674"(#loc344))
#loc791 = loc("add_680"(#loc345))
#loc792 = loc("relu_681"(#loc346))
#loc793 = loc("conv2d_682.dc.transpose.0"(#loc347))
#loc794 = loc("conv2d_682.dc.transpose.1"(#loc348))
#loc795 = loc("conv2d_682.dc.conv2d.2"(#loc349))
#loc796 = loc("conv2d_682.dc.transpose.3"(#loc350))
#loc797 = loc("conv2d_682.dc.transpose.4"(#loc351))
#loc798 = loc("multiply_690"(#loc352))
#loc799 = loc("add_696"(#loc353))
#loc800 = loc("add_697"(#loc354))
#loc801 = loc("relu_698"(#loc355))
#loc802 = loc("conv2d_699.dc.transpose.0"(#loc356))
#loc803 = loc("conv2d_699.dc.transpose.1"(#loc357))
#loc804 = loc("conv2d_699.dc.conv2d.2"(#loc358))
#loc805 = loc("conv2d_699.dc.transpose.3"(#loc359))
#loc806 = loc("conv2d_699.dc.transpose.4"(#loc360))
#loc807 = loc("multiply_707"(#loc361))
#loc808 = loc("add_713"(#loc362))
#loc809 = loc("relu_714"(#loc363))
#loc810 = loc("conv2d_715.dc.transpose.0"(#loc364))
#loc811 = loc("conv2d_715.dc.transpose.1"(#loc365))
#loc812 = loc("conv2d_715.dc.conv2d.2"(#loc366))
#loc813 = loc("conv2d_715.dc.transpose.3"(#loc367))
#loc814 = loc("conv2d_715.dc.transpose.4"(#loc368))
#loc815 = loc("multiply_723"(#loc369))
#loc816 = loc("add_729"(#loc370))
#loc817 = loc("relu_730"(#loc371))
#loc818 = loc("conv2d_731.dc.transpose.0"(#loc372))
#loc819 = loc("conv2d_731.dc.transpose.1"(#loc373))
#loc820 = loc("conv2d_731.dc.conv2d.2"(#loc374))
#loc821 = loc("conv2d_731.dc.transpose.3"(#loc375))
#loc822 = loc("conv2d_731.dc.transpose.4"(#loc376))
#loc823 = loc("multiply_739"(#loc377))
#loc824 = loc("add_745"(#loc378))
#loc825 = loc("conv2d_746.dc.transpose.0"(#loc379))
#loc826 = loc("conv2d_746.dc.transpose.1"(#loc380))
#loc827 = loc("conv2d_746.dc.conv2d.2"(#loc381))
#loc828 = loc("conv2d_746.dc.transpose.3"(#loc382))
#loc829 = loc("conv2d_746.dc.transpose.4"(#loc383))
#loc830 = loc("multiply_754"(#loc384))
#loc831 = loc("add_760"(#loc385))
#loc832 = loc("add_761"(#loc386))
#loc833 = loc("relu_762"(#loc387))
#loc834 = loc("conv2d_763.dc.transpose.0"(#loc388))
#loc835 = loc("conv2d_763.dc.transpose.1"(#loc389))
#loc836 = loc("conv2d_763.dc.conv2d.2"(#loc390))
#loc837 = loc("conv2d_763.dc.transpose.3"(#loc391))
#loc838 = loc("conv2d_763.dc.transpose.4"(#loc392))
#loc839 = loc("multiply_771"(#loc393))
#loc840 = loc("add_777"(#loc394))
#loc841 = loc("relu_778"(#loc395))
#loc842 = loc("conv2d_779.dc.transpose.0"(#loc396))
#loc843 = loc("conv2d_779.dc.transpose.1"(#loc397))
#loc844 = loc("conv2d_779.dc.conv2d.2"(#loc398))
#loc845 = loc("conv2d_779.dc.transpose.3"(#loc399))
#loc846 = loc("conv2d_779.dc.transpose.4"(#loc400))
#loc847 = loc("multiply_787"(#loc401))
#loc848 = loc("add_793"(#loc402))
#loc849 = loc("relu_794"(#loc403))
#loc850 = loc("conv2d_795.dc.transpose.0"(#loc404))
#loc851 = loc("conv2d_795.dc.transpose.1"(#loc405))
#loc852 = loc("conv2d_795.dc.conv2d.2"(#loc406))
#loc853 = loc("conv2d_795.dc.transpose.3"(#loc407))
#loc854 = loc("conv2d_795.dc.transpose.4"(#loc408))
#loc855 = loc("multiply_803"(#loc409))
#loc856 = loc("add_809"(#loc410))
#loc857 = loc("add_810"(#loc411))
#loc858 = loc("relu_811"(#loc412))
#loc859 = loc("conv2d_812.dc.transpose.0"(#loc413))
#loc860 = loc("conv2d_812.dc.transpose.1"(#loc414))
#loc861 = loc("conv2d_812.dc.conv2d.2"(#loc415))
#loc862 = loc("conv2d_812.dc.transpose.3"(#loc416))
#loc863 = loc("conv2d_812.dc.transpose.4"(#loc417))
#loc864 = loc("multiply_820"(#loc418))
#loc865 = loc("add_826"(#loc419))
#loc866 = loc("relu_827"(#loc420))
#loc867 = loc("conv2d_828.dc.transpose.0"(#loc421))
#loc868 = loc("conv2d_828.dc.transpose.1"(#loc422))
#loc869 = loc("conv2d_828.dc.conv2d.2"(#loc423))
#loc870 = loc("conv2d_828.dc.transpose.3"(#loc424))
#loc871 = loc("conv2d_828.dc.transpose.4"(#loc425))
#loc872 = loc("multiply_836"(#loc426))
#loc873 = loc("add_842"(#loc427))
#loc874 = loc("relu_843"(#loc428))
#loc875 = loc("conv2d_844.dc.transpose.0"(#loc429))
#loc876 = loc("conv2d_844.dc.transpose.1"(#loc430))
#loc877 = loc("conv2d_844.dc.conv2d.2"(#loc431))
#loc878 = loc("conv2d_844.dc.transpose.3"(#loc432))
#loc879 = loc("conv2d_844.dc.transpose.4"(#loc433))
#loc880 = loc("multiply_852"(#loc434))
#loc881 = loc("add_858"(#loc435))
#loc882 = loc("add_859"(#loc436))
#loc883 = loc("relu_860"(#loc437))
#loc884 = loc("avg_pool2d_861.dc.reshape.0"(#loc438))
#loc885 = loc("avg_pool2d_861.dc.transpose.1.dc.transpose.0"(#loc439))
#loc886 = loc("avg_pool2d_861.dc.reduce_avg.2"(#loc440))
#loc887 = loc("avg_pool2d_861.dc.reshape.4"(#loc441))
#loc888 = loc("squeeze_863"(#loc442))
#loc889 = loc("squeeze_864"(#loc443))
#loc890 = loc("matmul_866"(#loc444))
#loc891 = loc("add_867"(#loc445))
