module @SelfAttention attributes {tt.device = #tt.device<workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1)[s0, s1] -> (0, d0 floordiv s0, d1 floordiv s1, (d0 mod s0) * s1 + d1 mod s1), dramMap = (d0, d1)[s0, s1] -> (0, 0, ((((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 8192) mod 12, (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 98304 + (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) mod 8192), meshShape = , chipIds = [0]>, tt.system_desc = #tt.system_desc<[{role = host, target_triple = "x86_64-pc-linux-gnu"}], [{arch = <wormhole_b0>, grid = 8x8, l1_size = 1499136, num_dram_channels = 12, dram_channel_size = 1073741824, noc_l1_address_align_bytes = 16, pcie_address_align_bytes = 32, noc_dram_address_align_bytes = 32, l1_unreserved_base = 1024, erisc_l1_unreserved_base = 1024, dram_unreserved_base = 1024, dram_unreserved_end = 1073741824, physical_cores = {worker = [ 0x0,  0x1,  0x2,  0x3,  0x4,  0x5,  0x6,  0x7,  1x0,  1x1,  1x2,  1x3,  1x4,  1x5,  1x6,  1x7,  2x0,  2x1,  2x2,  2x3,  2x4,  2x5,  2x6,  2x7,  3x0,  3x1,  3x2,  3x3,  3x4,  3x5,  3x6,  3x7,  4x0,  4x1,  4x2,  4x3,  4x4,  4x5,  4x6,  4x7,  5x0,  5x1,  5x2,  5x3,  5x4,  5x5,  5x6,  5x7,  6x0,  6x1,  6x2,  6x3,  6x4,  6x5,  6x6,  6x7,  7x0,  7x1,  7x2,  7x3,  7x4,  7x5,  7x6,  7x7] dram = [ 8x0,  9x0,  10x0,  8x1,  9x1,  10x1,  8x2,  9x2,  10x2,  8x3,  9x3,  10x3]}, supported_data_types = [<f32>, <f16>, <bf16>, <bfp_f8>, <bfp_bf8>, <bfp_f4>, <bfp_bf4>, <bfp_f2>, <bfp_bf2>, <u32>, <u16>, <u8>], supported_tile_sizes = [ 4x16,  16x16,  32x16,  4x32,  16x32,  32x32], num_cbs = 32}], [0], [3 : i32], [ 0x0x0x0]>} {
  func.func @forward(%arg0: tensor<1x12x3200xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 12 + d1, d2), <1x1>, memref<12x3200xf32, #ttnn.buffer_type<system_memory>>>> {ttir.name = "hidden_states_1"}, %arg1: tensor<1x1x12x12xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 12 + d1 * 12 + d2, d3), <1x1>, memref<12x12xf32, #ttnn.buffer_type<system_memory>>>> {ttir.name = "attention_mask"}, %arg2: tensor<1x12xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x12xf32, #ttnn.buffer_type<system_memory>>>> {ttir.name = "position_ids"}, %arg3: tensor<1x50x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 50 + d1, d2), <1x1>, memref<50x1xf32, #ttnn.buffer_type<system_memory>>>> {ttir.name = "input_0_unsqueeze_12"}, %arg4: tensor<1x32x50x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1600 + d1 * 50 + d2, d3), <1x1>, memref<1600x100xf32, #ttnn.buffer_type<system_memory>>>> {ttir.name = "dc.input_tensor.index_25.2"}, %arg5: tensor<1xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xf32, #ttnn.buffer_type<system_memory>>>> {ttir.name = "input_1_multiply_26"}, %arg6: tensor<1x32x50x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1600 + d1 * 50 + d2, d3), <1x1>, memref<1600x100xf32, #ttnn.buffer_type<system_memory>>>> {ttir.name = "dc.input_tensor.index_27.2"}, %arg7: tensor<1x32x50x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1600 + d1 * 50 + d2, d3), <1x1>, memref<1600x100xf32, #ttnn.buffer_type<system_memory>>>> {ttir.name = "dc.input_tensor.index_39.2"}, %arg8: tensor<1xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xf32, #ttnn.buffer_type<system_memory>>>> {ttir.name = "input_1_multiply_40"}, %arg9: tensor<1x32x50x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1600 + d1 * 50 + d2, d3), <1x1>, memref<1600x100xf32, #ttnn.buffer_type<system_memory>>>> {ttir.name = "dc.input_tensor.index_41.2"}, %arg10: tensor<1xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xf32, #ttnn.buffer_type<system_memory>>>> {ttir.name = "input_1_multiply_49"}, %arg11: tensor<3200x3200xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<3200x3200xf32, #ttnn.buffer_type<system_memory>>>> {ttir.name = "model.q_proj.weight"}, %arg12: tensor<3200x3200xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<3200x3200xf32, #ttnn.buffer_type<system_memory>>>> {ttir.name = "model.k_proj.weight"}, %arg13: tensor<3200x3200xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<3200x3200xf32, #ttnn.buffer_type<system_memory>>>> {ttir.name = "model.v_proj.weight"}, %arg14: tensor<3200x3200xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<3200x3200xf32, #ttnn.buffer_type<system_memory>>>> {ttir.name = "model.o_proj.weight"}) -> (tensor<1x12x3200xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 12 + d1, d2), <1x1>, memref<12x3200xf32, #ttnn.buffer_type<system_memory>>>> {ttir.name = "SelfAttention.output_reshape_67"}) {
    %0 = "ttnn.get_device"() <{mesh_shape = #ttnn<mesh_shape 1x1>}> : () -> !tt.device<<workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1)[s0, s1] -> (0, d0 floordiv s0, d1 floordiv s1, (d0 mod s0) * s1 + d1 mod s1), dramMap = (d0, d1)[s0, s1] -> (0, 0, ((((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 8192) mod 12, (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 98304 + (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) mod 8192), meshShape = , chipIds = [0]>>
    %1 = "ttnn.to_layout"(%arg0) <{layout = #ttnn.layout<tile>}> : (tensor<1x12x3200xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 12 + d1, d2), <1x1>, memref<12x3200xf32, #ttnn.buffer_type<system_memory>>>>) -> tensor<1x12x3200xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 12 + d1, d2), <1x1>, memref<1x100x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %2 = "ttnn.to_device"(%1, %0) <{memory_config = #ttnn.memory_config<<dram>, <<1x100>>, <interleaved>>}> : (tensor<1x12x3200xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 12 + d1, d2), <1x1>, memref<1x100x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !tt.device<<workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1)[s0, s1] -> (0, d0 floordiv s0, d1 floordiv s1, (d0 mod s0) * s1 + d1 mod s1), dramMap = (d0, d1)[s0, s1] -> (0, 0, ((((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 8192) mod 12, (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 98304 + (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) mod 8192), meshShape = , chipIds = [0]>>) -> tensor<1x12x3200xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 12 + d1, d2), <1x1>, memref<1x100x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    "ttnn.deallocate"(%1) <{force = false}> : (tensor<1x12x3200xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 12 + d1, d2), <1x1>, memref<1x100x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    %3 = "ttnn.reshape"(%2) <{shape = [12 : i32, 3200 : i32]}> : (tensor<1x12x3200xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 12 + d1, d2), <1x1>, memref<1x100x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x3200xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x100x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    "ttnn.deallocate"(%2) <{force = false}> : (tensor<1x12x3200xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 12 + d1, d2), <1x1>, memref<1x100x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    %4 = "ttnn.to_layout"(%arg11) <{layout = #ttnn.layout<tile>}> : (tensor<3200x3200xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<3200x3200xf32, #ttnn.buffer_type<system_memory>>>>) -> tensor<3200x3200xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<100x100x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %5 = "ttnn.to_device"(%4, %0) <{memory_config = #ttnn.memory_config<<dram>, <<100x100>>, <interleaved>>}> : (tensor<3200x3200xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<100x100x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !tt.device<<workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1)[s0, s1] -> (0, d0 floordiv s0, d1 floordiv s1, (d0 mod s0) * s1 + d1 mod s1), dramMap = (d0, d1)[s0, s1] -> (0, 0, ((((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 8192) mod 12, (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 98304 + (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) mod 8192), meshShape = , chipIds = [0]>>) -> tensor<3200x3200xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<100x100x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    "ttnn.deallocate"(%4) <{force = false}> : (tensor<3200x3200xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<100x100x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    %6 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<<dram>, <<1x100>>, <interleaved>>, shape = #ttnn.shape<12x3200>}> : (!tt.device<<workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1)[s0, s1] -> (0, d0 floordiv s0, d1 floordiv s1, (d0 mod s0) * s1 + d1 mod s1), dramMap = (d0, d1)[s0, s1] -> (0, 0, ((((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 8192) mod 12, (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 98304 + (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) mod 8192), meshShape = , chipIds = [0]>>) -> tensor<12x3200xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x100x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %7 = "ttnn.matmul"(%3, %5, %6) : (tensor<12x3200xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x100x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<3200x3200xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<100x100x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<12x3200xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x100x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x3200xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x100x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    "ttnn.deallocate"(%5) <{force = false}> : (tensor<3200x3200xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<100x100x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    %8 = "ttnn.reshape"(%7) <{shape = [1 : i32, 12 : i32, 32 : i32, 100 : i32]}> : (tensor<12x3200xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x100x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x32x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    "ttnn.deallocate"(%6) <{force = false}> : (tensor<12x3200xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x100x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    %9 = "ttnn.transpose"(%8) <{dim0 = 1 : si32, dim1 = 2 : si32}> : (tensor<1x12x32x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x32x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    "ttnn.deallocate"(%8) <{force = false}> : (tensor<1x12x32x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    %10 = "ttnn.to_layout"(%arg2) <{layout = #ttnn.layout<tile>}> : (tensor<1x12xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x12xf32, #ttnn.buffer_type<system_memory>>>>) -> tensor<1x12xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %11 = "ttnn.to_device"(%10, %0) <{memory_config = #ttnn.memory_config<<dram>, <<1x1>>, <interleaved>>}> : (tensor<1x12xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !tt.device<<workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1)[s0, s1] -> (0, d0 floordiv s0, d1 floordiv s1, (d0 mod s0) * s1 + d1 mod s1), dramMap = (d0, d1)[s0, s1] -> (0, 0, ((((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 8192) mod 12, (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 98304 + (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) mod 8192), meshShape = , chipIds = [0]>>) -> tensor<1x12xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    "ttnn.deallocate"(%10) <{force = false}> : (tensor<1x12xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    %12 = "ttnn.reshape"(%11) <{shape = [1 : i32, 1 : i32, 12 : i32]}> : (tensor<1x12xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x12xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 + d1, d2), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    "ttnn.deallocate"(%11) <{force = false}> : (tensor<1x12xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    %13 = "ttnn.to_layout"(%arg3) <{layout = #ttnn.layout<tile>}> : (tensor<1x50x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 50 + d1, d2), <1x1>, memref<50x1xf32, #ttnn.buffer_type<system_memory>>>>) -> tensor<1x50x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 50 + d1, d2), <1x1>, memref<2x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %14 = "ttnn.to_device"(%13, %0) <{memory_config = #ttnn.memory_config<<dram>, <<2x1>>, <interleaved>>}> : (tensor<1x50x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 50 + d1, d2), <1x1>, memref<2x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !tt.device<<workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1)[s0, s1] -> (0, d0 floordiv s0, d1 floordiv s1, (d0 mod s0) * s1 + d1 mod s1), dramMap = (d0, d1)[s0, s1] -> (0, 0, ((((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 8192) mod 12, (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 98304 + (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) mod 8192), meshShape = , chipIds = [0]>>) -> tensor<1x50x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 50 + d1, d2), <1x1>, memref<2x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    "ttnn.deallocate"(%13) <{force = false}> : (tensor<1x50x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 50 + d1, d2), <1x1>, memref<2x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    %15 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<<dram>, <<2x1>>, <interleaved>>, shape = #ttnn.shape<1x50x12>}> : (!tt.device<<workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1)[s0, s1] -> (0, d0 floordiv s0, d1 floordiv s1, (d0 mod s0) * s1 + d1 mod s1), dramMap = (d0, d1)[s0, s1] -> (0, 0, ((((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 8192) mod 12, (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 98304 + (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) mod 8192), meshShape = , chipIds = [0]>>) -> tensor<1x50x12xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 50 + d1, d2), <1x1>, memref<2x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %16 = "ttnn.matmul"(%14, %12, %15) : (tensor<1x50x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 50 + d1, d2), <1x1>, memref<2x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x12xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 + d1, d2), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x50x12xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 50 + d1, d2), <1x1>, memref<2x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x50x12xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 50 + d1, d2), <1x1>, memref<2x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    "ttnn.deallocate"(%14) <{force = false}> : (tensor<1x50x1xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 50 + d1, d2), <1x1>, memref<2x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    "ttnn.deallocate"(%12) <{force = false}> : (tensor<1x1x12xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 + d1, d2), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    %17 = "ttnn.transpose"(%16) <{dim0 = 1 : si32, dim1 = 2 : si32}> : (tensor<1x50x12xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 50 + d1, d2), <1x1>, memref<2x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x50xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 12 + d1, d2), <1x1>, memref<1x2x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    "ttnn.deallocate"(%15) <{force = false}> : (tensor<1x50x12xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 50 + d1, d2), <1x1>, memref<2x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    %18 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<<dram>, <<1x4>>, <interleaved>>, shape = #ttnn.shape<1x12x100>}> : (!tt.device<<workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1)[s0, s1] -> (0, d0 floordiv s0, d1 floordiv s1, (d0 mod s0) * s1 + d1 mod s1), dramMap = (d0, d1)[s0, s1] -> (0, 0, ((((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 8192) mod 12, (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 98304 + (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) mod 8192), meshShape = , chipIds = [0]>>) -> tensor<1x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 12 + d1, d2), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %19 = "ttnn.concat"(%17, %17, %18) <{dim = 2 : si32}> : (tensor<1x12x50xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 12 + d1, d2), <1x1>, memref<1x2x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x12x50xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 12 + d1, d2), <1x1>, memref<1x2x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 12 + d1, d2), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 12 + d1, d2), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    "ttnn.deallocate"(%17) <{force = false}> : (tensor<1x12x50xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 12 + d1, d2), <1x1>, memref<1x2x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    %20 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<<dram>, <<1x4>>, <interleaved>>, shape = #ttnn.shape<1x12x100>}> : (!tt.device<<workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1)[s0, s1] -> (0, d0 floordiv s0, d1 floordiv s1, (d0 mod s0) * s1 + d1 mod s1), dramMap = (d0, d1)[s0, s1] -> (0, 0, ((((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 8192) mod 12, (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 98304 + (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) mod 8192), meshShape = , chipIds = [0]>>) -> tensor<1x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 12 + d1, d2), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %21 = "ttnn.cos"(%19, %20) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 12 + d1, d2), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 12 + d1, d2), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 12 + d1, d2), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %22 = "ttnn.reshape"(%21) <{shape = [1 : i32, 1 : i32, 12 : i32, 100 : i32]}> : (tensor<1x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 12 + d1, d2), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 12 + d1 * 12 + d2, d3), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    "ttnn.deallocate"(%20) <{force = false}> : (tensor<1x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 12 + d1, d2), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    %23 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<<dram>, <<12x4>>, <interleaved>>, shape = #ttnn.shape<1x32x12x100>}> : (!tt.device<<workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1)[s0, s1] -> (0, d0 floordiv s0, d1 floordiv s1, (d0 mod s0) * s1 + d1 mod s1), dramMap = (d0, d1)[s0, s1] -> (0, 0, ((((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 8192) mod 12, (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 98304 + (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) mod 8192), meshShape = , chipIds = [0]>>) -> tensor<1x32x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %24 = "ttnn.multiply"(%9, %22, %23) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 12 + d1 * 12 + d2, d3), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x32x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x32x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %25 = "ttnn.transpose"(%9) <{dim0 = 2 : si32, dim1 = 3 : si32}> : (tensor<1x32x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x32x100x12xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3200 + d1 * 100 + d2, d3), <1x1>, memref<100x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %26 = "ttnn.to_layout"(%arg4) <{layout = #ttnn.layout<tile>}> : (tensor<1x32x50x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1600 + d1 * 50 + d2, d3), <1x1>, memref<1600x100xf32, #ttnn.buffer_type<system_memory>>>>) -> tensor<1x32x50x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1600 + d1 * 50 + d2, d3), <1x1>, memref<50x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %27 = "ttnn.to_device"(%26, %0) <{memory_config = #ttnn.memory_config<<dram>, <<50x4>>, <interleaved>>}> : (tensor<1x32x50x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1600 + d1 * 50 + d2, d3), <1x1>, memref<50x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !tt.device<<workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1)[s0, s1] -> (0, d0 floordiv s0, d1 floordiv s1, (d0 mod s0) * s1 + d1 mod s1), dramMap = (d0, d1)[s0, s1] -> (0, 0, ((((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 8192) mod 12, (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 98304 + (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) mod 8192), meshShape = , chipIds = [0]>>) -> tensor<1x32x50x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1600 + d1 * 50 + d2, d3), <1x1>, memref<50x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    "ttnn.deallocate"(%26) <{force = false}> : (tensor<1x32x50x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1600 + d1 * 50 + d2, d3), <1x1>, memref<50x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    %28 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<<dram>, <<50x1>>, <interleaved>>, shape = #ttnn.shape<1x32x50x12>}> : (!tt.device<<workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1)[s0, s1] -> (0, d0 floordiv s0, d1 floordiv s1, (d0 mod s0) * s1 + d1 mod s1), dramMap = (d0, d1)[s0, s1] -> (0, 0, ((((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 8192) mod 12, (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 98304 + (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) mod 8192), meshShape = , chipIds = [0]>>) -> tensor<1x32x50x12xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1600 + d1 * 50 + d2, d3), <1x1>, memref<50x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %29 = "ttnn.matmul"(%27, %25, %28) : (tensor<1x32x50x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1600 + d1 * 50 + d2, d3), <1x1>, memref<50x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x32x100x12xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3200 + d1 * 100 + d2, d3), <1x1>, memref<100x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x32x50x12xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1600 + d1 * 50 + d2, d3), <1x1>, memref<50x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x32x50x12xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1600 + d1 * 50 + d2, d3), <1x1>, memref<50x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    "ttnn.deallocate"(%27) <{force = false}> : (tensor<1x32x50x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1600 + d1 * 50 + d2, d3), <1x1>, memref<50x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    "ttnn.deallocate"(%25) <{force = false}> : (tensor<1x32x100x12xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3200 + d1 * 100 + d2, d3), <1x1>, memref<100x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    %30 = "ttnn.transpose"(%29) <{dim0 = 2 : si32, dim1 = 3 : si32}> : (tensor<1x32x50x12xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1600 + d1 * 50 + d2, d3), <1x1>, memref<50x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x32x12x50xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x2x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    "ttnn.deallocate"(%28) <{force = false}> : (tensor<1x32x50x12xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1600 + d1 * 50 + d2, d3), <1x1>, memref<50x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    %31 = "ttnn.to_layout"(%arg5) <{layout = #ttnn.layout<tile>}> : (tensor<1xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xf32, #ttnn.buffer_type<system_memory>>>>) -> tensor<1xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %32 = "ttnn.to_device"(%31, %0) <{memory_config = #ttnn.memory_config<<dram>, <<1x1>>, <interleaved>>}> : (tensor<1xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !tt.device<<workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1)[s0, s1] -> (0, d0 floordiv s0, d1 floordiv s1, (d0 mod s0) * s1 + d1 mod s1), dramMap = (d0, d1)[s0, s1] -> (0, 0, ((((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 8192) mod 12, (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 98304 + (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) mod 8192), meshShape = , chipIds = [0]>>) -> tensor<1xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    "ttnn.deallocate"(%31) <{force = false}> : (tensor<1xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    %33 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<<dram>, <<12x2>>, <interleaved>>, shape = #ttnn.shape<1x32x12x50>}> : (!tt.device<<workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1)[s0, s1] -> (0, d0 floordiv s0, d1 floordiv s1, (d0 mod s0) * s1 + d1 mod s1), dramMap = (d0, d1)[s0, s1] -> (0, 0, ((((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 8192) mod 12, (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 98304 + (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) mod 8192), meshShape = , chipIds = [0]>>) -> tensor<1x32x12x50xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x2x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %34 = "ttnn.multiply"(%30, %32, %33) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x12x50xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x2x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x32x12x50xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x2x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x32x12x50xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x2x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    "ttnn.deallocate"(%32) <{force = false}> : (tensor<1xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    "ttnn.deallocate"(%30) <{force = false}> : (tensor<1x32x12x50xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x2x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    %35 = "ttnn.transpose"(%9) <{dim0 = 2 : si32, dim1 = 3 : si32}> : (tensor<1x32x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x32x100x12xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3200 + d1 * 100 + d2, d3), <1x1>, memref<100x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    "ttnn.deallocate"(%9) <{force = false}> : (tensor<1x32x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    %36 = "ttnn.to_layout"(%arg6) <{layout = #ttnn.layout<tile>}> : (tensor<1x32x50x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1600 + d1 * 50 + d2, d3), <1x1>, memref<1600x100xf32, #ttnn.buffer_type<system_memory>>>>) -> tensor<1x32x50x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1600 + d1 * 50 + d2, d3), <1x1>, memref<50x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %37 = "ttnn.to_device"(%36, %0) <{memory_config = #ttnn.memory_config<<dram>, <<50x4>>, <interleaved>>}> : (tensor<1x32x50x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1600 + d1 * 50 + d2, d3), <1x1>, memref<50x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !tt.device<<workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1)[s0, s1] -> (0, d0 floordiv s0, d1 floordiv s1, (d0 mod s0) * s1 + d1 mod s1), dramMap = (d0, d1)[s0, s1] -> (0, 0, ((((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 8192) mod 12, (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 98304 + (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) mod 8192), meshShape = , chipIds = [0]>>) -> tensor<1x32x50x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1600 + d1 * 50 + d2, d3), <1x1>, memref<50x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    "ttnn.deallocate"(%36) <{force = false}> : (tensor<1x32x50x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1600 + d1 * 50 + d2, d3), <1x1>, memref<50x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    %38 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<<dram>, <<50x1>>, <interleaved>>, shape = #ttnn.shape<1x32x50x12>}> : (!tt.device<<workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1)[s0, s1] -> (0, d0 floordiv s0, d1 floordiv s1, (d0 mod s0) * s1 + d1 mod s1), dramMap = (d0, d1)[s0, s1] -> (0, 0, ((((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 8192) mod 12, (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 98304 + (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) mod 8192), meshShape = , chipIds = [0]>>) -> tensor<1x32x50x12xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1600 + d1 * 50 + d2, d3), <1x1>, memref<50x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %39 = "ttnn.matmul"(%37, %35, %38) : (tensor<1x32x50x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1600 + d1 * 50 + d2, d3), <1x1>, memref<50x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x32x100x12xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3200 + d1 * 100 + d2, d3), <1x1>, memref<100x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x32x50x12xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1600 + d1 * 50 + d2, d3), <1x1>, memref<50x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x32x50x12xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1600 + d1 * 50 + d2, d3), <1x1>, memref<50x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    "ttnn.deallocate"(%37) <{force = false}> : (tensor<1x32x50x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1600 + d1 * 50 + d2, d3), <1x1>, memref<50x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    "ttnn.deallocate"(%35) <{force = false}> : (tensor<1x32x100x12xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3200 + d1 * 100 + d2, d3), <1x1>, memref<100x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    %40 = "ttnn.transpose"(%39) <{dim0 = 2 : si32, dim1 = 3 : si32}> : (tensor<1x32x50x12xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1600 + d1 * 50 + d2, d3), <1x1>, memref<50x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x32x12x50xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x2x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    "ttnn.deallocate"(%38) <{force = false}> : (tensor<1x32x50x12xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1600 + d1 * 50 + d2, d3), <1x1>, memref<50x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    %41 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<<dram>, <<12x4>>, <interleaved>>, shape = #ttnn.shape<1x32x12x100>}> : (!tt.device<<workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1)[s0, s1] -> (0, d0 floordiv s0, d1 floordiv s1, (d0 mod s0) * s1 + d1 mod s1), dramMap = (d0, d1)[s0, s1] -> (0, 0, ((((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 8192) mod 12, (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 98304 + (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) mod 8192), meshShape = , chipIds = [0]>>) -> tensor<1x32x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %42 = "ttnn.concat"(%34, %40, %41) <{dim = 3 : si32}> : (tensor<1x32x12x50xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x2x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x32x12x50xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x2x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x32x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x32x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    "ttnn.deallocate"(%40) <{force = false}> : (tensor<1x32x12x50xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x2x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    "ttnn.deallocate"(%33) <{force = false}> : (tensor<1x32x12x50xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x2x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    %43 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<<dram>, <<1x4>>, <interleaved>>, shape = #ttnn.shape<1x12x100>}> : (!tt.device<<workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1)[s0, s1] -> (0, d0 floordiv s0, d1 floordiv s1, (d0 mod s0) * s1 + d1 mod s1), dramMap = (d0, d1)[s0, s1] -> (0, 0, ((((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 8192) mod 12, (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 98304 + (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) mod 8192), meshShape = , chipIds = [0]>>) -> tensor<1x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 12 + d1, d2), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %44 = "ttnn.sin"(%19, %43) <{operandSegmentSizes = array<i32: 1, 1>}> : (tensor<1x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 12 + d1, d2), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 12 + d1, d2), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 12 + d1, d2), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    "ttnn.deallocate"(%18) <{force = false}> : (tensor<1x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 12 + d1, d2), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    %45 = "ttnn.reshape"(%44) <{shape = [1 : i32, 1 : i32, 12 : i32, 100 : i32]}> : (tensor<1x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 12 + d1, d2), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x1x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 12 + d1 * 12 + d2, d3), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    "ttnn.deallocate"(%43) <{force = false}> : (tensor<1x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 12 + d1, d2), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    %46 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<<dram>, <<12x4>>, <interleaved>>, shape = #ttnn.shape<1x32x12x100>}> : (!tt.device<<workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1)[s0, s1] -> (0, d0 floordiv s0, d1 floordiv s1, (d0 mod s0) * s1 + d1 mod s1), dramMap = (d0, d1)[s0, s1] -> (0, 0, ((((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 8192) mod 12, (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 98304 + (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) mod 8192), meshShape = , chipIds = [0]>>) -> tensor<1x32x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %47 = "ttnn.multiply"(%42, %45, %46) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 12 + d1 * 12 + d2, d3), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x32x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x32x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    "ttnn.deallocate"(%41) <{force = false}> : (tensor<1x32x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    %48 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<<dram>, <<12x4>>, <interleaved>>, shape = #ttnn.shape<1x32x12x100>}> : (!tt.device<<workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1)[s0, s1] -> (0, d0 floordiv s0, d1 floordiv s1, (d0 mod s0) * s1 + d1 mod s1), dramMap = (d0, d1)[s0, s1] -> (0, 0, ((((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 8192) mod 12, (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 98304 + (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) mod 8192), meshShape = , chipIds = [0]>>) -> tensor<1x32x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %49 = "ttnn.add"(%24, %47, %48) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x32x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x32x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x32x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    "ttnn.deallocate"(%46) <{force = false}> : (tensor<1x32x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    "ttnn.deallocate"(%23) <{force = false}> : (tensor<1x32x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    %50 = "ttnn.reshape"(%49) <{shape = [32 : i32, 12 : i32, 100 : i32]}> : (tensor<1x32x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<32x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 12 + d1, d2), <1x1>, memref<12x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    "ttnn.deallocate"(%48) <{force = false}> : (tensor<1x32x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    %51 = "ttnn.to_layout"(%arg12) <{layout = #ttnn.layout<tile>}> : (tensor<3200x3200xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<3200x3200xf32, #ttnn.buffer_type<system_memory>>>>) -> tensor<3200x3200xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<100x100x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %52 = "ttnn.to_device"(%51, %0) <{memory_config = #ttnn.memory_config<<dram>, <<100x100>>, <interleaved>>}> : (tensor<3200x3200xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<100x100x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !tt.device<<workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1)[s0, s1] -> (0, d0 floordiv s0, d1 floordiv s1, (d0 mod s0) * s1 + d1 mod s1), dramMap = (d0, d1)[s0, s1] -> (0, 0, ((((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 8192) mod 12, (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 98304 + (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) mod 8192), meshShape = , chipIds = [0]>>) -> tensor<3200x3200xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<100x100x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    "ttnn.deallocate"(%51) <{force = false}> : (tensor<3200x3200xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<100x100x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    %53 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<<dram>, <<1x100>>, <interleaved>>, shape = #ttnn.shape<12x3200>}> : (!tt.device<<workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1)[s0, s1] -> (0, d0 floordiv s0, d1 floordiv s1, (d0 mod s0) * s1 + d1 mod s1), dramMap = (d0, d1)[s0, s1] -> (0, 0, ((((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 8192) mod 12, (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 98304 + (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) mod 8192), meshShape = , chipIds = [0]>>) -> tensor<12x3200xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x100x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %54 = "ttnn.matmul"(%3, %52, %53) : (tensor<12x3200xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x100x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<3200x3200xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<100x100x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<12x3200xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x100x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x3200xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x100x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    "ttnn.deallocate"(%52) <{force = false}> : (tensor<3200x3200xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<100x100x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    %55 = "ttnn.reshape"(%54) <{shape = [1 : i32, 12 : i32, 32 : i32, 100 : i32]}> : (tensor<12x3200xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x100x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x32x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    "ttnn.deallocate"(%53) <{force = false}> : (tensor<12x3200xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x100x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    %56 = "ttnn.transpose"(%55) <{dim0 = 1 : si32, dim1 = 2 : si32}> : (tensor<1x12x32x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x32x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    "ttnn.deallocate"(%55) <{force = false}> : (tensor<1x12x32x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    %57 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<<dram>, <<12x4>>, <interleaved>>, shape = #ttnn.shape<1x32x12x100>}> : (!tt.device<<workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1)[s0, s1] -> (0, d0 floordiv s0, d1 floordiv s1, (d0 mod s0) * s1 + d1 mod s1), dramMap = (d0, d1)[s0, s1] -> (0, 0, ((((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 8192) mod 12, (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 98304 + (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) mod 8192), meshShape = , chipIds = [0]>>) -> tensor<1x32x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %58 = "ttnn.multiply"(%56, %22, %57) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 12 + d1 * 12 + d2, d3), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x32x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x32x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    "ttnn.deallocate"(%22) <{force = false}> : (tensor<1x1x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 12 + d1 * 12 + d2, d3), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    %59 = "ttnn.transpose"(%56) <{dim0 = 2 : si32, dim1 = 3 : si32}> : (tensor<1x32x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x32x100x12xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3200 + d1 * 100 + d2, d3), <1x1>, memref<100x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %60 = "ttnn.to_layout"(%arg7) <{layout = #ttnn.layout<tile>}> : (tensor<1x32x50x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1600 + d1 * 50 + d2, d3), <1x1>, memref<1600x100xf32, #ttnn.buffer_type<system_memory>>>>) -> tensor<1x32x50x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1600 + d1 * 50 + d2, d3), <1x1>, memref<50x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %61 = "ttnn.to_device"(%60, %0) <{memory_config = #ttnn.memory_config<<dram>, <<50x4>>, <interleaved>>}> : (tensor<1x32x50x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1600 + d1 * 50 + d2, d3), <1x1>, memref<50x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !tt.device<<workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1)[s0, s1] -> (0, d0 floordiv s0, d1 floordiv s1, (d0 mod s0) * s1 + d1 mod s1), dramMap = (d0, d1)[s0, s1] -> (0, 0, ((((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 8192) mod 12, (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 98304 + (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) mod 8192), meshShape = , chipIds = [0]>>) -> tensor<1x32x50x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1600 + d1 * 50 + d2, d3), <1x1>, memref<50x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    "ttnn.deallocate"(%60) <{force = false}> : (tensor<1x32x50x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1600 + d1 * 50 + d2, d3), <1x1>, memref<50x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    %62 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<<dram>, <<50x1>>, <interleaved>>, shape = #ttnn.shape<1x32x50x12>}> : (!tt.device<<workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1)[s0, s1] -> (0, d0 floordiv s0, d1 floordiv s1, (d0 mod s0) * s1 + d1 mod s1), dramMap = (d0, d1)[s0, s1] -> (0, 0, ((((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 8192) mod 12, (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 98304 + (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) mod 8192), meshShape = , chipIds = [0]>>) -> tensor<1x32x50x12xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1600 + d1 * 50 + d2, d3), <1x1>, memref<50x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %63 = "ttnn.matmul"(%61, %59, %62) : (tensor<1x32x50x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1600 + d1 * 50 + d2, d3), <1x1>, memref<50x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x32x100x12xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3200 + d1 * 100 + d2, d3), <1x1>, memref<100x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x32x50x12xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1600 + d1 * 50 + d2, d3), <1x1>, memref<50x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x32x50x12xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1600 + d1 * 50 + d2, d3), <1x1>, memref<50x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    "ttnn.deallocate"(%61) <{force = false}> : (tensor<1x32x50x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1600 + d1 * 50 + d2, d3), <1x1>, memref<50x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    "ttnn.deallocate"(%59) <{force = false}> : (tensor<1x32x100x12xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3200 + d1 * 100 + d2, d3), <1x1>, memref<100x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    %64 = "ttnn.transpose"(%63) <{dim0 = 2 : si32, dim1 = 3 : si32}> : (tensor<1x32x50x12xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1600 + d1 * 50 + d2, d3), <1x1>, memref<50x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x32x12x50xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x2x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    "ttnn.deallocate"(%62) <{force = false}> : (tensor<1x32x50x12xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1600 + d1 * 50 + d2, d3), <1x1>, memref<50x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    %65 = "ttnn.to_layout"(%arg8) <{layout = #ttnn.layout<tile>}> : (tensor<1xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xf32, #ttnn.buffer_type<system_memory>>>>) -> tensor<1xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %66 = "ttnn.to_device"(%65, %0) <{memory_config = #ttnn.memory_config<<dram>, <<1x1>>, <interleaved>>}> : (tensor<1xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !tt.device<<workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1)[s0, s1] -> (0, d0 floordiv s0, d1 floordiv s1, (d0 mod s0) * s1 + d1 mod s1), dramMap = (d0, d1)[s0, s1] -> (0, 0, ((((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 8192) mod 12, (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 98304 + (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) mod 8192), meshShape = , chipIds = [0]>>) -> tensor<1xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    "ttnn.deallocate"(%65) <{force = false}> : (tensor<1xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    %67 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<<dram>, <<12x2>>, <interleaved>>, shape = #ttnn.shape<1x32x12x50>}> : (!tt.device<<workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1)[s0, s1] -> (0, d0 floordiv s0, d1 floordiv s1, (d0 mod s0) * s1 + d1 mod s1), dramMap = (d0, d1)[s0, s1] -> (0, 0, ((((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 8192) mod 12, (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 98304 + (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) mod 8192), meshShape = , chipIds = [0]>>) -> tensor<1x32x12x50xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x2x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %68 = "ttnn.multiply"(%64, %66, %67) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x12x50xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x2x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x32x12x50xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x2x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x32x12x50xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x2x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    "ttnn.deallocate"(%66) <{force = false}> : (tensor<1xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    "ttnn.deallocate"(%64) <{force = false}> : (tensor<1x32x12x50xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x2x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    %69 = "ttnn.transpose"(%56) <{dim0 = 2 : si32, dim1 = 3 : si32}> : (tensor<1x32x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x32x100x12xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3200 + d1 * 100 + d2, d3), <1x1>, memref<100x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    "ttnn.deallocate"(%56) <{force = false}> : (tensor<1x32x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    %70 = "ttnn.to_layout"(%arg9) <{layout = #ttnn.layout<tile>}> : (tensor<1x32x50x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1600 + d1 * 50 + d2, d3), <1x1>, memref<1600x100xf32, #ttnn.buffer_type<system_memory>>>>) -> tensor<1x32x50x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1600 + d1 * 50 + d2, d3), <1x1>, memref<50x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %71 = "ttnn.to_device"(%70, %0) <{memory_config = #ttnn.memory_config<<dram>, <<50x4>>, <interleaved>>}> : (tensor<1x32x50x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1600 + d1 * 50 + d2, d3), <1x1>, memref<50x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !tt.device<<workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1)[s0, s1] -> (0, d0 floordiv s0, d1 floordiv s1, (d0 mod s0) * s1 + d1 mod s1), dramMap = (d0, d1)[s0, s1] -> (0, 0, ((((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 8192) mod 12, (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 98304 + (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) mod 8192), meshShape = , chipIds = [0]>>) -> tensor<1x32x50x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1600 + d1 * 50 + d2, d3), <1x1>, memref<50x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    "ttnn.deallocate"(%70) <{force = false}> : (tensor<1x32x50x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1600 + d1 * 50 + d2, d3), <1x1>, memref<50x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    %72 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<<dram>, <<50x1>>, <interleaved>>, shape = #ttnn.shape<1x32x50x12>}> : (!tt.device<<workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1)[s0, s1] -> (0, d0 floordiv s0, d1 floordiv s1, (d0 mod s0) * s1 + d1 mod s1), dramMap = (d0, d1)[s0, s1] -> (0, 0, ((((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 8192) mod 12, (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 98304 + (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) mod 8192), meshShape = , chipIds = [0]>>) -> tensor<1x32x50x12xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1600 + d1 * 50 + d2, d3), <1x1>, memref<50x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %73 = "ttnn.matmul"(%71, %69, %72) : (tensor<1x32x50x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1600 + d1 * 50 + d2, d3), <1x1>, memref<50x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x32x100x12xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3200 + d1 * 100 + d2, d3), <1x1>, memref<100x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x32x50x12xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1600 + d1 * 50 + d2, d3), <1x1>, memref<50x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x32x50x12xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1600 + d1 * 50 + d2, d3), <1x1>, memref<50x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    "ttnn.deallocate"(%71) <{force = false}> : (tensor<1x32x50x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1600 + d1 * 50 + d2, d3), <1x1>, memref<50x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    "ttnn.deallocate"(%69) <{force = false}> : (tensor<1x32x100x12xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3200 + d1 * 100 + d2, d3), <1x1>, memref<100x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    %74 = "ttnn.transpose"(%73) <{dim0 = 2 : si32, dim1 = 3 : si32}> : (tensor<1x32x50x12xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1600 + d1 * 50 + d2, d3), <1x1>, memref<50x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x32x12x50xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x2x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    "ttnn.deallocate"(%72) <{force = false}> : (tensor<1x32x50x12xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 1600 + d1 * 50 + d2, d3), <1x1>, memref<50x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    %75 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<<dram>, <<12x4>>, <interleaved>>, shape = #ttnn.shape<1x32x12x100>}> : (!tt.device<<workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1)[s0, s1] -> (0, d0 floordiv s0, d1 floordiv s1, (d0 mod s0) * s1 + d1 mod s1), dramMap = (d0, d1)[s0, s1] -> (0, 0, ((((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 8192) mod 12, (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 98304 + (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) mod 8192), meshShape = , chipIds = [0]>>) -> tensor<1x32x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %76 = "ttnn.concat"(%68, %74, %75) <{dim = 3 : si32}> : (tensor<1x32x12x50xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x2x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x32x12x50xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x2x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x32x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x32x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    "ttnn.deallocate"(%74) <{force = false}> : (tensor<1x32x12x50xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x2x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    "ttnn.deallocate"(%67) <{force = false}> : (tensor<1x32x12x50xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x2x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    %77 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<<dram>, <<12x4>>, <interleaved>>, shape = #ttnn.shape<1x32x12x100>}> : (!tt.device<<workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1)[s0, s1] -> (0, d0 floordiv s0, d1 floordiv s1, (d0 mod s0) * s1 + d1 mod s1), dramMap = (d0, d1)[s0, s1] -> (0, 0, ((((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 8192) mod 12, (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 98304 + (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) mod 8192), meshShape = , chipIds = [0]>>) -> tensor<1x32x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %78 = "ttnn.multiply"(%76, %45, %77) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 12 + d1 * 12 + d2, d3), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x32x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x32x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    "ttnn.deallocate"(%75) <{force = false}> : (tensor<1x32x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    "ttnn.deallocate"(%45) <{force = false}> : (tensor<1x1x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 12 + d1 * 12 + d2, d3), <1x1>, memref<1x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    %79 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<<dram>, <<12x4>>, <interleaved>>, shape = #ttnn.shape<1x32x12x100>}> : (!tt.device<<workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1)[s0, s1] -> (0, d0 floordiv s0, d1 floordiv s1, (d0 mod s0) * s1 + d1 mod s1), dramMap = (d0, d1)[s0, s1] -> (0, 0, ((((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 8192) mod 12, (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 98304 + (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) mod 8192), meshShape = , chipIds = [0]>>) -> tensor<1x32x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %80 = "ttnn.add"(%58, %78, %79) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x32x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x32x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x32x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    "ttnn.deallocate"(%77) <{force = false}> : (tensor<1x32x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    "ttnn.deallocate"(%57) <{force = false}> : (tensor<1x32x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    %81 = "ttnn.reshape"(%80) <{shape = [32 : i32, 12 : i32, 100 : i32]}> : (tensor<1x32x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<32x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 12 + d1, d2), <1x1>, memref<12x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    "ttnn.deallocate"(%79) <{force = false}> : (tensor<1x32x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    %82 = "ttnn.transpose"(%81) <{dim0 = 1 : si32, dim1 = 2 : si32}> : (tensor<32x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 12 + d1, d2), <1x1>, memref<12x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<32x100x12xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 100 + d1, d2), <1x1>, memref<100x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    "ttnn.deallocate"(%81) <{force = false}> : (tensor<32x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 12 + d1, d2), <1x1>, memref<12x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    %83 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<<dram>, <<12x1>>, <interleaved>>, shape = #ttnn.shape<32x12x12>}> : (!tt.device<<workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1)[s0, s1] -> (0, d0 floordiv s0, d1 floordiv s1, (d0 mod s0) * s1 + d1 mod s1), dramMap = (d0, d1)[s0, s1] -> (0, 0, ((((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 8192) mod 12, (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 98304 + (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) mod 8192), meshShape = , chipIds = [0]>>) -> tensor<32x12x12xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 12 + d1, d2), <1x1>, memref<12x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %84 = "ttnn.matmul"(%50, %82, %83) : (tensor<32x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 12 + d1, d2), <1x1>, memref<12x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<32x100x12xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 100 + d1, d2), <1x1>, memref<100x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<32x12x12xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 12 + d1, d2), <1x1>, memref<12x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<32x12x12xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 12 + d1, d2), <1x1>, memref<12x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    "ttnn.deallocate"(%82) <{force = false}> : (tensor<32x100x12xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 100 + d1, d2), <1x1>, memref<100x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    "ttnn.deallocate"(%50) <{force = false}> : (tensor<32x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 12 + d1, d2), <1x1>, memref<12x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    %85 = "ttnn.reshape"(%84) <{shape = [1 : i32, 32 : i32, 12 : i32, 12 : i32]}> : (tensor<32x12x12xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 12 + d1, d2), <1x1>, memref<12x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x32x12x12xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    "ttnn.deallocate"(%83) <{force = false}> : (tensor<32x12x12xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 12 + d1, d2), <1x1>, memref<12x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    %86 = "ttnn.to_layout"(%arg10) <{layout = #ttnn.layout<tile>}> : (tensor<1xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1xf32, #ttnn.buffer_type<system_memory>>>>) -> tensor<1xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %87 = "ttnn.to_device"(%86, %0) <{memory_config = #ttnn.memory_config<<dram>, <<1x1>>, <interleaved>>}> : (tensor<1xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !tt.device<<workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1)[s0, s1] -> (0, d0 floordiv s0, d1 floordiv s1, (d0 mod s0) * s1 + d1 mod s1), dramMap = (d0, d1)[s0, s1] -> (0, 0, ((((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 8192) mod 12, (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 98304 + (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) mod 8192), meshShape = , chipIds = [0]>>) -> tensor<1xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    "ttnn.deallocate"(%86) <{force = false}> : (tensor<1xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    %88 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<<dram>, <<12x1>>, <interleaved>>, shape = #ttnn.shape<1x32x12x12>}> : (!tt.device<<workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1)[s0, s1] -> (0, d0 floordiv s0, d1 floordiv s1, (d0 mod s0) * s1 + d1 mod s1), dramMap = (d0, d1)[s0, s1] -> (0, 0, ((((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 8192) mod 12, (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 98304 + (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) mod 8192), meshShape = , chipIds = [0]>>) -> tensor<1x32x12x12xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %89 = "ttnn.multiply"(%85, %87, %88) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x12x12xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x32x12x12xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x32x12x12xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    "ttnn.deallocate"(%87) <{force = false}> : (tensor<1xf32, #ttnn.ttnn_layout<(d0) -> (0, d0), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    "ttnn.deallocate"(%85) <{force = false}> : (tensor<1x32x12x12xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    %90 = "ttnn.to_layout"(%arg1) <{layout = #ttnn.layout<tile>}> : (tensor<1x1x12x12xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 12 + d1 * 12 + d2, d3), <1x1>, memref<12x12xf32, #ttnn.buffer_type<system_memory>>>>) -> tensor<1x1x12x12xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 12 + d1 * 12 + d2, d3), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %91 = "ttnn.to_device"(%90, %0) <{memory_config = #ttnn.memory_config<<dram>, <<1x1>>, <interleaved>>}> : (tensor<1x1x12x12xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 12 + d1 * 12 + d2, d3), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !tt.device<<workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1)[s0, s1] -> (0, d0 floordiv s0, d1 floordiv s1, (d0 mod s0) * s1 + d1 mod s1), dramMap = (d0, d1)[s0, s1] -> (0, 0, ((((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 8192) mod 12, (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 98304 + (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) mod 8192), meshShape = , chipIds = [0]>>) -> tensor<1x1x12x12xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 12 + d1 * 12 + d2, d3), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    "ttnn.deallocate"(%90) <{force = false}> : (tensor<1x1x12x12xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 12 + d1 * 12 + d2, d3), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    %92 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<<dram>, <<12x1>>, <interleaved>>, shape = #ttnn.shape<1x32x12x12>}> : (!tt.device<<workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1)[s0, s1] -> (0, d0 floordiv s0, d1 floordiv s1, (d0 mod s0) * s1 + d1 mod s1), dramMap = (d0, d1)[s0, s1] -> (0, 0, ((((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 8192) mod 12, (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 98304 + (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) mod 8192), meshShape = , chipIds = [0]>>) -> tensor<1x32x12x12xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %93 = "ttnn.add"(%89, %91, %92) <{operandSegmentSizes = array<i32: 2, 1>}> : (tensor<1x32x12x12xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x1x12x12xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 12 + d1 * 12 + d2, d3), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<1x32x12x12xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x32x12x12xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    "ttnn.deallocate"(%91) <{force = false}> : (tensor<1x1x12x12xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 12 + d1 * 12 + d2, d3), <1x1>, memref<1x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    "ttnn.deallocate"(%88) <{force = false}> : (tensor<1x32x12x12xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    %94 = "ttnn.softmax"(%93) <{dimension = -1 : si32}> : (tensor<1x32x12x12xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x32x12x12xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    "ttnn.deallocate"(%92) <{force = false}> : (tensor<1x32x12x12xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    %95 = "ttnn.reshape"(%94) <{shape = [32 : i32, 12 : i32, 12 : i32]}> : (tensor<1x32x12x12xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<32x12x12xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 12 + d1, d2), <1x1>, memref<12x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    "ttnn.deallocate"(%94) <{force = false}> : (tensor<1x32x12x12xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    %96 = "ttnn.to_layout"(%arg13) <{layout = #ttnn.layout<tile>}> : (tensor<3200x3200xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<3200x3200xf32, #ttnn.buffer_type<system_memory>>>>) -> tensor<3200x3200xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<100x100x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %97 = "ttnn.to_device"(%96, %0) <{memory_config = #ttnn.memory_config<<dram>, <<100x100>>, <interleaved>>}> : (tensor<3200x3200xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<100x100x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !tt.device<<workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1)[s0, s1] -> (0, d0 floordiv s0, d1 floordiv s1, (d0 mod s0) * s1 + d1 mod s1), dramMap = (d0, d1)[s0, s1] -> (0, 0, ((((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 8192) mod 12, (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 98304 + (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) mod 8192), meshShape = , chipIds = [0]>>) -> tensor<3200x3200xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<100x100x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    "ttnn.deallocate"(%96) <{force = false}> : (tensor<3200x3200xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<100x100x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    %98 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<<dram>, <<1x100>>, <interleaved>>, shape = #ttnn.shape<12x3200>}> : (!tt.device<<workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1)[s0, s1] -> (0, d0 floordiv s0, d1 floordiv s1, (d0 mod s0) * s1 + d1 mod s1), dramMap = (d0, d1)[s0, s1] -> (0, 0, ((((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 8192) mod 12, (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 98304 + (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) mod 8192), meshShape = , chipIds = [0]>>) -> tensor<12x3200xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x100x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %99 = "ttnn.matmul"(%3, %97, %98) : (tensor<12x3200xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x100x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<3200x3200xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<100x100x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<12x3200xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x100x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x3200xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x100x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    "ttnn.deallocate"(%97) <{force = false}> : (tensor<3200x3200xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<100x100x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    "ttnn.deallocate"(%3) <{force = false}> : (tensor<12x3200xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x100x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    %100 = "ttnn.reshape"(%99) <{shape = [1 : i32, 12 : i32, 32 : i32, 100 : i32]}> : (tensor<12x3200xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x100x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x32x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    "ttnn.deallocate"(%98) <{force = false}> : (tensor<12x3200xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x100x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    %101 = "ttnn.transpose"(%100) <{dim0 = 1 : si32, dim1 = 2 : si32}> : (tensor<1x12x32x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x32x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    "ttnn.deallocate"(%100) <{force = false}> : (tensor<1x12x32x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    %102 = "ttnn.transpose"(%101) <{dim0 = 2 : si32, dim1 = 3 : si32}> : (tensor<1x32x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x32x100x12xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3200 + d1 * 100 + d2, d3), <1x1>, memref<100x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    "ttnn.deallocate"(%101) <{force = false}> : (tensor<1x32x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    %103 = "ttnn.reshape"(%102) <{shape = [32 : i32, 100 : i32, 12 : i32]}> : (tensor<1x32x100x12xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3200 + d1 * 100 + d2, d3), <1x1>, memref<100x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<32x100x12xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 100 + d1, d2), <1x1>, memref<100x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    "ttnn.deallocate"(%102) <{force = false}> : (tensor<1x32x100x12xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 3200 + d1 * 100 + d2, d3), <1x1>, memref<100x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    %104 = "ttnn.transpose"(%103) <{dim0 = 1 : si32, dim1 = 2 : si32}> : (tensor<32x100x12xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 100 + d1, d2), <1x1>, memref<100x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<32x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 12 + d1, d2), <1x1>, memref<12x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    "ttnn.deallocate"(%103) <{force = false}> : (tensor<32x100x12xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 100 + d1, d2), <1x1>, memref<100x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    %105 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<<dram>, <<12x4>>, <interleaved>>, shape = #ttnn.shape<32x12x100>}> : (!tt.device<<workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1)[s0, s1] -> (0, d0 floordiv s0, d1 floordiv s1, (d0 mod s0) * s1 + d1 mod s1), dramMap = (d0, d1)[s0, s1] -> (0, 0, ((((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 8192) mod 12, (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 98304 + (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) mod 8192), meshShape = , chipIds = [0]>>) -> tensor<32x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 12 + d1, d2), <1x1>, memref<12x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %106 = "ttnn.matmul"(%95, %104, %105) : (tensor<32x12x12xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 12 + d1, d2), <1x1>, memref<12x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<32x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 12 + d1, d2), <1x1>, memref<12x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<32x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 12 + d1, d2), <1x1>, memref<12x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<32x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 12 + d1, d2), <1x1>, memref<12x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    "ttnn.deallocate"(%104) <{force = false}> : (tensor<32x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 12 + d1, d2), <1x1>, memref<12x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    "ttnn.deallocate"(%95) <{force = false}> : (tensor<32x12x12xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 12 + d1, d2), <1x1>, memref<12x1x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    %107 = "ttnn.reshape"(%106) <{shape = [1 : i32, 32 : i32, 12 : i32, 100 : i32]}> : (tensor<32x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 12 + d1, d2), <1x1>, memref<12x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x32x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    "ttnn.deallocate"(%105) <{force = false}> : (tensor<32x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 12 + d1, d2), <1x1>, memref<12x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    %108 = "ttnn.transpose"(%107) <{dim0 = 1 : si32, dim1 = 2 : si32}> : (tensor<1x32x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x32x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    "ttnn.deallocate"(%107) <{force = false}> : (tensor<1x32x12x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 12 + d2, d3), <1x1>, memref<12x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    %109 = "ttnn.reshape"(%108) <{shape = [12 : i32, 3200 : i32]}> : (tensor<1x12x32x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x3200xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x100x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    "ttnn.deallocate"(%108) <{force = false}> : (tensor<1x12x32x100xf32, #ttnn.ttnn_layout<(d0, d1, d2, d3) -> (d0 * 384 + d1 * 32 + d2, d3), <1x1>, memref<12x4x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    %110 = "ttnn.to_layout"(%arg14) <{layout = #ttnn.layout<tile>}> : (tensor<3200x3200xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<3200x3200xf32, #ttnn.buffer_type<system_memory>>>>) -> tensor<3200x3200xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<100x100x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %111 = "ttnn.to_device"(%110, %0) <{memory_config = #ttnn.memory_config<<dram>, <<100x100>>, <interleaved>>}> : (tensor<3200x3200xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<100x100x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, !tt.device<<workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1)[s0, s1] -> (0, d0 floordiv s0, d1 floordiv s1, (d0 mod s0) * s1 + d1 mod s1), dramMap = (d0, d1)[s0, s1] -> (0, 0, ((((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 8192) mod 12, (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 98304 + (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) mod 8192), meshShape = , chipIds = [0]>>) -> tensor<3200x3200xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<100x100x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    "ttnn.deallocate"(%110) <{force = false}> : (tensor<3200x3200xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<100x100x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    %112 = "ttnn.empty"(%0) <{dtype = #tt.supportedDataTypes<f32>, layout = #ttnn.layout<tile>, memory_config = #ttnn.memory_config<<dram>, <<1x100>>, <interleaved>>, shape = #ttnn.shape<12x3200>}> : (!tt.device<<workerGrid = #tt.grid<8x8, (d0, d1) -> (0, d0, d1)>, l1Map = (d0, d1)[s0, s1] -> (0, d0 floordiv s0, d1 floordiv s1, (d0 mod s0) * s1 + d1 mod s1), dramMap = (d0, d1)[s0, s1] -> (0, 0, ((((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 8192) mod 12, (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) floordiv 98304 + (((d0 floordiv s0) * 8 + d1 floordiv s1) * (s1 * s0) + (d0 mod s0) * s1 + d1 mod s1) mod 8192), meshShape = , chipIds = [0]>>) -> tensor<12x3200xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x100x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    %113 = "ttnn.matmul"(%109, %111, %112) : (tensor<12x3200xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x100x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<3200x3200xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<100x100x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>, tensor<12x3200xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x100x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<12x3200xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x100x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    "ttnn.deallocate"(%111) <{force = false}> : (tensor<3200x3200xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<100x100x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    "ttnn.deallocate"(%109) <{force = false}> : (tensor<12x3200xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x100x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    %114 = "ttnn.reshape"(%113) <{shape = [1 : i32, 12 : i32, 3200 : i32]}> : (tensor<12x3200xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x100x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x3200xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 12 + d1, d2), <1x1>, memref<1x100x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>
    "ttnn.deallocate"(%112) <{force = false}> : (tensor<12x3200xf32, #ttnn.ttnn_layout<(d0, d1) -> (d0, d1), <1x1>, memref<1x100x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    %115 = "ttnn.from_device"(%114) : (tensor<1x12x3200xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 12 + d1, d2), <1x1>, memref<1x100x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> tensor<1x12x3200xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 12 + d1, d2), <1x1>, memref<12x3200xf32, #ttnn.buffer_type<system_memory>>>>
    "ttnn.deallocate"(%114) <{force = false}> : (tensor<1x12x3200xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 12 + d1, d2), <1x1>, memref<1x100x!tt.tile<32x32, f32>, #ttnn.buffer_type<dram>>, <interleaved>>>) -> ()
    %116 = "ttnn.to_layout"(%115) <{layout = #ttnn.layout<row_major>}> : (tensor<1x12x3200xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 12 + d1, d2), <1x1>, memref<12x3200xf32, #ttnn.buffer_type<system_memory>>>>) -> tensor<1x12x3200xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 12 + d1, d2), <1x1>, memref<12x3200xf32, #ttnn.buffer_type<system_memory>>>>
    "ttnn.deallocate"(%115) <{force = false}> : (tensor<1x12x3200xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 12 + d1, d2), <1x1>, memref<12x3200xf32, #ttnn.buffer_type<system_memory>>>>) -> ()
    return %116 : tensor<1x12x3200xf32, #ttnn.ttnn_layout<(d0, d1, d2) -> (d0 * 12 + d1, d2), <1x1>, memref<12x3200xf32, #ttnn.buffer_type<system_memory>>>>
  }
}