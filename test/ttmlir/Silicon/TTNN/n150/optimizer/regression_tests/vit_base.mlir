// REQUIRES: opmodel, regression
// RUN: ttmlir-opt --ttir-to-ttnn-backend-pipeline="system-desc-path=%system_desc_path% enable-optimizer=true memory-layout-analysis-enabled=false enable-fusing-pass=true" -o vit_base_ttnn.mlir %s
// RUN: ttmlir-translate --ttnn-to-flatbuffer vit_base_ttnn.mlir > %t.ttnn
// UNSUPPORTED: true
//
// unsupported with optimizer because it fails on OOM
// 2025-07-16 11:25:47.412 | critical |          Always | Out of Memory: Not enough space to allocate 43753472 B L1 buffer across 49 banks, where each bank needs to store 892928 B (assert.hpp:107)
#loc = loc("ViTBase":0:0)
module @ViTBase {
  func.func @forward(%arg0: tensor<8x3x224x224xbf16> {ttcore.argument_type = #ttcore.argument_type<input>, ttir.name = "pixel_values"} loc("ViTBase":0:0), %arg1: tensor<8x197x1024xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_11.1"} loc("ViTBase":0:0), %arg2: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_11.6"} loc("ViTBase":0:0), %arg3: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_11.8"} loc("ViTBase":0:0), %arg4: tensor<1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "input_1_multiply_30"} loc("ViTBase":0:0), %arg5: tensor<197x197xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "input_1_add_31"} loc("ViTBase":0:0), %arg6: tensor<8x197x1024xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_52.1"} loc("ViTBase":0:0), %arg7: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_52.6"} loc("ViTBase":0:0), %arg8: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_52.8"} loc("ViTBase":0:0), %arg9: tensor<8x197x1024xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_66.1"} loc("ViTBase":0:0), %arg10: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_66.6"} loc("ViTBase":0:0), %arg11: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_66.8"} loc("ViTBase":0:0), %arg12: tensor<1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "input_1_multiply_85"} loc("ViTBase":0:0), %arg13: tensor<197x197xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "input_1_add_86"} loc("ViTBase":0:0), %arg14: tensor<8x197x1024xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_107.1"} loc("ViTBase":0:0), %arg15: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_107.6"} loc("ViTBase":0:0), %arg16: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_107.8"} loc("ViTBase":0:0), %arg17: tensor<8x197x1024xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_121.1"} loc("ViTBase":0:0), %arg18: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_121.6"} loc("ViTBase":0:0), %arg19: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_121.8"} loc("ViTBase":0:0), %arg20: tensor<1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "input_1_multiply_140"} loc("ViTBase":0:0), %arg21: tensor<197x197xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "input_1_add_141"} loc("ViTBase":0:0), %arg22: tensor<8x197x1024xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_162.1"} loc("ViTBase":0:0), %arg23: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_162.6"} loc("ViTBase":0:0), %arg24: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_162.8"} loc("ViTBase":0:0), %arg25: tensor<8x197x1024xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_176.1"} loc("ViTBase":0:0), %arg26: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_176.6"} loc("ViTBase":0:0), %arg27: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_176.8"} loc("ViTBase":0:0), %arg28: tensor<1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "input_1_multiply_195"} loc("ViTBase":0:0), %arg29: tensor<197x197xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "input_1_add_196"} loc("ViTBase":0:0), %arg30: tensor<8x197x1024xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_217.1"} loc("ViTBase":0:0), %arg31: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_217.6"} loc("ViTBase":0:0), %arg32: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_217.8"} loc("ViTBase":0:0), %arg33: tensor<8x197x1024xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_231.1"} loc("ViTBase":0:0), %arg34: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_231.6"} loc("ViTBase":0:0), %arg35: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_231.8"} loc("ViTBase":0:0), %arg36: tensor<1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "input_1_multiply_250"} loc("ViTBase":0:0), %arg37: tensor<197x197xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "input_1_add_251"} loc("ViTBase":0:0), %arg38: tensor<8x197x1024xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_272.1"} loc("ViTBase":0:0), %arg39: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_272.6"} loc("ViTBase":0:0), %arg40: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_272.8"} loc("ViTBase":0:0), %arg41: tensor<8x197x1024xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_286.1"} loc("ViTBase":0:0), %arg42: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_286.6"} loc("ViTBase":0:0), %arg43: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_286.8"} loc("ViTBase":0:0), %arg44: tensor<1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "input_1_multiply_305"} loc("ViTBase":0:0), %arg45: tensor<197x197xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "input_1_add_306"} loc("ViTBase":0:0), %arg46: tensor<8x197x1024xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_327.1"} loc("ViTBase":0:0), %arg47: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_327.6"} loc("ViTBase":0:0), %arg48: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_327.8"} loc("ViTBase":0:0), %arg49: tensor<8x197x1024xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_341.1"} loc("ViTBase":0:0), %arg50: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_341.6"} loc("ViTBase":0:0), %arg51: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_341.8"} loc("ViTBase":0:0), %arg52: tensor<1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "input_1_multiply_360"} loc("ViTBase":0:0), %arg53: tensor<197x197xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "input_1_add_361"} loc("ViTBase":0:0), %arg54: tensor<8x197x1024xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_382.1"} loc("ViTBase":0:0), %arg55: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_382.6"} loc("ViTBase":0:0), %arg56: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_382.8"} loc("ViTBase":0:0), %arg57: tensor<8x197x1024xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_396.1"} loc("ViTBase":0:0), %arg58: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_396.6"} loc("ViTBase":0:0), %arg59: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_396.8"} loc("ViTBase":0:0), %arg60: tensor<1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "input_1_multiply_415"} loc("ViTBase":0:0), %arg61: tensor<197x197xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "input_1_add_416"} loc("ViTBase":0:0), %arg62: tensor<8x197x1024xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_437.1"} loc("ViTBase":0:0), %arg63: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_437.6"} loc("ViTBase":0:0), %arg64: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_437.8"} loc("ViTBase":0:0), %arg65: tensor<8x197x1024xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_451.1"} loc("ViTBase":0:0), %arg66: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_451.6"} loc("ViTBase":0:0), %arg67: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_451.8"} loc("ViTBase":0:0), %arg68: tensor<1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "input_1_multiply_470"} loc("ViTBase":0:0), %arg69: tensor<197x197xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "input_1_add_471"} loc("ViTBase":0:0), %arg70: tensor<8x197x1024xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_492.1"} loc("ViTBase":0:0), %arg71: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_492.6"} loc("ViTBase":0:0), %arg72: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_492.8"} loc("ViTBase":0:0), %arg73: tensor<8x197x1024xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_506.1"} loc("ViTBase":0:0), %arg74: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_506.6"} loc("ViTBase":0:0), %arg75: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_506.8"} loc("ViTBase":0:0), %arg76: tensor<1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "input_1_multiply_525"} loc("ViTBase":0:0), %arg77: tensor<197x197xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "input_1_add_526"} loc("ViTBase":0:0), %arg78: tensor<8x197x1024xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_547.1"} loc("ViTBase":0:0), %arg79: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_547.6"} loc("ViTBase":0:0), %arg80: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_547.8"} loc("ViTBase":0:0), %arg81: tensor<8x197x1024xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_561.1"} loc("ViTBase":0:0), %arg82: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_561.6"} loc("ViTBase":0:0), %arg83: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_561.8"} loc("ViTBase":0:0), %arg84: tensor<1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "input_1_multiply_580"} loc("ViTBase":0:0), %arg85: tensor<197x197xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "input_1_add_581"} loc("ViTBase":0:0), %arg86: tensor<8x197x1024xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_602.1"} loc("ViTBase":0:0), %arg87: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_602.6"} loc("ViTBase":0:0), %arg88: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_602.8"} loc("ViTBase":0:0), %arg89: tensor<8x197x1024xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_616.1"} loc("ViTBase":0:0), %arg90: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_616.6"} loc("ViTBase":0:0), %arg91: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_616.8"} loc("ViTBase":0:0), %arg92: tensor<1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "input_1_multiply_635"} loc("ViTBase":0:0), %arg93: tensor<197x197xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "input_1_add_636"} loc("ViTBase":0:0), %arg94: tensor<8x197x1024xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_657.1"} loc("ViTBase":0:0), %arg95: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_657.6"} loc("ViTBase":0:0), %arg96: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_657.8"} loc("ViTBase":0:0), %arg97: tensor<8x197x1024xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_671.1"} loc("ViTBase":0:0), %arg98: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_671.6"} loc("ViTBase":0:0), %arg99: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_671.8"} loc("ViTBase":0:0), %arg100: tensor<1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "input_1_multiply_690"} loc("ViTBase":0:0), %arg101: tensor<197x197xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "input_1_add_691"} loc("ViTBase":0:0), %arg102: tensor<8x197x1024xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_712.1"} loc("ViTBase":0:0), %arg103: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_712.6"} loc("ViTBase":0:0), %arg104: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_712.8"} loc("ViTBase":0:0), %arg105: tensor<8x197x1024xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_726.1"} loc("ViTBase":0:0), %arg106: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_726.6"} loc("ViTBase":0:0), %arg107: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_726.8"} loc("ViTBase":0:0), %arg108: tensor<1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "input_1_multiply_745"} loc("ViTBase":0:0), %arg109: tensor<197x197xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "input_1_add_746"} loc("ViTBase":0:0), %arg110: tensor<8x197x1024xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_767.1"} loc("ViTBase":0:0), %arg111: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_767.6"} loc("ViTBase":0:0), %arg112: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_767.8"} loc("ViTBase":0:0), %arg113: tensor<8x197x1024xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_781.1"} loc("ViTBase":0:0), %arg114: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_781.6"} loc("ViTBase":0:0), %arg115: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_781.8"} loc("ViTBase":0:0), %arg116: tensor<1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "input_1_multiply_800"} loc("ViTBase":0:0), %arg117: tensor<197x197xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "input_1_add_801"} loc("ViTBase":0:0), %arg118: tensor<8x197x1024xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_822.1"} loc("ViTBase":0:0), %arg119: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_822.6"} loc("ViTBase":0:0), %arg120: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_822.8"} loc("ViTBase":0:0), %arg121: tensor<8x197x1024xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_836.1"} loc("ViTBase":0:0), %arg122: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_836.6"} loc("ViTBase":0:0), %arg123: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_836.8"} loc("ViTBase":0:0), %arg124: tensor<1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "input_1_multiply_855"} loc("ViTBase":0:0), %arg125: tensor<197x197xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "input_1_add_856"} loc("ViTBase":0:0), %arg126: tensor<8x197x1024xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_877.1"} loc("ViTBase":0:0), %arg127: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_877.6"} loc("ViTBase":0:0), %arg128: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_877.8"} loc("ViTBase":0:0), %arg129: tensor<8x197x1024xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_891.1"} loc("ViTBase":0:0), %arg130: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_891.6"} loc("ViTBase":0:0), %arg131: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_891.8"} loc("ViTBase":0:0), %arg132: tensor<1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "input_1_multiply_910"} loc("ViTBase":0:0), %arg133: tensor<197x197xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "input_1_add_911"} loc("ViTBase":0:0), %arg134: tensor<8x197x1024xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_932.1"} loc("ViTBase":0:0), %arg135: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_932.6"} loc("ViTBase":0:0), %arg136: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_932.8"} loc("ViTBase":0:0), %arg137: tensor<8x197x1024xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_946.1"} loc("ViTBase":0:0), %arg138: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_946.6"} loc("ViTBase":0:0), %arg139: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_946.8"} loc("ViTBase":0:0), %arg140: tensor<1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "input_1_multiply_965"} loc("ViTBase":0:0), %arg141: tensor<197x197xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "input_1_add_966"} loc("ViTBase":0:0), %arg142: tensor<8x197x1024xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_987.1"} loc("ViTBase":0:0), %arg143: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_987.6"} loc("ViTBase":0:0), %arg144: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_987.8"} loc("ViTBase":0:0), %arg145: tensor<8x197x1024xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_1001.1"} loc("ViTBase":0:0), %arg146: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_1001.6"} loc("ViTBase":0:0), %arg147: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_1001.8"} loc("ViTBase":0:0), %arg148: tensor<1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "input_1_multiply_1020"} loc("ViTBase":0:0), %arg149: tensor<197x197xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "input_1_add_1021"} loc("ViTBase":0:0), %arg150: tensor<8x197x1024xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_1042.1"} loc("ViTBase":0:0), %arg151: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_1042.6"} loc("ViTBase":0:0), %arg152: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_1042.8"} loc("ViTBase":0:0), %arg153: tensor<8x197x1024xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_1056.1"} loc("ViTBase":0:0), %arg154: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_1056.6"} loc("ViTBase":0:0), %arg155: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_1056.8"} loc("ViTBase":0:0), %arg156: tensor<1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "input_1_multiply_1075"} loc("ViTBase":0:0), %arg157: tensor<197x197xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "input_1_add_1076"} loc("ViTBase":0:0), %arg158: tensor<8x197x1024xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_1097.1"} loc("ViTBase":0:0), %arg159: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_1097.6"} loc("ViTBase":0:0), %arg160: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_1097.8"} loc("ViTBase":0:0), %arg161: tensor<8x197x1024xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_1111.1"} loc("ViTBase":0:0), %arg162: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_1111.6"} loc("ViTBase":0:0), %arg163: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_1111.8"} loc("ViTBase":0:0), %arg164: tensor<1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "input_1_multiply_1130"} loc("ViTBase":0:0), %arg165: tensor<197x197xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "input_1_add_1131"} loc("ViTBase":0:0), %arg166: tensor<8x197x1024xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_1152.1"} loc("ViTBase":0:0), %arg167: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_1152.6"} loc("ViTBase":0:0), %arg168: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_1152.8"} loc("ViTBase":0:0), %arg169: tensor<8x197x1024xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_1166.1"} loc("ViTBase":0:0), %arg170: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_1166.6"} loc("ViTBase":0:0), %arg171: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_1166.8"} loc("ViTBase":0:0), %arg172: tensor<1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "input_1_multiply_1185"} loc("ViTBase":0:0), %arg173: tensor<197x197xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "input_1_add_1186"} loc("ViTBase":0:0), %arg174: tensor<8x197x1024xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_1207.1"} loc("ViTBase":0:0), %arg175: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_1207.6"} loc("ViTBase":0:0), %arg176: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_1207.8"} loc("ViTBase":0:0), %arg177: tensor<8x197x1024xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_1221.1"} loc("ViTBase":0:0), %arg178: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_1221.6"} loc("ViTBase":0:0), %arg179: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_1221.8"} loc("ViTBase":0:0), %arg180: tensor<1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "input_1_multiply_1240"} loc("ViTBase":0:0), %arg181: tensor<197x197xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "input_1_add_1241"} loc("ViTBase":0:0), %arg182: tensor<8x197x1024xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_1262.1"} loc("ViTBase":0:0), %arg183: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_1262.6"} loc("ViTBase":0:0), %arg184: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_1262.8"} loc("ViTBase":0:0), %arg185: tensor<8x197x1024xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_1276.1"} loc("ViTBase":0:0), %arg186: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_1276.6"} loc("ViTBase":0:0), %arg187: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_1276.8"} loc("ViTBase":0:0), %arg188: tensor<1xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "input_1_multiply_1295"} loc("ViTBase":0:0), %arg189: tensor<197x197xbf16> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "input_1_add_1296"} loc("ViTBase":0:0), %arg190: tensor<8x197x1024xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_1317.1"} loc("ViTBase":0:0), %arg191: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_1317.6"} loc("ViTBase":0:0), %arg192: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_1317.8"} loc("ViTBase":0:0), %arg193: tensor<8x197x1024xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_1331.1"} loc("ViTBase":0:0), %arg194: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_1331.6"} loc("ViTBase":0:0), %arg195: tensor<8x197x1xf32> {ttcore.argument_type = #ttcore.argument_type<constant>, ttir.name = "dc.input_tensor.layernorm_1331.8"} loc("ViTBase":0:0), %arg196: tensor<8x1x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.embeddings.cls_token"} loc("ViTBase":0:0), %arg197: tensor<1024x3x16x16xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.embeddings.patch_embeddings.projection.weight"} loc("ViTBase":0:0), %arg198: tensor<1x1x1x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.embeddings.patch_embeddings.projection.bias"} loc("ViTBase":0:0), %arg199: tensor<1x197x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.embeddings.position_embeddings"} loc("ViTBase":0:0), %arg200: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.0.layernorm_before.weight"} loc("ViTBase":0:0), %arg201: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.0.layernorm_before.bias"} loc("ViTBase":0:0), %arg202: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.0.attention.attention.query.weight"} loc("ViTBase":0:0), %arg203: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.0.attention.attention.query.bias"} loc("ViTBase":0:0), %arg204: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.0.attention.attention.key.weight"} loc("ViTBase":0:0), %arg205: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.0.attention.attention.key.bias"} loc("ViTBase":0:0), %arg206: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.0.attention.attention.value.weight"} loc("ViTBase":0:0), %arg207: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.0.attention.attention.value.bias"} loc("ViTBase":0:0), %arg208: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.0.attention.output.dense.weight"} loc("ViTBase":0:0), %arg209: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.0.attention.output.dense.bias"} loc("ViTBase":0:0), %arg210: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.0.layernorm_after.weight"} loc("ViTBase":0:0), %arg211: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.0.layernorm_after.bias"} loc("ViTBase":0:0), %arg212: tensor<1024x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.0.intermediate.dense.weight"} loc("ViTBase":0:0), %arg213: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.0.intermediate.dense.bias"} loc("ViTBase":0:0), %arg214: tensor<4096x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.0.output.dense.weight"} loc("ViTBase":0:0), %arg215: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.0.output.dense.bias"} loc("ViTBase":0:0), %arg216: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.1.layernorm_before.weight"} loc("ViTBase":0:0), %arg217: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.1.layernorm_before.bias"} loc("ViTBase":0:0), %arg218: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.1.attention.attention.query.weight"} loc("ViTBase":0:0), %arg219: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.1.attention.attention.query.bias"} loc("ViTBase":0:0), %arg220: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.1.attention.attention.key.weight"} loc("ViTBase":0:0), %arg221: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.1.attention.attention.key.bias"} loc("ViTBase":0:0), %arg222: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.1.attention.attention.value.weight"} loc("ViTBase":0:0), %arg223: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.1.attention.attention.value.bias"} loc("ViTBase":0:0), %arg224: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.1.attention.output.dense.weight"} loc("ViTBase":0:0), %arg225: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.1.attention.output.dense.bias"} loc("ViTBase":0:0), %arg226: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.1.layernorm_after.weight"} loc("ViTBase":0:0), %arg227: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.1.layernorm_after.bias"} loc("ViTBase":0:0), %arg228: tensor<1024x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.1.intermediate.dense.weight"} loc("ViTBase":0:0), %arg229: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.1.intermediate.dense.bias"} loc("ViTBase":0:0), %arg230: tensor<4096x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.1.output.dense.weight"} loc("ViTBase":0:0), %arg231: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.1.output.dense.bias"} loc("ViTBase":0:0), %arg232: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.2.layernorm_before.weight"} loc("ViTBase":0:0), %arg233: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.2.layernorm_before.bias"} loc("ViTBase":0:0), %arg234: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.2.attention.attention.query.weight"} loc("ViTBase":0:0), %arg235: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.2.attention.attention.query.bias"} loc("ViTBase":0:0), %arg236: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.2.attention.attention.key.weight"} loc("ViTBase":0:0), %arg237: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.2.attention.attention.key.bias"} loc("ViTBase":0:0), %arg238: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.2.attention.attention.value.weight"} loc("ViTBase":0:0), %arg239: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.2.attention.attention.value.bias"} loc("ViTBase":0:0), %arg240: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.2.attention.output.dense.weight"} loc("ViTBase":0:0), %arg241: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.2.attention.output.dense.bias"} loc("ViTBase":0:0), %arg242: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.2.layernorm_after.weight"} loc("ViTBase":0:0), %arg243: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.2.layernorm_after.bias"} loc("ViTBase":0:0), %arg244: tensor<1024x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.2.intermediate.dense.weight"} loc("ViTBase":0:0), %arg245: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.2.intermediate.dense.bias"} loc("ViTBase":0:0), %arg246: tensor<4096x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.2.output.dense.weight"} loc("ViTBase":0:0), %arg247: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.2.output.dense.bias"} loc("ViTBase":0:0), %arg248: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.3.layernorm_before.weight"} loc("ViTBase":0:0), %arg249: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.3.layernorm_before.bias"} loc("ViTBase":0:0), %arg250: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.3.attention.attention.query.weight"} loc("ViTBase":0:0), %arg251: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.3.attention.attention.query.bias"} loc("ViTBase":0:0), %arg252: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.3.attention.attention.key.weight"} loc("ViTBase":0:0), %arg253: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.3.attention.attention.key.bias"} loc("ViTBase":0:0), %arg254: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.3.attention.attention.value.weight"} loc("ViTBase":0:0), %arg255: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.3.attention.attention.value.bias"} loc("ViTBase":0:0), %arg256: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.3.attention.output.dense.weight"} loc("ViTBase":0:0), %arg257: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.3.attention.output.dense.bias"} loc("ViTBase":0:0), %arg258: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.3.layernorm_after.weight"} loc("ViTBase":0:0), %arg259: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.3.layernorm_after.bias"} loc("ViTBase":0:0), %arg260: tensor<1024x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.3.intermediate.dense.weight"} loc("ViTBase":0:0), %arg261: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.3.intermediate.dense.bias"} loc("ViTBase":0:0), %arg262: tensor<4096x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.3.output.dense.weight"} loc("ViTBase":0:0), %arg263: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.3.output.dense.bias"} loc("ViTBase":0:0), %arg264: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.4.layernorm_before.weight"} loc("ViTBase":0:0), %arg265: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.4.layernorm_before.bias"} loc("ViTBase":0:0), %arg266: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.4.attention.attention.query.weight"} loc("ViTBase":0:0), %arg267: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.4.attention.attention.query.bias"} loc("ViTBase":0:0), %arg268: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.4.attention.attention.key.weight"} loc("ViTBase":0:0), %arg269: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.4.attention.attention.key.bias"} loc("ViTBase":0:0), %arg270: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.4.attention.attention.value.weight"} loc("ViTBase":0:0), %arg271: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.4.attention.attention.value.bias"} loc("ViTBase":0:0), %arg272: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.4.attention.output.dense.weight"} loc("ViTBase":0:0), %arg273: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.4.attention.output.dense.bias"} loc("ViTBase":0:0), %arg274: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.4.layernorm_after.weight"} loc("ViTBase":0:0), %arg275: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.4.layernorm_after.bias"} loc("ViTBase":0:0), %arg276: tensor<1024x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.4.intermediate.dense.weight"} loc("ViTBase":0:0), %arg277: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.4.intermediate.dense.bias"} loc("ViTBase":0:0), %arg278: tensor<4096x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.4.output.dense.weight"} loc("ViTBase":0:0), %arg279: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.4.output.dense.bias"} loc("ViTBase":0:0), %arg280: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.5.layernorm_before.weight"} loc("ViTBase":0:0), %arg281: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.5.layernorm_before.bias"} loc("ViTBase":0:0), %arg282: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.5.attention.attention.query.weight"} loc("ViTBase":0:0), %arg283: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.5.attention.attention.query.bias"} loc("ViTBase":0:0), %arg284: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.5.attention.attention.key.weight"} loc("ViTBase":0:0), %arg285: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.5.attention.attention.key.bias"} loc("ViTBase":0:0), %arg286: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.5.attention.attention.value.weight"} loc("ViTBase":0:0), %arg287: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.5.attention.attention.value.bias"} loc("ViTBase":0:0), %arg288: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.5.attention.output.dense.weight"} loc("ViTBase":0:0), %arg289: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.5.attention.output.dense.bias"} loc("ViTBase":0:0), %arg290: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.5.layernorm_after.weight"} loc("ViTBase":0:0), %arg291: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.5.layernorm_after.bias"} loc("ViTBase":0:0), %arg292: tensor<1024x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.5.intermediate.dense.weight"} loc("ViTBase":0:0), %arg293: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.5.intermediate.dense.bias"} loc("ViTBase":0:0), %arg294: tensor<4096x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.5.output.dense.weight"} loc("ViTBase":0:0), %arg295: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.5.output.dense.bias"} loc("ViTBase":0:0), %arg296: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.6.layernorm_before.weight"} loc("ViTBase":0:0), %arg297: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.6.layernorm_before.bias"} loc("ViTBase":0:0), %arg298: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.6.attention.attention.query.weight"} loc("ViTBase":0:0), %arg299: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.6.attention.attention.query.bias"} loc("ViTBase":0:0), %arg300: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.6.attention.attention.key.weight"} loc("ViTBase":0:0), %arg301: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.6.attention.attention.key.bias"} loc("ViTBase":0:0), %arg302: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.6.attention.attention.value.weight"} loc("ViTBase":0:0), %arg303: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.6.attention.attention.value.bias"} loc("ViTBase":0:0), %arg304: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.6.attention.output.dense.weight"} loc("ViTBase":0:0), %arg305: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.6.attention.output.dense.bias"} loc("ViTBase":0:0), %arg306: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.6.layernorm_after.weight"} loc("ViTBase":0:0), %arg307: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.6.layernorm_after.bias"} loc("ViTBase":0:0), %arg308: tensor<1024x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.6.intermediate.dense.weight"} loc("ViTBase":0:0), %arg309: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.6.intermediate.dense.bias"} loc("ViTBase":0:0), %arg310: tensor<4096x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.6.output.dense.weight"} loc("ViTBase":0:0), %arg311: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.6.output.dense.bias"} loc("ViTBase":0:0), %arg312: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.7.layernorm_before.weight"} loc("ViTBase":0:0), %arg313: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.7.layernorm_before.bias"} loc("ViTBase":0:0), %arg314: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.7.attention.attention.query.weight"} loc("ViTBase":0:0), %arg315: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.7.attention.attention.query.bias"} loc("ViTBase":0:0), %arg316: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.7.attention.attention.key.weight"} loc("ViTBase":0:0), %arg317: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.7.attention.attention.key.bias"} loc("ViTBase":0:0), %arg318: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.7.attention.attention.value.weight"} loc("ViTBase":0:0), %arg319: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.7.attention.attention.value.bias"} loc("ViTBase":0:0), %arg320: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.7.attention.output.dense.weight"} loc("ViTBase":0:0), %arg321: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.7.attention.output.dense.bias"} loc("ViTBase":0:0), %arg322: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.7.layernorm_after.weight"} loc("ViTBase":0:0), %arg323: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.7.layernorm_after.bias"} loc("ViTBase":0:0), %arg324: tensor<1024x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.7.intermediate.dense.weight"} loc("ViTBase":0:0), %arg325: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.7.intermediate.dense.bias"} loc("ViTBase":0:0), %arg326: tensor<4096x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.7.output.dense.weight"} loc("ViTBase":0:0), %arg327: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.7.output.dense.bias"} loc("ViTBase":0:0), %arg328: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.8.layernorm_before.weight"} loc("ViTBase":0:0), %arg329: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.8.layernorm_before.bias"} loc("ViTBase":0:0), %arg330: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.8.attention.attention.query.weight"} loc("ViTBase":0:0), %arg331: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.8.attention.attention.query.bias"} loc("ViTBase":0:0), %arg332: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.8.attention.attention.key.weight"} loc("ViTBase":0:0), %arg333: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.8.attention.attention.key.bias"} loc("ViTBase":0:0), %arg334: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.8.attention.attention.value.weight"} loc("ViTBase":0:0), %arg335: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.8.attention.attention.value.bias"} loc("ViTBase":0:0), %arg336: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.8.attention.output.dense.weight"} loc("ViTBase":0:0), %arg337: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.8.attention.output.dense.bias"} loc("ViTBase":0:0), %arg338: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.8.layernorm_after.weight"} loc("ViTBase":0:0), %arg339: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.8.layernorm_after.bias"} loc("ViTBase":0:0), %arg340: tensor<1024x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.8.intermediate.dense.weight"} loc("ViTBase":0:0), %arg341: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.8.intermediate.dense.bias"} loc("ViTBase":0:0), %arg342: tensor<4096x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.8.output.dense.weight"} loc("ViTBase":0:0), %arg343: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.8.output.dense.bias"} loc("ViTBase":0:0), %arg344: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.9.layernorm_before.weight"} loc("ViTBase":0:0), %arg345: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.9.layernorm_before.bias"} loc("ViTBase":0:0), %arg346: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.9.attention.attention.query.weight"} loc("ViTBase":0:0), %arg347: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.9.attention.attention.query.bias"} loc("ViTBase":0:0), %arg348: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.9.attention.attention.key.weight"} loc("ViTBase":0:0), %arg349: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.9.attention.attention.key.bias"} loc("ViTBase":0:0), %arg350: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.9.attention.attention.value.weight"} loc("ViTBase":0:0), %arg351: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.9.attention.attention.value.bias"} loc("ViTBase":0:0), %arg352: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.9.attention.output.dense.weight"} loc("ViTBase":0:0), %arg353: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.9.attention.output.dense.bias"} loc("ViTBase":0:0), %arg354: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.9.layernorm_after.weight"} loc("ViTBase":0:0), %arg355: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.9.layernorm_after.bias"} loc("ViTBase":0:0), %arg356: tensor<1024x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.9.intermediate.dense.weight"} loc("ViTBase":0:0), %arg357: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.9.intermediate.dense.bias"} loc("ViTBase":0:0), %arg358: tensor<4096x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.9.output.dense.weight"} loc("ViTBase":0:0), %arg359: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.9.output.dense.bias"} loc("ViTBase":0:0), %arg360: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.10.layernorm_before.weight"} loc("ViTBase":0:0), %arg361: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.10.layernorm_before.bias"} loc("ViTBase":0:0), %arg362: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.10.attention.attention.query.weight"} loc("ViTBase":0:0), %arg363: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.10.attention.attention.query.bias"} loc("ViTBase":0:0), %arg364: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.10.attention.attention.key.weight"} loc("ViTBase":0:0), %arg365: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.10.attention.attention.key.bias"} loc("ViTBase":0:0), %arg366: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.10.attention.attention.value.weight"} loc("ViTBase":0:0), %arg367: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.10.attention.attention.value.bias"} loc("ViTBase":0:0), %arg368: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.10.attention.output.dense.weight"} loc("ViTBase":0:0), %arg369: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.10.attention.output.dense.bias"} loc("ViTBase":0:0), %arg370: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.10.layernorm_after.weight"} loc("ViTBase":0:0), %arg371: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.10.layernorm_after.bias"} loc("ViTBase":0:0), %arg372: tensor<1024x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.10.intermediate.dense.weight"} loc("ViTBase":0:0), %arg373: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.10.intermediate.dense.bias"} loc("ViTBase":0:0), %arg374: tensor<4096x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.10.output.dense.weight"} loc("ViTBase":0:0), %arg375: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.10.output.dense.bias"} loc("ViTBase":0:0), %arg376: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.11.layernorm_before.weight"} loc("ViTBase":0:0), %arg377: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.11.layernorm_before.bias"} loc("ViTBase":0:0), %arg378: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.11.attention.attention.query.weight"} loc("ViTBase":0:0), %arg379: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.11.attention.attention.query.bias"} loc("ViTBase":0:0), %arg380: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.11.attention.attention.key.weight"} loc("ViTBase":0:0), %arg381: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.11.attention.attention.key.bias"} loc("ViTBase":0:0), %arg382: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.11.attention.attention.value.weight"} loc("ViTBase":0:0), %arg383: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.11.attention.attention.value.bias"} loc("ViTBase":0:0), %arg384: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.11.attention.output.dense.weight"} loc("ViTBase":0:0), %arg385: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.11.attention.output.dense.bias"} loc("ViTBase":0:0), %arg386: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.11.layernorm_after.weight"} loc("ViTBase":0:0), %arg387: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.11.layernorm_after.bias"} loc("ViTBase":0:0), %arg388: tensor<1024x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.11.intermediate.dense.weight"} loc("ViTBase":0:0), %arg389: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.11.intermediate.dense.bias"} loc("ViTBase":0:0), %arg390: tensor<4096x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.11.output.dense.weight"} loc("ViTBase":0:0), %arg391: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.11.output.dense.bias"} loc("ViTBase":0:0), %arg392: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.12.layernorm_before.weight"} loc("ViTBase":0:0), %arg393: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.12.layernorm_before.bias"} loc("ViTBase":0:0), %arg394: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.12.attention.attention.query.weight"} loc("ViTBase":0:0), %arg395: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.12.attention.attention.query.bias"} loc("ViTBase":0:0), %arg396: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.12.attention.attention.key.weight"} loc("ViTBase":0:0), %arg397: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.12.attention.attention.key.bias"} loc("ViTBase":0:0), %arg398: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.12.attention.attention.value.weight"} loc("ViTBase":0:0), %arg399: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.12.attention.attention.value.bias"} loc("ViTBase":0:0), %arg400: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.12.attention.output.dense.weight"} loc("ViTBase":0:0), %arg401: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.12.attention.output.dense.bias"} loc("ViTBase":0:0), %arg402: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.12.layernorm_after.weight"} loc("ViTBase":0:0), %arg403: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.12.layernorm_after.bias"} loc("ViTBase":0:0), %arg404: tensor<1024x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.12.intermediate.dense.weight"} loc("ViTBase":0:0), %arg405: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.12.intermediate.dense.bias"} loc("ViTBase":0:0), %arg406: tensor<4096x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.12.output.dense.weight"} loc("ViTBase":0:0), %arg407: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.12.output.dense.bias"} loc("ViTBase":0:0), %arg408: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.13.layernorm_before.weight"} loc("ViTBase":0:0), %arg409: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.13.layernorm_before.bias"} loc("ViTBase":0:0), %arg410: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.13.attention.attention.query.weight"} loc("ViTBase":0:0), %arg411: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.13.attention.attention.query.bias"} loc("ViTBase":0:0), %arg412: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.13.attention.attention.key.weight"} loc("ViTBase":0:0), %arg413: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.13.attention.attention.key.bias"} loc("ViTBase":0:0), %arg414: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.13.attention.attention.value.weight"} loc("ViTBase":0:0), %arg415: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.13.attention.attention.value.bias"} loc("ViTBase":0:0), %arg416: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.13.attention.output.dense.weight"} loc("ViTBase":0:0), %arg417: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.13.attention.output.dense.bias"} loc("ViTBase":0:0), %arg418: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.13.layernorm_after.weight"} loc("ViTBase":0:0), %arg419: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.13.layernorm_after.bias"} loc("ViTBase":0:0), %arg420: tensor<1024x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.13.intermediate.dense.weight"} loc("ViTBase":0:0), %arg421: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.13.intermediate.dense.bias"} loc("ViTBase":0:0), %arg422: tensor<4096x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.13.output.dense.weight"} loc("ViTBase":0:0), %arg423: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.13.output.dense.bias"} loc("ViTBase":0:0), %arg424: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.14.layernorm_before.weight"} loc("ViTBase":0:0), %arg425: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.14.layernorm_before.bias"} loc("ViTBase":0:0), %arg426: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.14.attention.attention.query.weight"} loc("ViTBase":0:0), %arg427: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.14.attention.attention.query.bias"} loc("ViTBase":0:0), %arg428: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.14.attention.attention.key.weight"} loc("ViTBase":0:0), %arg429: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.14.attention.attention.key.bias"} loc("ViTBase":0:0), %arg430: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.14.attention.attention.value.weight"} loc("ViTBase":0:0), %arg431: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.14.attention.attention.value.bias"} loc("ViTBase":0:0), %arg432: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.14.attention.output.dense.weight"} loc("ViTBase":0:0), %arg433: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.14.attention.output.dense.bias"} loc("ViTBase":0:0), %arg434: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.14.layernorm_after.weight"} loc("ViTBase":0:0), %arg435: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.14.layernorm_after.bias"} loc("ViTBase":0:0), %arg436: tensor<1024x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.14.intermediate.dense.weight"} loc("ViTBase":0:0), %arg437: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.14.intermediate.dense.bias"} loc("ViTBase":0:0), %arg438: tensor<4096x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.14.output.dense.weight"} loc("ViTBase":0:0), %arg439: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.14.output.dense.bias"} loc("ViTBase":0:0), %arg440: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.15.layernorm_before.weight"} loc("ViTBase":0:0), %arg441: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.15.layernorm_before.bias"} loc("ViTBase":0:0), %arg442: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.15.attention.attention.query.weight"} loc("ViTBase":0:0), %arg443: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.15.attention.attention.query.bias"} loc("ViTBase":0:0), %arg444: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.15.attention.attention.key.weight"} loc("ViTBase":0:0), %arg445: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.15.attention.attention.key.bias"} loc("ViTBase":0:0), %arg446: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.15.attention.attention.value.weight"} loc("ViTBase":0:0), %arg447: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.15.attention.attention.value.bias"} loc("ViTBase":0:0), %arg448: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.15.attention.output.dense.weight"} loc("ViTBase":0:0), %arg449: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.15.attention.output.dense.bias"} loc("ViTBase":0:0), %arg450: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.15.layernorm_after.weight"} loc("ViTBase":0:0), %arg451: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.15.layernorm_after.bias"} loc("ViTBase":0:0), %arg452: tensor<1024x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.15.intermediate.dense.weight"} loc("ViTBase":0:0), %arg453: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.15.intermediate.dense.bias"} loc("ViTBase":0:0), %arg454: tensor<4096x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.15.output.dense.weight"} loc("ViTBase":0:0), %arg455: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.15.output.dense.bias"} loc("ViTBase":0:0), %arg456: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.16.layernorm_before.weight"} loc("ViTBase":0:0), %arg457: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.16.layernorm_before.bias"} loc("ViTBase":0:0), %arg458: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.16.attention.attention.query.weight"} loc("ViTBase":0:0), %arg459: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.16.attention.attention.query.bias"} loc("ViTBase":0:0), %arg460: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.16.attention.attention.key.weight"} loc("ViTBase":0:0), %arg461: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.16.attention.attention.key.bias"} loc("ViTBase":0:0), %arg462: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.16.attention.attention.value.weight"} loc("ViTBase":0:0), %arg463: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.16.attention.attention.value.bias"} loc("ViTBase":0:0), %arg464: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.16.attention.output.dense.weight"} loc("ViTBase":0:0), %arg465: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.16.attention.output.dense.bias"} loc("ViTBase":0:0), %arg466: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.16.layernorm_after.weight"} loc("ViTBase":0:0), %arg467: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.16.layernorm_after.bias"} loc("ViTBase":0:0), %arg468: tensor<1024x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.16.intermediate.dense.weight"} loc("ViTBase":0:0), %arg469: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.16.intermediate.dense.bias"} loc("ViTBase":0:0), %arg470: tensor<4096x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.16.output.dense.weight"} loc("ViTBase":0:0), %arg471: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.16.output.dense.bias"} loc("ViTBase":0:0), %arg472: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.17.layernorm_before.weight"} loc("ViTBase":0:0), %arg473: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.17.layernorm_before.bias"} loc("ViTBase":0:0), %arg474: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.17.attention.attention.query.weight"} loc("ViTBase":0:0), %arg475: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.17.attention.attention.query.bias"} loc("ViTBase":0:0), %arg476: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.17.attention.attention.key.weight"} loc("ViTBase":0:0), %arg477: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.17.attention.attention.key.bias"} loc("ViTBase":0:0), %arg478: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.17.attention.attention.value.weight"} loc("ViTBase":0:0), %arg479: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.17.attention.attention.value.bias"} loc("ViTBase":0:0), %arg480: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.17.attention.output.dense.weight"} loc("ViTBase":0:0), %arg481: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.17.attention.output.dense.bias"} loc("ViTBase":0:0), %arg482: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.17.layernorm_after.weight"} loc("ViTBase":0:0), %arg483: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.17.layernorm_after.bias"} loc("ViTBase":0:0), %arg484: tensor<1024x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.17.intermediate.dense.weight"} loc("ViTBase":0:0), %arg485: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.17.intermediate.dense.bias"} loc("ViTBase":0:0), %arg486: tensor<4096x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.17.output.dense.weight"} loc("ViTBase":0:0), %arg487: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.17.output.dense.bias"} loc("ViTBase":0:0), %arg488: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.18.layernorm_before.weight"} loc("ViTBase":0:0), %arg489: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.18.layernorm_before.bias"} loc("ViTBase":0:0), %arg490: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.18.attention.attention.query.weight"} loc("ViTBase":0:0), %arg491: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.18.attention.attention.query.bias"} loc("ViTBase":0:0), %arg492: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.18.attention.attention.key.weight"} loc("ViTBase":0:0), %arg493: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.18.attention.attention.key.bias"} loc("ViTBase":0:0), %arg494: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.18.attention.attention.value.weight"} loc("ViTBase":0:0), %arg495: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.18.attention.attention.value.bias"} loc("ViTBase":0:0), %arg496: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.18.attention.output.dense.weight"} loc("ViTBase":0:0), %arg497: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.18.attention.output.dense.bias"} loc("ViTBase":0:0), %arg498: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.18.layernorm_after.weight"} loc("ViTBase":0:0), %arg499: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.18.layernorm_after.bias"} loc("ViTBase":0:0), %arg500: tensor<1024x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.18.intermediate.dense.weight"} loc("ViTBase":0:0), %arg501: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.18.intermediate.dense.bias"} loc("ViTBase":0:0), %arg502: tensor<4096x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.18.output.dense.weight"} loc("ViTBase":0:0), %arg503: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.18.output.dense.bias"} loc("ViTBase":0:0), %arg504: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.19.layernorm_before.weight"} loc("ViTBase":0:0), %arg505: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.19.layernorm_before.bias"} loc("ViTBase":0:0), %arg506: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.19.attention.attention.query.weight"} loc("ViTBase":0:0), %arg507: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.19.attention.attention.query.bias"} loc("ViTBase":0:0), %arg508: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.19.attention.attention.key.weight"} loc("ViTBase":0:0), %arg509: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.19.attention.attention.key.bias"} loc("ViTBase":0:0), %arg510: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.19.attention.attention.value.weight"} loc("ViTBase":0:0), %arg511: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.19.attention.attention.value.bias"} loc("ViTBase":0:0), %arg512: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.19.attention.output.dense.weight"} loc("ViTBase":0:0), %arg513: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.19.attention.output.dense.bias"} loc("ViTBase":0:0), %arg514: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.19.layernorm_after.weight"} loc("ViTBase":0:0), %arg515: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.19.layernorm_after.bias"} loc("ViTBase":0:0), %arg516: tensor<1024x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.19.intermediate.dense.weight"} loc("ViTBase":0:0), %arg517: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.19.intermediate.dense.bias"} loc("ViTBase":0:0), %arg518: tensor<4096x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.19.output.dense.weight"} loc("ViTBase":0:0), %arg519: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.19.output.dense.bias"} loc("ViTBase":0:0), %arg520: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.20.layernorm_before.weight"} loc("ViTBase":0:0), %arg521: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.20.layernorm_before.bias"} loc("ViTBase":0:0), %arg522: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.20.attention.attention.query.weight"} loc("ViTBase":0:0), %arg523: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.20.attention.attention.query.bias"} loc("ViTBase":0:0), %arg524: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.20.attention.attention.key.weight"} loc("ViTBase":0:0), %arg525: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.20.attention.attention.key.bias"} loc("ViTBase":0:0), %arg526: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.20.attention.attention.value.weight"} loc("ViTBase":0:0), %arg527: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.20.attention.attention.value.bias"} loc("ViTBase":0:0), %arg528: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.20.attention.output.dense.weight"} loc("ViTBase":0:0), %arg529: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.20.attention.output.dense.bias"} loc("ViTBase":0:0), %arg530: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.20.layernorm_after.weight"} loc("ViTBase":0:0), %arg531: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.20.layernorm_after.bias"} loc("ViTBase":0:0), %arg532: tensor<1024x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.20.intermediate.dense.weight"} loc("ViTBase":0:0), %arg533: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.20.intermediate.dense.bias"} loc("ViTBase":0:0), %arg534: tensor<4096x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.20.output.dense.weight"} loc("ViTBase":0:0), %arg535: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.20.output.dense.bias"} loc("ViTBase":0:0), %arg536: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.21.layernorm_before.weight"} loc("ViTBase":0:0), %arg537: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.21.layernorm_before.bias"} loc("ViTBase":0:0), %arg538: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.21.attention.attention.query.weight"} loc("ViTBase":0:0), %arg539: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.21.attention.attention.query.bias"} loc("ViTBase":0:0), %arg540: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.21.attention.attention.key.weight"} loc("ViTBase":0:0), %arg541: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.21.attention.attention.key.bias"} loc("ViTBase":0:0), %arg542: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.21.attention.attention.value.weight"} loc("ViTBase":0:0), %arg543: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.21.attention.attention.value.bias"} loc("ViTBase":0:0), %arg544: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.21.attention.output.dense.weight"} loc("ViTBase":0:0), %arg545: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.21.attention.output.dense.bias"} loc("ViTBase":0:0), %arg546: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.21.layernorm_after.weight"} loc("ViTBase":0:0), %arg547: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.21.layernorm_after.bias"} loc("ViTBase":0:0), %arg548: tensor<1024x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.21.intermediate.dense.weight"} loc("ViTBase":0:0), %arg549: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.21.intermediate.dense.bias"} loc("ViTBase":0:0), %arg550: tensor<4096x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.21.output.dense.weight"} loc("ViTBase":0:0), %arg551: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.21.output.dense.bias"} loc("ViTBase":0:0), %arg552: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.22.layernorm_before.weight"} loc("ViTBase":0:0), %arg553: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.22.layernorm_before.bias"} loc("ViTBase":0:0), %arg554: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.22.attention.attention.query.weight"} loc("ViTBase":0:0), %arg555: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.22.attention.attention.query.bias"} loc("ViTBase":0:0), %arg556: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.22.attention.attention.key.weight"} loc("ViTBase":0:0), %arg557: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.22.attention.attention.key.bias"} loc("ViTBase":0:0), %arg558: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.22.attention.attention.value.weight"} loc("ViTBase":0:0), %arg559: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.22.attention.attention.value.bias"} loc("ViTBase":0:0), %arg560: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.22.attention.output.dense.weight"} loc("ViTBase":0:0), %arg561: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.22.attention.output.dense.bias"} loc("ViTBase":0:0), %arg562: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.22.layernorm_after.weight"} loc("ViTBase":0:0), %arg563: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.22.layernorm_after.bias"} loc("ViTBase":0:0), %arg564: tensor<1024x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.22.intermediate.dense.weight"} loc("ViTBase":0:0), %arg565: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.22.intermediate.dense.bias"} loc("ViTBase":0:0), %arg566: tensor<4096x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.22.output.dense.weight"} loc("ViTBase":0:0), %arg567: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.22.output.dense.bias"} loc("ViTBase":0:0), %arg568: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.23.layernorm_before.weight"} loc("ViTBase":0:0), %arg569: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.23.layernorm_before.bias"} loc("ViTBase":0:0), %arg570: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.23.attention.attention.query.weight"} loc("ViTBase":0:0), %arg571: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.23.attention.attention.query.bias"} loc("ViTBase":0:0), %arg572: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.23.attention.attention.key.weight"} loc("ViTBase":0:0), %arg573: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.23.attention.attention.key.bias"} loc("ViTBase":0:0), %arg574: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.23.attention.attention.value.weight"} loc("ViTBase":0:0), %arg575: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.23.attention.attention.value.bias"} loc("ViTBase":0:0), %arg576: tensor<1024x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.23.attention.output.dense.weight"} loc("ViTBase":0:0), %arg577: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.23.attention.output.dense.bias"} loc("ViTBase":0:0), %arg578: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.23.layernorm_after.weight"} loc("ViTBase":0:0), %arg579: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.23.layernorm_after.bias"} loc("ViTBase":0:0), %arg580: tensor<1024x4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.23.intermediate.dense.weight"} loc("ViTBase":0:0), %arg581: tensor<4096xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.23.intermediate.dense.bias"} loc("ViTBase":0:0), %arg582: tensor<4096x1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.23.output.dense.weight"} loc("ViTBase":0:0), %arg583: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.encoder.layer.23.output.dense.bias"} loc("ViTBase":0:0), %arg584: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.layernorm.weight"} loc("ViTBase":0:0), %arg585: tensor<1024xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "vit.layernorm.bias"} loc("ViTBase":0:0), %arg586: tensor<1024x1000xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "classifier.weight"} loc("ViTBase":0:0), %arg587: tensor<1000xbf16> {ttcore.argument_type = #ttcore.argument_type<parameter>, ttir.name = "classifier.bias"} loc("ViTBase":0:0)) -> (tensor<8x1000xbf16> {ttir.name = "ViTBase.output_add_1336"}) {
    %0 = ttir.empty() : tensor<8x224x3x224xbf16> loc(#loc367)
    %1 = "ttir.transpose"(%arg0, %0) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x3x224x224xbf16>, tensor<8x224x3x224xbf16>) -> tensor<8x224x3x224xbf16> loc(#loc367)
    %2 = ttir.empty() : tensor<8x224x224x3xbf16> loc(#loc368)
    %3 = "ttir.transpose"(%1, %2) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<8x224x3x224xbf16>, tensor<8x224x224x3xbf16>) -> tensor<8x224x224x3xbf16> loc(#loc368)
    %4 = ttir.empty() : tensor<8x14x14x1024xbf16> loc(#loc369)
    %5 = "ttir.conv2d"(%3, %arg197, %arg198, %4) <{dilation = array<i32: 1, 1>, groups = 1 : i32, padding = array<i32: 0, 0, 0, 0>, stride = array<i32: 16, 16>}> {channel_last = 1 : si32} : (tensor<8x224x224x3xbf16>, tensor<1024x3x16x16xbf16>, tensor<1x1x1x1024xbf16>, tensor<8x14x14x1024xbf16>) -> tensor<8x14x14x1024xbf16> loc(#loc369)
    %6 = ttir.empty() : tensor<8x14x1024x14xbf16> loc(#loc370)
    %7 = "ttir.transpose"(%5, %6) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<8x14x14x1024xbf16>, tensor<8x14x1024x14xbf16>) -> tensor<8x14x1024x14xbf16> loc(#loc370)
    %8 = ttir.empty() : tensor<8x1024x14x14xbf16> loc(#loc371)
    %9 = "ttir.transpose"(%7, %8) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x14x1024x14xbf16>, tensor<8x1024x14x14xbf16>) -> tensor<8x1024x14x14xbf16> loc(#loc371)
    %10 = ttir.empty() : tensor<8x1024x196x1xbf16> loc(#loc220)
    %11 = "ttir.reshape"(%9, %10) <{shape = [8 : i32, 1024 : i32, 196 : i32, 1 : i32]}> : (tensor<8x1024x14x14xbf16>, tensor<8x1024x196x1xbf16>) -> tensor<8x1024x196x1xbf16> loc(#loc220)
    %12 = ttir.empty() : tensor<8x1024x196xbf16> loc(#loc221)
    %13 = "ttir.squeeze"(%11, %12) <{dim = -1 : si32}> : (tensor<8x1024x196x1xbf16>, tensor<8x1024x196xbf16>) -> tensor<8x1024x196xbf16> loc(#loc221)
    %14 = ttir.empty() : tensor<8x196x1024xbf16> loc(#loc222)
    %15 = "ttir.transpose"(%13, %14) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<8x1024x196xbf16>, tensor<8x196x1024xbf16>) -> tensor<8x196x1024xbf16> loc(#loc222)
    %16 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc181)
    %17 = "ttir.concat"(%arg196, %15, %16) <{dim = -2 : si32}> : (tensor<8x1x1024xbf16>, tensor<8x196x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc181)
    %18 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc182)
    %19 = "ttir.add"(%17, %arg199, %18) : (tensor<8x197x1024xbf16>, tensor<1x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc182)
    %20 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc372)
    %21 = "ttir.sum"(%19, %20) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc372)
    %22 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc373)
    %23 = "ttir.multiply"(%arg1, %21, %22) : (tensor<8x197x1024xf32>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc373)
    %24 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc374)
    %25 = "ttir.subtract"(%19, %23, %24) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc374)
    %26 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc375)
    %27 = "ttir.multiply"(%25, %25, %26) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc375)
    %28 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc376)
    %29 = "ttir.sum"(%27, %28) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc376)
    %30 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc377)
    %31 = "ttir.multiply"(%arg2, %29, %30) : (tensor<8x197x1xf32>, tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc377)
    %32 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc378)
    %33 = "ttir.add"(%31, %arg3, %32) : (tensor<8x197x1xbf16>, tensor<8x197x1xf32>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc378)
    %34 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc379)
    %35 = "ttir.sqrt"(%33, %34) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc379)
    %36 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc380)
    %37 = "ttir.reciprocal"(%35, %36) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc380)
    %38 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc381)
    %39 = "ttir.multiply"(%25, %37, %38) : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc381)
    %40 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc382)
    %41 = "ttir.multiply"(%39, %arg200, %40) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc382)
    %42 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc383)
    %43 = "ttir.add"(%41, %arg201, %42) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc383)
    %44 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1788)
    %45 = "ttir.reshape"(%43, %44) <{shape = [1576 : i32, 1024 : i32]}> : (tensor<8x197x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1788)
    %46 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1789)
    %47 = "ttir.matmul"(%45, %arg202, %46) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1789)
    %48 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1790)
    %49 = "ttir.reshape"(%47, %48) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1790)
    %50 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc2)
    %51 = "ttir.add"(%49, %arg203, %50) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc2)
    %52 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1093)
    %53 = "ttir.reshape"(%51, %52) <{shape = [8 : i32, 197 : i32, 16 : i32, 64 : i32]}> : (tensor<8x197x1024xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1093)
    %54 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1094)
    %55 = "ttir.transpose"(%53, %54) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x197x16x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1094)
    %56 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1095)
    %57 = "ttir.reshape"(%55, %56) <{shape = [128 : i32, 197 : i32, 64 : i32]}> : (tensor<8x16x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1095)
    %58 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1791)
    %59 = "ttir.matmul"(%45, %arg204, %58) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1791)
    %60 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1792)
    %61 = "ttir.reshape"(%59, %60) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1792)
    %62 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc3)
    %63 = "ttir.add"(%61, %arg205, %62) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc3)
    %64 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1097)
    %65 = "ttir.reshape"(%63, %64) <{shape = [8 : i32, 197 : i32, 16 : i32, 64 : i32]}> : (tensor<8x197x1024xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1097)
    %66 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1098)
    %67 = "ttir.transpose"(%65, %66) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x197x16x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1098)
    %68 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1099)
    %69 = "ttir.reshape"(%67, %68) <{shape = [128 : i32, 197 : i32, 64 : i32]}> : (tensor<8x16x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1099)
    %70 = ttir.empty() : tensor<128x64x197xbf16> loc(#loc4)
    %71 = "ttir.transpose"(%69, %70) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<128x197x64xbf16>, tensor<128x64x197xbf16>) -> tensor<128x64x197xbf16> loc(#loc4)
    %72 = ttir.empty() : tensor<128x197x197xbf16> loc(#loc1100)
    %73 = "ttir.matmul"(%57, %71, %72) <{transpose_a = false, transpose_b = false}> : (tensor<128x197x64xbf16>, tensor<128x64x197xbf16>, tensor<128x197x197xbf16>) -> tensor<128x197x197xbf16> loc(#loc1100)
    %74 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1101)
    %75 = "ttir.reshape"(%73, %74) <{shape = [8 : i32, 16 : i32, 197 : i32, 197 : i32]}> : (tensor<128x197x197xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1101)
    %76 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1102)
    %77 = "ttir.multiply"(%75, %arg4, %76) : (tensor<8x16x197x197xbf16>, tensor<1xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1102)
    %78 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1103)
    %79 = "ttir.add"(%77, %arg5, %78) : (tensor<8x16x197x197xbf16>, tensor<197x197xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1103)
    %80 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1104)
    %81 = "ttir.softmax"(%79, %80) <{dimension = -1 : si32}> : (tensor<8x16x197x197xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1104)
    %82 = ttir.empty() : tensor<128x197x197xbf16> loc(#loc1105)
    %83 = "ttir.reshape"(%81, %82) <{shape = [128 : i32, 197 : i32, 197 : i32]}> : (tensor<8x16x197x197xbf16>, tensor<128x197x197xbf16>) -> tensor<128x197x197xbf16> loc(#loc1105)
    %84 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1793)
    %85 = "ttir.matmul"(%45, %arg206, %84) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1793)
    %86 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1794)
    %87 = "ttir.reshape"(%85, %86) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1794)
    %88 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc5)
    %89 = "ttir.add"(%87, %arg207, %88) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc5)
    %90 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1107)
    %91 = "ttir.reshape"(%89, %90) <{shape = [8 : i32, 197 : i32, 16 : i32, 64 : i32]}> : (tensor<8x197x1024xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1107)
    %92 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1108)
    %93 = "ttir.transpose"(%91, %92) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x197x16x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1108)
    %94 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1109)
    %95 = "ttir.reshape"(%93, %94) <{shape = [128 : i32, 197 : i32, 64 : i32]}> : (tensor<8x16x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1109)
    %96 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1110)
    %97 = "ttir.matmul"(%83, %95, %96) <{transpose_a = false, transpose_b = false}> : (tensor<128x197x197xbf16>, tensor<128x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1110)
    %98 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1111)
    %99 = "ttir.reshape"(%97, %98) <{shape = [8 : i32, 16 : i32, 197 : i32, 64 : i32]}> : (tensor<128x197x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1111)
    %100 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1112)
    %101 = "ttir.transpose"(%99, %100) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x16x197x64xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1112)
    %102 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1795)
    %103 = "ttir.reshape"(%101, %102) <{shape = [1576 : i32, 1024 : i32]}> : (tensor<8x197x16x64xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1795)
    %104 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1796)
    %105 = "ttir.matmul"(%103, %arg208, %104) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1796)
    %106 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1797)
    %107 = "ttir.reshape"(%105, %106) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1797)
    %108 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc6)
    %109 = "ttir.add"(%107, %arg209, %108) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc6)
    %110 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc225)
    %111 = "ttir.add"(%109, %19, %110) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc225)
    %112 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc386)
    %113 = "ttir.sum"(%111, %112) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc386)
    %114 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc387)
    %115 = "ttir.multiply"(%arg6, %113, %114) : (tensor<8x197x1024xf32>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc387)
    %116 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc388)
    %117 = "ttir.subtract"(%111, %115, %116) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc388)
    %118 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc389)
    %119 = "ttir.multiply"(%117, %117, %118) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc389)
    %120 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc390)
    %121 = "ttir.sum"(%119, %120) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc390)
    %122 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc391)
    %123 = "ttir.multiply"(%arg7, %121, %122) : (tensor<8x197x1xf32>, tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc391)
    %124 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc392)
    %125 = "ttir.add"(%123, %arg8, %124) : (tensor<8x197x1xbf16>, tensor<8x197x1xf32>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc392)
    %126 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc393)
    %127 = "ttir.sqrt"(%125, %126) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc393)
    %128 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc394)
    %129 = "ttir.reciprocal"(%127, %128) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc394)
    %130 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc395)
    %131 = "ttir.multiply"(%117, %129, %130) : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc395)
    %132 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc396)
    %133 = "ttir.multiply"(%131, %arg210, %132) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc396)
    %134 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc397)
    %135 = "ttir.add"(%133, %arg211, %134) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc397)
    %136 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1114)
    %137 = "ttir.reshape"(%135, %136) <{shape = [1576 : i32, 1024 : i32]}> : (tensor<8x197x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1114)
    %138 = ttir.empty() : tensor<1576x4096xbf16> loc(#loc1115)
    %139 = "ttir.matmul"(%137, %arg212, %138) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x4096xbf16>, tensor<1576x4096xbf16>) -> tensor<1576x4096xbf16> loc(#loc1115)
    %140 = ttir.empty() : tensor<8x197x4096xbf16> loc(#loc1116)
    %141 = "ttir.reshape"(%139, %140) <{shape = [8 : i32, 197 : i32, 4096 : i32]}> : (tensor<1576x4096xbf16>, tensor<8x197x4096xbf16>) -> tensor<8x197x4096xbf16> loc(#loc1116)
    %142 = ttir.empty() : tensor<8x197x4096xbf16> loc(#loc7)
    %143 = "ttir.add"(%141, %arg213, %142) : (tensor<8x197x4096xbf16>, tensor<4096xbf16>, tensor<8x197x4096xbf16>) -> tensor<8x197x4096xbf16> loc(#loc7)
    %144 = ttir.empty() : tensor<8x197x4096xbf16> loc(#loc1117)
    %145 = "ttir.gelu"(%143, %144) : (tensor<8x197x4096xbf16>, tensor<8x197x4096xbf16>) -> tensor<8x197x4096xbf16> loc(#loc1117)
    %146 = ttir.empty() : tensor<1576x4096xbf16> loc(#loc1118)
    %147 = "ttir.reshape"(%145, %146) <{shape = [1576 : i32, 4096 : i32]}> : (tensor<8x197x4096xbf16>, tensor<1576x4096xbf16>) -> tensor<1576x4096xbf16> loc(#loc1118)
    %148 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1119)
    %149 = "ttir.matmul"(%147, %arg214, %148) <{transpose_a = false, transpose_b = false}> : (tensor<1576x4096xbf16>, tensor<4096x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1119)
    %150 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1120)
    %151 = "ttir.reshape"(%149, %150) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1120)
    %152 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc8)
    %153 = "ttir.add"(%151, %arg215, %152) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc8)
    %154 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc401)
    %155 = "ttir.add"(%153, %111, %154) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc401)
    %156 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc402)
    %157 = "ttir.sum"(%155, %156) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc402)
    %158 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc403)
    %159 = "ttir.multiply"(%arg9, %157, %158) : (tensor<8x197x1024xf32>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc403)
    %160 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc404)
    %161 = "ttir.subtract"(%155, %159, %160) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc404)
    %162 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc405)
    %163 = "ttir.multiply"(%161, %161, %162) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc405)
    %164 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc406)
    %165 = "ttir.sum"(%163, %164) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc406)
    %166 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc407)
    %167 = "ttir.multiply"(%arg10, %165, %166) : (tensor<8x197x1xf32>, tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc407)
    %168 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc408)
    %169 = "ttir.add"(%167, %arg11, %168) : (tensor<8x197x1xbf16>, tensor<8x197x1xf32>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc408)
    %170 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc409)
    %171 = "ttir.sqrt"(%169, %170) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc409)
    %172 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc410)
    %173 = "ttir.reciprocal"(%171, %172) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc410)
    %174 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc411)
    %175 = "ttir.multiply"(%161, %173, %174) : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc411)
    %176 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc412)
    %177 = "ttir.multiply"(%175, %arg216, %176) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc412)
    %178 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc413)
    %179 = "ttir.add"(%177, %arg217, %178) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc413)
    %180 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1798)
    %181 = "ttir.reshape"(%179, %180) <{shape = [1576 : i32, 1024 : i32]}> : (tensor<8x197x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1798)
    %182 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1799)
    %183 = "ttir.matmul"(%181, %arg218, %182) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1799)
    %184 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1800)
    %185 = "ttir.reshape"(%183, %184) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1800)
    %186 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc9)
    %187 = "ttir.add"(%185, %arg219, %186) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc9)
    %188 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1122)
    %189 = "ttir.reshape"(%187, %188) <{shape = [8 : i32, 197 : i32, 16 : i32, 64 : i32]}> : (tensor<8x197x1024xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1122)
    %190 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1123)
    %191 = "ttir.transpose"(%189, %190) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x197x16x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1123)
    %192 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1124)
    %193 = "ttir.reshape"(%191, %192) <{shape = [128 : i32, 197 : i32, 64 : i32]}> : (tensor<8x16x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1124)
    %194 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1801)
    %195 = "ttir.matmul"(%181, %arg220, %194) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1801)
    %196 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1802)
    %197 = "ttir.reshape"(%195, %196) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1802)
    %198 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc10)
    %199 = "ttir.add"(%197, %arg221, %198) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc10)
    %200 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1126)
    %201 = "ttir.reshape"(%199, %200) <{shape = [8 : i32, 197 : i32, 16 : i32, 64 : i32]}> : (tensor<8x197x1024xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1126)
    %202 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1127)
    %203 = "ttir.transpose"(%201, %202) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x197x16x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1127)
    %204 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1128)
    %205 = "ttir.reshape"(%203, %204) <{shape = [128 : i32, 197 : i32, 64 : i32]}> : (tensor<8x16x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1128)
    %206 = ttir.empty() : tensor<128x64x197xbf16> loc(#loc11)
    %207 = "ttir.transpose"(%205, %206) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<128x197x64xbf16>, tensor<128x64x197xbf16>) -> tensor<128x64x197xbf16> loc(#loc11)
    %208 = ttir.empty() : tensor<128x197x197xbf16> loc(#loc1129)
    %209 = "ttir.matmul"(%193, %207, %208) <{transpose_a = false, transpose_b = false}> : (tensor<128x197x64xbf16>, tensor<128x64x197xbf16>, tensor<128x197x197xbf16>) -> tensor<128x197x197xbf16> loc(#loc1129)
    %210 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1130)
    %211 = "ttir.reshape"(%209, %210) <{shape = [8 : i32, 16 : i32, 197 : i32, 197 : i32]}> : (tensor<128x197x197xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1130)
    %212 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1131)
    %213 = "ttir.multiply"(%211, %arg12, %212) : (tensor<8x16x197x197xbf16>, tensor<1xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1131)
    %214 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1132)
    %215 = "ttir.add"(%213, %arg13, %214) : (tensor<8x16x197x197xbf16>, tensor<197x197xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1132)
    %216 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1133)
    %217 = "ttir.softmax"(%215, %216) <{dimension = -1 : si32}> : (tensor<8x16x197x197xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1133)
    %218 = ttir.empty() : tensor<128x197x197xbf16> loc(#loc1134)
    %219 = "ttir.reshape"(%217, %218) <{shape = [128 : i32, 197 : i32, 197 : i32]}> : (tensor<8x16x197x197xbf16>, tensor<128x197x197xbf16>) -> tensor<128x197x197xbf16> loc(#loc1134)
    %220 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1803)
    %221 = "ttir.matmul"(%181, %arg222, %220) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1803)
    %222 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1804)
    %223 = "ttir.reshape"(%221, %222) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1804)
    %224 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc12)
    %225 = "ttir.add"(%223, %arg223, %224) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc12)
    %226 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1136)
    %227 = "ttir.reshape"(%225, %226) <{shape = [8 : i32, 197 : i32, 16 : i32, 64 : i32]}> : (tensor<8x197x1024xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1136)
    %228 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1137)
    %229 = "ttir.transpose"(%227, %228) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x197x16x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1137)
    %230 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1138)
    %231 = "ttir.reshape"(%229, %230) <{shape = [128 : i32, 197 : i32, 64 : i32]}> : (tensor<8x16x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1138)
    %232 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1139)
    %233 = "ttir.matmul"(%219, %231, %232) <{transpose_a = false, transpose_b = false}> : (tensor<128x197x197xbf16>, tensor<128x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1139)
    %234 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1140)
    %235 = "ttir.reshape"(%233, %234) <{shape = [8 : i32, 16 : i32, 197 : i32, 64 : i32]}> : (tensor<128x197x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1140)
    %236 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1141)
    %237 = "ttir.transpose"(%235, %236) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x16x197x64xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1141)
    %238 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1805)
    %239 = "ttir.reshape"(%237, %238) <{shape = [1576 : i32, 1024 : i32]}> : (tensor<8x197x16x64xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1805)
    %240 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1806)
    %241 = "ttir.matmul"(%239, %arg224, %240) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1806)
    %242 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1807)
    %243 = "ttir.reshape"(%241, %242) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1807)
    %244 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc13)
    %245 = "ttir.add"(%243, %arg225, %244) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc13)
    %246 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc231)
    %247 = "ttir.add"(%245, %155, %246) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc231)
    %248 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc416)
    %249 = "ttir.sum"(%247, %248) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc416)
    %250 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc417)
    %251 = "ttir.multiply"(%arg14, %249, %250) : (tensor<8x197x1024xf32>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc417)
    %252 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc418)
    %253 = "ttir.subtract"(%247, %251, %252) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc418)
    %254 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc419)
    %255 = "ttir.multiply"(%253, %253, %254) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc419)
    %256 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc420)
    %257 = "ttir.sum"(%255, %256) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc420)
    %258 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc421)
    %259 = "ttir.multiply"(%arg15, %257, %258) : (tensor<8x197x1xf32>, tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc421)
    %260 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc422)
    %261 = "ttir.add"(%259, %arg16, %260) : (tensor<8x197x1xbf16>, tensor<8x197x1xf32>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc422)
    %262 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc423)
    %263 = "ttir.sqrt"(%261, %262) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc423)
    %264 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc424)
    %265 = "ttir.reciprocal"(%263, %264) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc424)
    %266 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc425)
    %267 = "ttir.multiply"(%253, %265, %266) : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc425)
    %268 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc426)
    %269 = "ttir.multiply"(%267, %arg226, %268) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc426)
    %270 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc427)
    %271 = "ttir.add"(%269, %arg227, %270) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc427)
    %272 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1143)
    %273 = "ttir.reshape"(%271, %272) <{shape = [1576 : i32, 1024 : i32]}> : (tensor<8x197x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1143)
    %274 = ttir.empty() : tensor<1576x4096xbf16> loc(#loc1144)
    %275 = "ttir.matmul"(%273, %arg228, %274) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x4096xbf16>, tensor<1576x4096xbf16>) -> tensor<1576x4096xbf16> loc(#loc1144)
    %276 = ttir.empty() : tensor<8x197x4096xbf16> loc(#loc1145)
    %277 = "ttir.reshape"(%275, %276) <{shape = [8 : i32, 197 : i32, 4096 : i32]}> : (tensor<1576x4096xbf16>, tensor<8x197x4096xbf16>) -> tensor<8x197x4096xbf16> loc(#loc1145)
    %278 = ttir.empty() : tensor<8x197x4096xbf16> loc(#loc14)
    %279 = "ttir.add"(%277, %arg229, %278) : (tensor<8x197x4096xbf16>, tensor<4096xbf16>, tensor<8x197x4096xbf16>) -> tensor<8x197x4096xbf16> loc(#loc14)
    %280 = ttir.empty() : tensor<8x197x4096xbf16> loc(#loc1146)
    %281 = "ttir.gelu"(%279, %280) : (tensor<8x197x4096xbf16>, tensor<8x197x4096xbf16>) -> tensor<8x197x4096xbf16> loc(#loc1146)
    %282 = ttir.empty() : tensor<1576x4096xbf16> loc(#loc1147)
    %283 = "ttir.reshape"(%281, %282) <{shape = [1576 : i32, 4096 : i32]}> : (tensor<8x197x4096xbf16>, tensor<1576x4096xbf16>) -> tensor<1576x4096xbf16> loc(#loc1147)
    %284 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1148)
    %285 = "ttir.matmul"(%283, %arg230, %284) <{transpose_a = false, transpose_b = false}> : (tensor<1576x4096xbf16>, tensor<4096x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1148)
    %286 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1149)
    %287 = "ttir.reshape"(%285, %286) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1149)
    %288 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc15)
    %289 = "ttir.add"(%287, %arg231, %288) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc15)
    %290 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc431)
    %291 = "ttir.add"(%289, %247, %290) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc431)
    %292 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc432)
    %293 = "ttir.sum"(%291, %292) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc432)
    %294 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc433)
    %295 = "ttir.multiply"(%arg17, %293, %294) : (tensor<8x197x1024xf32>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc433)
    %296 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc434)
    %297 = "ttir.subtract"(%291, %295, %296) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc434)
    %298 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc435)
    %299 = "ttir.multiply"(%297, %297, %298) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc435)
    %300 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc436)
    %301 = "ttir.sum"(%299, %300) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc436)
    %302 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc437)
    %303 = "ttir.multiply"(%arg18, %301, %302) : (tensor<8x197x1xf32>, tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc437)
    %304 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc438)
    %305 = "ttir.add"(%303, %arg19, %304) : (tensor<8x197x1xbf16>, tensor<8x197x1xf32>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc438)
    %306 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc439)
    %307 = "ttir.sqrt"(%305, %306) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc439)
    %308 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc440)
    %309 = "ttir.reciprocal"(%307, %308) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc440)
    %310 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc441)
    %311 = "ttir.multiply"(%297, %309, %310) : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc441)
    %312 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc442)
    %313 = "ttir.multiply"(%311, %arg232, %312) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc442)
    %314 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc443)
    %315 = "ttir.add"(%313, %arg233, %314) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc443)
    %316 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1808)
    %317 = "ttir.reshape"(%315, %316) <{shape = [1576 : i32, 1024 : i32]}> : (tensor<8x197x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1808)
    %318 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1809)
    %319 = "ttir.matmul"(%317, %arg234, %318) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1809)
    %320 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1810)
    %321 = "ttir.reshape"(%319, %320) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1810)
    %322 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc16)
    %323 = "ttir.add"(%321, %arg235, %322) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc16)
    %324 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1151)
    %325 = "ttir.reshape"(%323, %324) <{shape = [8 : i32, 197 : i32, 16 : i32, 64 : i32]}> : (tensor<8x197x1024xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1151)
    %326 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1152)
    %327 = "ttir.transpose"(%325, %326) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x197x16x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1152)
    %328 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1153)
    %329 = "ttir.reshape"(%327, %328) <{shape = [128 : i32, 197 : i32, 64 : i32]}> : (tensor<8x16x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1153)
    %330 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1811)
    %331 = "ttir.matmul"(%317, %arg236, %330) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1811)
    %332 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1812)
    %333 = "ttir.reshape"(%331, %332) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1812)
    %334 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc17)
    %335 = "ttir.add"(%333, %arg237, %334) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc17)
    %336 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1155)
    %337 = "ttir.reshape"(%335, %336) <{shape = [8 : i32, 197 : i32, 16 : i32, 64 : i32]}> : (tensor<8x197x1024xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1155)
    %338 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1156)
    %339 = "ttir.transpose"(%337, %338) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x197x16x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1156)
    %340 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1157)
    %341 = "ttir.reshape"(%339, %340) <{shape = [128 : i32, 197 : i32, 64 : i32]}> : (tensor<8x16x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1157)
    %342 = ttir.empty() : tensor<128x64x197xbf16> loc(#loc18)
    %343 = "ttir.transpose"(%341, %342) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<128x197x64xbf16>, tensor<128x64x197xbf16>) -> tensor<128x64x197xbf16> loc(#loc18)
    %344 = ttir.empty() : tensor<128x197x197xbf16> loc(#loc1158)
    %345 = "ttir.matmul"(%329, %343, %344) <{transpose_a = false, transpose_b = false}> : (tensor<128x197x64xbf16>, tensor<128x64x197xbf16>, tensor<128x197x197xbf16>) -> tensor<128x197x197xbf16> loc(#loc1158)
    %346 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1159)
    %347 = "ttir.reshape"(%345, %346) <{shape = [8 : i32, 16 : i32, 197 : i32, 197 : i32]}> : (tensor<128x197x197xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1159)
    %348 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1160)
    %349 = "ttir.multiply"(%347, %arg20, %348) : (tensor<8x16x197x197xbf16>, tensor<1xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1160)
    %350 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1161)
    %351 = "ttir.add"(%349, %arg21, %350) : (tensor<8x16x197x197xbf16>, tensor<197x197xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1161)
    %352 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1162)
    %353 = "ttir.softmax"(%351, %352) <{dimension = -1 : si32}> : (tensor<8x16x197x197xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1162)
    %354 = ttir.empty() : tensor<128x197x197xbf16> loc(#loc1163)
    %355 = "ttir.reshape"(%353, %354) <{shape = [128 : i32, 197 : i32, 197 : i32]}> : (tensor<8x16x197x197xbf16>, tensor<128x197x197xbf16>) -> tensor<128x197x197xbf16> loc(#loc1163)
    %356 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1813)
    %357 = "ttir.matmul"(%317, %arg238, %356) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1813)
    %358 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1814)
    %359 = "ttir.reshape"(%357, %358) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1814)
    %360 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc19)
    %361 = "ttir.add"(%359, %arg239, %360) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc19)
    %362 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1165)
    %363 = "ttir.reshape"(%361, %362) <{shape = [8 : i32, 197 : i32, 16 : i32, 64 : i32]}> : (tensor<8x197x1024xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1165)
    %364 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1166)
    %365 = "ttir.transpose"(%363, %364) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x197x16x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1166)
    %366 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1167)
    %367 = "ttir.reshape"(%365, %366) <{shape = [128 : i32, 197 : i32, 64 : i32]}> : (tensor<8x16x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1167)
    %368 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1168)
    %369 = "ttir.matmul"(%355, %367, %368) <{transpose_a = false, transpose_b = false}> : (tensor<128x197x197xbf16>, tensor<128x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1168)
    %370 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1169)
    %371 = "ttir.reshape"(%369, %370) <{shape = [8 : i32, 16 : i32, 197 : i32, 64 : i32]}> : (tensor<128x197x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1169)
    %372 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1170)
    %373 = "ttir.transpose"(%371, %372) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x16x197x64xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1170)
    %374 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1815)
    %375 = "ttir.reshape"(%373, %374) <{shape = [1576 : i32, 1024 : i32]}> : (tensor<8x197x16x64xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1815)
    %376 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1816)
    %377 = "ttir.matmul"(%375, %arg240, %376) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1816)
    %378 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1817)
    %379 = "ttir.reshape"(%377, %378) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1817)
    %380 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc20)
    %381 = "ttir.add"(%379, %arg241, %380) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc20)
    %382 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc237)
    %383 = "ttir.add"(%381, %291, %382) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc237)
    %384 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc446)
    %385 = "ttir.sum"(%383, %384) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc446)
    %386 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc447)
    %387 = "ttir.multiply"(%arg22, %385, %386) : (tensor<8x197x1024xf32>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc447)
    %388 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc448)
    %389 = "ttir.subtract"(%383, %387, %388) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc448)
    %390 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc449)
    %391 = "ttir.multiply"(%389, %389, %390) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc449)
    %392 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc450)
    %393 = "ttir.sum"(%391, %392) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc450)
    %394 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc451)
    %395 = "ttir.multiply"(%arg23, %393, %394) : (tensor<8x197x1xf32>, tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc451)
    %396 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc452)
    %397 = "ttir.add"(%395, %arg24, %396) : (tensor<8x197x1xbf16>, tensor<8x197x1xf32>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc452)
    %398 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc453)
    %399 = "ttir.sqrt"(%397, %398) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc453)
    %400 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc454)
    %401 = "ttir.reciprocal"(%399, %400) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc454)
    %402 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc455)
    %403 = "ttir.multiply"(%389, %401, %402) : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc455)
    %404 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc456)
    %405 = "ttir.multiply"(%403, %arg242, %404) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc456)
    %406 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc457)
    %407 = "ttir.add"(%405, %arg243, %406) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc457)
    %408 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1172)
    %409 = "ttir.reshape"(%407, %408) <{shape = [1576 : i32, 1024 : i32]}> : (tensor<8x197x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1172)
    %410 = ttir.empty() : tensor<1576x4096xbf16> loc(#loc1173)
    %411 = "ttir.matmul"(%409, %arg244, %410) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x4096xbf16>, tensor<1576x4096xbf16>) -> tensor<1576x4096xbf16> loc(#loc1173)
    %412 = ttir.empty() : tensor<8x197x4096xbf16> loc(#loc1174)
    %413 = "ttir.reshape"(%411, %412) <{shape = [8 : i32, 197 : i32, 4096 : i32]}> : (tensor<1576x4096xbf16>, tensor<8x197x4096xbf16>) -> tensor<8x197x4096xbf16> loc(#loc1174)
    %414 = ttir.empty() : tensor<8x197x4096xbf16> loc(#loc21)
    %415 = "ttir.add"(%413, %arg245, %414) : (tensor<8x197x4096xbf16>, tensor<4096xbf16>, tensor<8x197x4096xbf16>) -> tensor<8x197x4096xbf16> loc(#loc21)
    %416 = ttir.empty() : tensor<8x197x4096xbf16> loc(#loc1175)
    %417 = "ttir.gelu"(%415, %416) : (tensor<8x197x4096xbf16>, tensor<8x197x4096xbf16>) -> tensor<8x197x4096xbf16> loc(#loc1175)
    %418 = ttir.empty() : tensor<1576x4096xbf16> loc(#loc1176)
    %419 = "ttir.reshape"(%417, %418) <{shape = [1576 : i32, 4096 : i32]}> : (tensor<8x197x4096xbf16>, tensor<1576x4096xbf16>) -> tensor<1576x4096xbf16> loc(#loc1176)
    %420 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1177)
    %421 = "ttir.matmul"(%419, %arg246, %420) <{transpose_a = false, transpose_b = false}> : (tensor<1576x4096xbf16>, tensor<4096x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1177)
    %422 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1178)
    %423 = "ttir.reshape"(%421, %422) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1178)
    %424 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc22)
    %425 = "ttir.add"(%423, %arg247, %424) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc22)
    %426 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc461)
    %427 = "ttir.add"(%425, %383, %426) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc461)
    %428 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc462)
    %429 = "ttir.sum"(%427, %428) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc462)
    %430 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc463)
    %431 = "ttir.multiply"(%arg25, %429, %430) : (tensor<8x197x1024xf32>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc463)
    %432 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc464)
    %433 = "ttir.subtract"(%427, %431, %432) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc464)
    %434 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc465)
    %435 = "ttir.multiply"(%433, %433, %434) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc465)
    %436 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc466)
    %437 = "ttir.sum"(%435, %436) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc466)
    %438 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc467)
    %439 = "ttir.multiply"(%arg26, %437, %438) : (tensor<8x197x1xf32>, tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc467)
    %440 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc468)
    %441 = "ttir.add"(%439, %arg27, %440) : (tensor<8x197x1xbf16>, tensor<8x197x1xf32>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc468)
    %442 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc469)
    %443 = "ttir.sqrt"(%441, %442) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc469)
    %444 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc470)
    %445 = "ttir.reciprocal"(%443, %444) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc470)
    %446 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc471)
    %447 = "ttir.multiply"(%433, %445, %446) : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc471)
    %448 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc472)
    %449 = "ttir.multiply"(%447, %arg248, %448) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc472)
    %450 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc473)
    %451 = "ttir.add"(%449, %arg249, %450) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc473)
    %452 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1818)
    %453 = "ttir.reshape"(%451, %452) <{shape = [1576 : i32, 1024 : i32]}> : (tensor<8x197x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1818)
    %454 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1819)
    %455 = "ttir.matmul"(%453, %arg250, %454) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1819)
    %456 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1820)
    %457 = "ttir.reshape"(%455, %456) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1820)
    %458 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc23)
    %459 = "ttir.add"(%457, %arg251, %458) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc23)
    %460 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1180)
    %461 = "ttir.reshape"(%459, %460) <{shape = [8 : i32, 197 : i32, 16 : i32, 64 : i32]}> : (tensor<8x197x1024xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1180)
    %462 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1181)
    %463 = "ttir.transpose"(%461, %462) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x197x16x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1181)
    %464 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1182)
    %465 = "ttir.reshape"(%463, %464) <{shape = [128 : i32, 197 : i32, 64 : i32]}> : (tensor<8x16x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1182)
    %466 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1821)
    %467 = "ttir.matmul"(%453, %arg252, %466) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1821)
    %468 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1822)
    %469 = "ttir.reshape"(%467, %468) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1822)
    %470 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc24)
    %471 = "ttir.add"(%469, %arg253, %470) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc24)
    %472 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1184)
    %473 = "ttir.reshape"(%471, %472) <{shape = [8 : i32, 197 : i32, 16 : i32, 64 : i32]}> : (tensor<8x197x1024xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1184)
    %474 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1185)
    %475 = "ttir.transpose"(%473, %474) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x197x16x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1185)
    %476 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1186)
    %477 = "ttir.reshape"(%475, %476) <{shape = [128 : i32, 197 : i32, 64 : i32]}> : (tensor<8x16x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1186)
    %478 = ttir.empty() : tensor<128x64x197xbf16> loc(#loc25)
    %479 = "ttir.transpose"(%477, %478) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<128x197x64xbf16>, tensor<128x64x197xbf16>) -> tensor<128x64x197xbf16> loc(#loc25)
    %480 = ttir.empty() : tensor<128x197x197xbf16> loc(#loc1187)
    %481 = "ttir.matmul"(%465, %479, %480) <{transpose_a = false, transpose_b = false}> : (tensor<128x197x64xbf16>, tensor<128x64x197xbf16>, tensor<128x197x197xbf16>) -> tensor<128x197x197xbf16> loc(#loc1187)
    %482 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1188)
    %483 = "ttir.reshape"(%481, %482) <{shape = [8 : i32, 16 : i32, 197 : i32, 197 : i32]}> : (tensor<128x197x197xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1188)
    %484 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1189)
    %485 = "ttir.multiply"(%483, %arg28, %484) : (tensor<8x16x197x197xbf16>, tensor<1xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1189)
    %486 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1190)
    %487 = "ttir.add"(%485, %arg29, %486) : (tensor<8x16x197x197xbf16>, tensor<197x197xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1190)
    %488 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1191)
    %489 = "ttir.softmax"(%487, %488) <{dimension = -1 : si32}> : (tensor<8x16x197x197xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1191)
    %490 = ttir.empty() : tensor<128x197x197xbf16> loc(#loc1192)
    %491 = "ttir.reshape"(%489, %490) <{shape = [128 : i32, 197 : i32, 197 : i32]}> : (tensor<8x16x197x197xbf16>, tensor<128x197x197xbf16>) -> tensor<128x197x197xbf16> loc(#loc1192)
    %492 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1823)
    %493 = "ttir.matmul"(%453, %arg254, %492) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1823)
    %494 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1824)
    %495 = "ttir.reshape"(%493, %494) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1824)
    %496 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc26)
    %497 = "ttir.add"(%495, %arg255, %496) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc26)
    %498 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1194)
    %499 = "ttir.reshape"(%497, %498) <{shape = [8 : i32, 197 : i32, 16 : i32, 64 : i32]}> : (tensor<8x197x1024xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1194)
    %500 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1195)
    %501 = "ttir.transpose"(%499, %500) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x197x16x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1195)
    %502 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1196)
    %503 = "ttir.reshape"(%501, %502) <{shape = [128 : i32, 197 : i32, 64 : i32]}> : (tensor<8x16x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1196)
    %504 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1197)
    %505 = "ttir.matmul"(%491, %503, %504) <{transpose_a = false, transpose_b = false}> : (tensor<128x197x197xbf16>, tensor<128x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1197)
    %506 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1198)
    %507 = "ttir.reshape"(%505, %506) <{shape = [8 : i32, 16 : i32, 197 : i32, 64 : i32]}> : (tensor<128x197x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1198)
    %508 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1199)
    %509 = "ttir.transpose"(%507, %508) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x16x197x64xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1199)
    %510 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1825)
    %511 = "ttir.reshape"(%509, %510) <{shape = [1576 : i32, 1024 : i32]}> : (tensor<8x197x16x64xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1825)
    %512 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1826)
    %513 = "ttir.matmul"(%511, %arg256, %512) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1826)
    %514 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1827)
    %515 = "ttir.reshape"(%513, %514) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1827)
    %516 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc27)
    %517 = "ttir.add"(%515, %arg257, %516) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc27)
    %518 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc243)
    %519 = "ttir.add"(%517, %427, %518) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc243)
    %520 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc476)
    %521 = "ttir.sum"(%519, %520) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc476)
    %522 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc477)
    %523 = "ttir.multiply"(%arg30, %521, %522) : (tensor<8x197x1024xf32>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc477)
    %524 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc478)
    %525 = "ttir.subtract"(%519, %523, %524) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc478)
    %526 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc479)
    %527 = "ttir.multiply"(%525, %525, %526) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc479)
    %528 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc480)
    %529 = "ttir.sum"(%527, %528) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc480)
    %530 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc481)
    %531 = "ttir.multiply"(%arg31, %529, %530) : (tensor<8x197x1xf32>, tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc481)
    %532 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc482)
    %533 = "ttir.add"(%531, %arg32, %532) : (tensor<8x197x1xbf16>, tensor<8x197x1xf32>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc482)
    %534 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc483)
    %535 = "ttir.sqrt"(%533, %534) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc483)
    %536 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc484)
    %537 = "ttir.reciprocal"(%535, %536) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc484)
    %538 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc485)
    %539 = "ttir.multiply"(%525, %537, %538) : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc485)
    %540 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc486)
    %541 = "ttir.multiply"(%539, %arg258, %540) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc486)
    %542 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc487)
    %543 = "ttir.add"(%541, %arg259, %542) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc487)
    %544 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1201)
    %545 = "ttir.reshape"(%543, %544) <{shape = [1576 : i32, 1024 : i32]}> : (tensor<8x197x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1201)
    %546 = ttir.empty() : tensor<1576x4096xbf16> loc(#loc1202)
    %547 = "ttir.matmul"(%545, %arg260, %546) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x4096xbf16>, tensor<1576x4096xbf16>) -> tensor<1576x4096xbf16> loc(#loc1202)
    %548 = ttir.empty() : tensor<8x197x4096xbf16> loc(#loc1203)
    %549 = "ttir.reshape"(%547, %548) <{shape = [8 : i32, 197 : i32, 4096 : i32]}> : (tensor<1576x4096xbf16>, tensor<8x197x4096xbf16>) -> tensor<8x197x4096xbf16> loc(#loc1203)
    %550 = ttir.empty() : tensor<8x197x4096xbf16> loc(#loc28)
    %551 = "ttir.add"(%549, %arg261, %550) : (tensor<8x197x4096xbf16>, tensor<4096xbf16>, tensor<8x197x4096xbf16>) -> tensor<8x197x4096xbf16> loc(#loc28)
    %552 = ttir.empty() : tensor<8x197x4096xbf16> loc(#loc1204)
    %553 = "ttir.gelu"(%551, %552) : (tensor<8x197x4096xbf16>, tensor<8x197x4096xbf16>) -> tensor<8x197x4096xbf16> loc(#loc1204)
    %554 = ttir.empty() : tensor<1576x4096xbf16> loc(#loc1205)
    %555 = "ttir.reshape"(%553, %554) <{shape = [1576 : i32, 4096 : i32]}> : (tensor<8x197x4096xbf16>, tensor<1576x4096xbf16>) -> tensor<1576x4096xbf16> loc(#loc1205)
    %556 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1206)
    %557 = "ttir.matmul"(%555, %arg262, %556) <{transpose_a = false, transpose_b = false}> : (tensor<1576x4096xbf16>, tensor<4096x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1206)
    %558 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1207)
    %559 = "ttir.reshape"(%557, %558) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1207)
    %560 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc29)
    %561 = "ttir.add"(%559, %arg263, %560) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc29)
    %562 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc491)
    %563 = "ttir.add"(%561, %519, %562) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc491)
    %564 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc492)
    %565 = "ttir.sum"(%563, %564) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc492)
    %566 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc493)
    %567 = "ttir.multiply"(%arg33, %565, %566) : (tensor<8x197x1024xf32>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc493)
    %568 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc494)
    %569 = "ttir.subtract"(%563, %567, %568) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc494)
    %570 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc495)
    %571 = "ttir.multiply"(%569, %569, %570) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc495)
    %572 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc496)
    %573 = "ttir.sum"(%571, %572) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc496)
    %574 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc497)
    %575 = "ttir.multiply"(%arg34, %573, %574) : (tensor<8x197x1xf32>, tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc497)
    %576 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc498)
    %577 = "ttir.add"(%575, %arg35, %576) : (tensor<8x197x1xbf16>, tensor<8x197x1xf32>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc498)
    %578 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc499)
    %579 = "ttir.sqrt"(%577, %578) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc499)
    %580 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc500)
    %581 = "ttir.reciprocal"(%579, %580) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc500)
    %582 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc501)
    %583 = "ttir.multiply"(%569, %581, %582) : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc501)
    %584 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc502)
    %585 = "ttir.multiply"(%583, %arg264, %584) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc502)
    %586 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc503)
    %587 = "ttir.add"(%585, %arg265, %586) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc503)
    %588 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1828)
    %589 = "ttir.reshape"(%587, %588) <{shape = [1576 : i32, 1024 : i32]}> : (tensor<8x197x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1828)
    %590 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1829)
    %591 = "ttir.matmul"(%589, %arg266, %590) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1829)
    %592 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1830)
    %593 = "ttir.reshape"(%591, %592) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1830)
    %594 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc30)
    %595 = "ttir.add"(%593, %arg267, %594) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc30)
    %596 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1209)
    %597 = "ttir.reshape"(%595, %596) <{shape = [8 : i32, 197 : i32, 16 : i32, 64 : i32]}> : (tensor<8x197x1024xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1209)
    %598 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1210)
    %599 = "ttir.transpose"(%597, %598) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x197x16x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1210)
    %600 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1211)
    %601 = "ttir.reshape"(%599, %600) <{shape = [128 : i32, 197 : i32, 64 : i32]}> : (tensor<8x16x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1211)
    %602 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1831)
    %603 = "ttir.matmul"(%589, %arg268, %602) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1831)
    %604 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1832)
    %605 = "ttir.reshape"(%603, %604) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1832)
    %606 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc31)
    %607 = "ttir.add"(%605, %arg269, %606) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc31)
    %608 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1213)
    %609 = "ttir.reshape"(%607, %608) <{shape = [8 : i32, 197 : i32, 16 : i32, 64 : i32]}> : (tensor<8x197x1024xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1213)
    %610 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1214)
    %611 = "ttir.transpose"(%609, %610) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x197x16x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1214)
    %612 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1215)
    %613 = "ttir.reshape"(%611, %612) <{shape = [128 : i32, 197 : i32, 64 : i32]}> : (tensor<8x16x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1215)
    %614 = ttir.empty() : tensor<128x64x197xbf16> loc(#loc32)
    %615 = "ttir.transpose"(%613, %614) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<128x197x64xbf16>, tensor<128x64x197xbf16>) -> tensor<128x64x197xbf16> loc(#loc32)
    %616 = ttir.empty() : tensor<128x197x197xbf16> loc(#loc1216)
    %617 = "ttir.matmul"(%601, %615, %616) <{transpose_a = false, transpose_b = false}> : (tensor<128x197x64xbf16>, tensor<128x64x197xbf16>, tensor<128x197x197xbf16>) -> tensor<128x197x197xbf16> loc(#loc1216)
    %618 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1217)
    %619 = "ttir.reshape"(%617, %618) <{shape = [8 : i32, 16 : i32, 197 : i32, 197 : i32]}> : (tensor<128x197x197xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1217)
    %620 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1218)
    %621 = "ttir.multiply"(%619, %arg36, %620) : (tensor<8x16x197x197xbf16>, tensor<1xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1218)
    %622 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1219)
    %623 = "ttir.add"(%621, %arg37, %622) : (tensor<8x16x197x197xbf16>, tensor<197x197xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1219)
    %624 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1220)
    %625 = "ttir.softmax"(%623, %624) <{dimension = -1 : si32}> : (tensor<8x16x197x197xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1220)
    %626 = ttir.empty() : tensor<128x197x197xbf16> loc(#loc1221)
    %627 = "ttir.reshape"(%625, %626) <{shape = [128 : i32, 197 : i32, 197 : i32]}> : (tensor<8x16x197x197xbf16>, tensor<128x197x197xbf16>) -> tensor<128x197x197xbf16> loc(#loc1221)
    %628 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1833)
    %629 = "ttir.matmul"(%589, %arg270, %628) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1833)
    %630 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1834)
    %631 = "ttir.reshape"(%629, %630) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1834)
    %632 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc33)
    %633 = "ttir.add"(%631, %arg271, %632) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc33)
    %634 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1223)
    %635 = "ttir.reshape"(%633, %634) <{shape = [8 : i32, 197 : i32, 16 : i32, 64 : i32]}> : (tensor<8x197x1024xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1223)
    %636 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1224)
    %637 = "ttir.transpose"(%635, %636) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x197x16x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1224)
    %638 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1225)
    %639 = "ttir.reshape"(%637, %638) <{shape = [128 : i32, 197 : i32, 64 : i32]}> : (tensor<8x16x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1225)
    %640 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1226)
    %641 = "ttir.matmul"(%627, %639, %640) <{transpose_a = false, transpose_b = false}> : (tensor<128x197x197xbf16>, tensor<128x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1226)
    %642 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1227)
    %643 = "ttir.reshape"(%641, %642) <{shape = [8 : i32, 16 : i32, 197 : i32, 64 : i32]}> : (tensor<128x197x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1227)
    %644 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1228)
    %645 = "ttir.transpose"(%643, %644) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x16x197x64xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1228)
    %646 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1835)
    %647 = "ttir.reshape"(%645, %646) <{shape = [1576 : i32, 1024 : i32]}> : (tensor<8x197x16x64xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1835)
    %648 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1836)
    %649 = "ttir.matmul"(%647, %arg272, %648) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1836)
    %650 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1837)
    %651 = "ttir.reshape"(%649, %650) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1837)
    %652 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc34)
    %653 = "ttir.add"(%651, %arg273, %652) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc34)
    %654 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc249)
    %655 = "ttir.add"(%653, %563, %654) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc249)
    %656 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc506)
    %657 = "ttir.sum"(%655, %656) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc506)
    %658 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc507)
    %659 = "ttir.multiply"(%arg38, %657, %658) : (tensor<8x197x1024xf32>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc507)
    %660 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc508)
    %661 = "ttir.subtract"(%655, %659, %660) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc508)
    %662 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc509)
    %663 = "ttir.multiply"(%661, %661, %662) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc509)
    %664 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc510)
    %665 = "ttir.sum"(%663, %664) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc510)
    %666 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc511)
    %667 = "ttir.multiply"(%arg39, %665, %666) : (tensor<8x197x1xf32>, tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc511)
    %668 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc512)
    %669 = "ttir.add"(%667, %arg40, %668) : (tensor<8x197x1xbf16>, tensor<8x197x1xf32>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc512)
    %670 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc513)
    %671 = "ttir.sqrt"(%669, %670) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc513)
    %672 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc514)
    %673 = "ttir.reciprocal"(%671, %672) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc514)
    %674 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc515)
    %675 = "ttir.multiply"(%661, %673, %674) : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc515)
    %676 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc516)
    %677 = "ttir.multiply"(%675, %arg274, %676) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc516)
    %678 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc517)
    %679 = "ttir.add"(%677, %arg275, %678) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc517)
    %680 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1230)
    %681 = "ttir.reshape"(%679, %680) <{shape = [1576 : i32, 1024 : i32]}> : (tensor<8x197x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1230)
    %682 = ttir.empty() : tensor<1576x4096xbf16> loc(#loc1231)
    %683 = "ttir.matmul"(%681, %arg276, %682) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x4096xbf16>, tensor<1576x4096xbf16>) -> tensor<1576x4096xbf16> loc(#loc1231)
    %684 = ttir.empty() : tensor<8x197x4096xbf16> loc(#loc1232)
    %685 = "ttir.reshape"(%683, %684) <{shape = [8 : i32, 197 : i32, 4096 : i32]}> : (tensor<1576x4096xbf16>, tensor<8x197x4096xbf16>) -> tensor<8x197x4096xbf16> loc(#loc1232)
    %686 = ttir.empty() : tensor<8x197x4096xbf16> loc(#loc35)
    %687 = "ttir.add"(%685, %arg277, %686) : (tensor<8x197x4096xbf16>, tensor<4096xbf16>, tensor<8x197x4096xbf16>) -> tensor<8x197x4096xbf16> loc(#loc35)
    %688 = ttir.empty() : tensor<8x197x4096xbf16> loc(#loc1233)
    %689 = "ttir.gelu"(%687, %688) : (tensor<8x197x4096xbf16>, tensor<8x197x4096xbf16>) -> tensor<8x197x4096xbf16> loc(#loc1233)
    %690 = ttir.empty() : tensor<1576x4096xbf16> loc(#loc1234)
    %691 = "ttir.reshape"(%689, %690) <{shape = [1576 : i32, 4096 : i32]}> : (tensor<8x197x4096xbf16>, tensor<1576x4096xbf16>) -> tensor<1576x4096xbf16> loc(#loc1234)
    %692 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1235)
    %693 = "ttir.matmul"(%691, %arg278, %692) <{transpose_a = false, transpose_b = false}> : (tensor<1576x4096xbf16>, tensor<4096x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1235)
    %694 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1236)
    %695 = "ttir.reshape"(%693, %694) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1236)
    %696 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc36)
    %697 = "ttir.add"(%695, %arg279, %696) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc36)
    %698 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc521)
    %699 = "ttir.add"(%697, %655, %698) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc521)
    %700 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc522)
    %701 = "ttir.sum"(%699, %700) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc522)
    %702 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc523)
    %703 = "ttir.multiply"(%arg41, %701, %702) : (tensor<8x197x1024xf32>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc523)
    %704 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc524)
    %705 = "ttir.subtract"(%699, %703, %704) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc524)
    %706 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc525)
    %707 = "ttir.multiply"(%705, %705, %706) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc525)
    %708 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc526)
    %709 = "ttir.sum"(%707, %708) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc526)
    %710 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc527)
    %711 = "ttir.multiply"(%arg42, %709, %710) : (tensor<8x197x1xf32>, tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc527)
    %712 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc528)
    %713 = "ttir.add"(%711, %arg43, %712) : (tensor<8x197x1xbf16>, tensor<8x197x1xf32>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc528)
    %714 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc529)
    %715 = "ttir.sqrt"(%713, %714) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc529)
    %716 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc530)
    %717 = "ttir.reciprocal"(%715, %716) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc530)
    %718 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc531)
    %719 = "ttir.multiply"(%705, %717, %718) : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc531)
    %720 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc532)
    %721 = "ttir.multiply"(%719, %arg280, %720) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc532)
    %722 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc533)
    %723 = "ttir.add"(%721, %arg281, %722) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc533)
    %724 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1838)
    %725 = "ttir.reshape"(%723, %724) <{shape = [1576 : i32, 1024 : i32]}> : (tensor<8x197x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1838)
    %726 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1839)
    %727 = "ttir.matmul"(%725, %arg282, %726) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1839)
    %728 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1840)
    %729 = "ttir.reshape"(%727, %728) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1840)
    %730 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc37)
    %731 = "ttir.add"(%729, %arg283, %730) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc37)
    %732 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1238)
    %733 = "ttir.reshape"(%731, %732) <{shape = [8 : i32, 197 : i32, 16 : i32, 64 : i32]}> : (tensor<8x197x1024xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1238)
    %734 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1239)
    %735 = "ttir.transpose"(%733, %734) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x197x16x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1239)
    %736 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1240)
    %737 = "ttir.reshape"(%735, %736) <{shape = [128 : i32, 197 : i32, 64 : i32]}> : (tensor<8x16x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1240)
    %738 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1841)
    %739 = "ttir.matmul"(%725, %arg284, %738) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1841)
    %740 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1842)
    %741 = "ttir.reshape"(%739, %740) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1842)
    %742 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc38)
    %743 = "ttir.add"(%741, %arg285, %742) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc38)
    %744 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1242)
    %745 = "ttir.reshape"(%743, %744) <{shape = [8 : i32, 197 : i32, 16 : i32, 64 : i32]}> : (tensor<8x197x1024xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1242)
    %746 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1243)
    %747 = "ttir.transpose"(%745, %746) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x197x16x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1243)
    %748 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1244)
    %749 = "ttir.reshape"(%747, %748) <{shape = [128 : i32, 197 : i32, 64 : i32]}> : (tensor<8x16x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1244)
    %750 = ttir.empty() : tensor<128x64x197xbf16> loc(#loc39)
    %751 = "ttir.transpose"(%749, %750) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<128x197x64xbf16>, tensor<128x64x197xbf16>) -> tensor<128x64x197xbf16> loc(#loc39)
    %752 = ttir.empty() : tensor<128x197x197xbf16> loc(#loc1245)
    %753 = "ttir.matmul"(%737, %751, %752) <{transpose_a = false, transpose_b = false}> : (tensor<128x197x64xbf16>, tensor<128x64x197xbf16>, tensor<128x197x197xbf16>) -> tensor<128x197x197xbf16> loc(#loc1245)
    %754 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1246)
    %755 = "ttir.reshape"(%753, %754) <{shape = [8 : i32, 16 : i32, 197 : i32, 197 : i32]}> : (tensor<128x197x197xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1246)
    %756 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1247)
    %757 = "ttir.multiply"(%755, %arg44, %756) : (tensor<8x16x197x197xbf16>, tensor<1xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1247)
    %758 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1248)
    %759 = "ttir.add"(%757, %arg45, %758) : (tensor<8x16x197x197xbf16>, tensor<197x197xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1248)
    %760 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1249)
    %761 = "ttir.softmax"(%759, %760) <{dimension = -1 : si32}> : (tensor<8x16x197x197xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1249)
    %762 = ttir.empty() : tensor<128x197x197xbf16> loc(#loc1250)
    %763 = "ttir.reshape"(%761, %762) <{shape = [128 : i32, 197 : i32, 197 : i32]}> : (tensor<8x16x197x197xbf16>, tensor<128x197x197xbf16>) -> tensor<128x197x197xbf16> loc(#loc1250)
    %764 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1843)
    %765 = "ttir.matmul"(%725, %arg286, %764) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1843)
    %766 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1844)
    %767 = "ttir.reshape"(%765, %766) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1844)
    %768 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc40)
    %769 = "ttir.add"(%767, %arg287, %768) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc40)
    %770 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1252)
    %771 = "ttir.reshape"(%769, %770) <{shape = [8 : i32, 197 : i32, 16 : i32, 64 : i32]}> : (tensor<8x197x1024xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1252)
    %772 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1253)
    %773 = "ttir.transpose"(%771, %772) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x197x16x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1253)
    %774 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1254)
    %775 = "ttir.reshape"(%773, %774) <{shape = [128 : i32, 197 : i32, 64 : i32]}> : (tensor<8x16x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1254)
    %776 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1255)
    %777 = "ttir.matmul"(%763, %775, %776) <{transpose_a = false, transpose_b = false}> : (tensor<128x197x197xbf16>, tensor<128x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1255)
    %778 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1256)
    %779 = "ttir.reshape"(%777, %778) <{shape = [8 : i32, 16 : i32, 197 : i32, 64 : i32]}> : (tensor<128x197x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1256)
    %780 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1257)
    %781 = "ttir.transpose"(%779, %780) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x16x197x64xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1257)
    %782 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1845)
    %783 = "ttir.reshape"(%781, %782) <{shape = [1576 : i32, 1024 : i32]}> : (tensor<8x197x16x64xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1845)
    %784 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1846)
    %785 = "ttir.matmul"(%783, %arg288, %784) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1846)
    %786 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1847)
    %787 = "ttir.reshape"(%785, %786) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1847)
    %788 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc41)
    %789 = "ttir.add"(%787, %arg289, %788) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc41)
    %790 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc255)
    %791 = "ttir.add"(%789, %699, %790) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc255)
    %792 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc536)
    %793 = "ttir.sum"(%791, %792) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc536)
    %794 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc537)
    %795 = "ttir.multiply"(%arg46, %793, %794) : (tensor<8x197x1024xf32>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc537)
    %796 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc538)
    %797 = "ttir.subtract"(%791, %795, %796) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc538)
    %798 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc539)
    %799 = "ttir.multiply"(%797, %797, %798) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc539)
    %800 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc540)
    %801 = "ttir.sum"(%799, %800) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc540)
    %802 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc541)
    %803 = "ttir.multiply"(%arg47, %801, %802) : (tensor<8x197x1xf32>, tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc541)
    %804 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc542)
    %805 = "ttir.add"(%803, %arg48, %804) : (tensor<8x197x1xbf16>, tensor<8x197x1xf32>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc542)
    %806 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc543)
    %807 = "ttir.sqrt"(%805, %806) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc543)
    %808 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc544)
    %809 = "ttir.reciprocal"(%807, %808) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc544)
    %810 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc545)
    %811 = "ttir.multiply"(%797, %809, %810) : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc545)
    %812 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc546)
    %813 = "ttir.multiply"(%811, %arg290, %812) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc546)
    %814 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc547)
    %815 = "ttir.add"(%813, %arg291, %814) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc547)
    %816 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1259)
    %817 = "ttir.reshape"(%815, %816) <{shape = [1576 : i32, 1024 : i32]}> : (tensor<8x197x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1259)
    %818 = ttir.empty() : tensor<1576x4096xbf16> loc(#loc1260)
    %819 = "ttir.matmul"(%817, %arg292, %818) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x4096xbf16>, tensor<1576x4096xbf16>) -> tensor<1576x4096xbf16> loc(#loc1260)
    %820 = ttir.empty() : tensor<8x197x4096xbf16> loc(#loc1261)
    %821 = "ttir.reshape"(%819, %820) <{shape = [8 : i32, 197 : i32, 4096 : i32]}> : (tensor<1576x4096xbf16>, tensor<8x197x4096xbf16>) -> tensor<8x197x4096xbf16> loc(#loc1261)
    %822 = ttir.empty() : tensor<8x197x4096xbf16> loc(#loc42)
    %823 = "ttir.add"(%821, %arg293, %822) : (tensor<8x197x4096xbf16>, tensor<4096xbf16>, tensor<8x197x4096xbf16>) -> tensor<8x197x4096xbf16> loc(#loc42)
    %824 = ttir.empty() : tensor<8x197x4096xbf16> loc(#loc1262)
    %825 = "ttir.gelu"(%823, %824) : (tensor<8x197x4096xbf16>, tensor<8x197x4096xbf16>) -> tensor<8x197x4096xbf16> loc(#loc1262)
    %826 = ttir.empty() : tensor<1576x4096xbf16> loc(#loc1263)
    %827 = "ttir.reshape"(%825, %826) <{shape = [1576 : i32, 4096 : i32]}> : (tensor<8x197x4096xbf16>, tensor<1576x4096xbf16>) -> tensor<1576x4096xbf16> loc(#loc1263)
    %828 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1264)
    %829 = "ttir.matmul"(%827, %arg294, %828) <{transpose_a = false, transpose_b = false}> : (tensor<1576x4096xbf16>, tensor<4096x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1264)
    %830 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1265)
    %831 = "ttir.reshape"(%829, %830) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1265)
    %832 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc43)
    %833 = "ttir.add"(%831, %arg295, %832) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc43)
    %834 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc551)
    %835 = "ttir.add"(%833, %791, %834) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc551)
    %836 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc552)
    %837 = "ttir.sum"(%835, %836) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc552)
    %838 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc553)
    %839 = "ttir.multiply"(%arg49, %837, %838) : (tensor<8x197x1024xf32>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc553)
    %840 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc554)
    %841 = "ttir.subtract"(%835, %839, %840) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc554)
    %842 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc555)
    %843 = "ttir.multiply"(%841, %841, %842) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc555)
    %844 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc556)
    %845 = "ttir.sum"(%843, %844) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc556)
    %846 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc557)
    %847 = "ttir.multiply"(%arg50, %845, %846) : (tensor<8x197x1xf32>, tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc557)
    %848 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc558)
    %849 = "ttir.add"(%847, %arg51, %848) : (tensor<8x197x1xbf16>, tensor<8x197x1xf32>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc558)
    %850 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc559)
    %851 = "ttir.sqrt"(%849, %850) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc559)
    %852 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc560)
    %853 = "ttir.reciprocal"(%851, %852) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc560)
    %854 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc561)
    %855 = "ttir.multiply"(%841, %853, %854) : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc561)
    %856 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc562)
    %857 = "ttir.multiply"(%855, %arg296, %856) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc562)
    %858 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc563)
    %859 = "ttir.add"(%857, %arg297, %858) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc563)
    %860 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1848)
    %861 = "ttir.reshape"(%859, %860) <{shape = [1576 : i32, 1024 : i32]}> : (tensor<8x197x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1848)
    %862 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1849)
    %863 = "ttir.matmul"(%861, %arg298, %862) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1849)
    %864 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1850)
    %865 = "ttir.reshape"(%863, %864) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1850)
    %866 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc44)
    %867 = "ttir.add"(%865, %arg299, %866) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc44)
    %868 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1267)
    %869 = "ttir.reshape"(%867, %868) <{shape = [8 : i32, 197 : i32, 16 : i32, 64 : i32]}> : (tensor<8x197x1024xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1267)
    %870 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1268)
    %871 = "ttir.transpose"(%869, %870) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x197x16x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1268)
    %872 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1269)
    %873 = "ttir.reshape"(%871, %872) <{shape = [128 : i32, 197 : i32, 64 : i32]}> : (tensor<8x16x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1269)
    %874 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1851)
    %875 = "ttir.matmul"(%861, %arg300, %874) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1851)
    %876 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1852)
    %877 = "ttir.reshape"(%875, %876) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1852)
    %878 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc45)
    %879 = "ttir.add"(%877, %arg301, %878) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc45)
    %880 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1271)
    %881 = "ttir.reshape"(%879, %880) <{shape = [8 : i32, 197 : i32, 16 : i32, 64 : i32]}> : (tensor<8x197x1024xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1271)
    %882 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1272)
    %883 = "ttir.transpose"(%881, %882) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x197x16x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1272)
    %884 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1273)
    %885 = "ttir.reshape"(%883, %884) <{shape = [128 : i32, 197 : i32, 64 : i32]}> : (tensor<8x16x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1273)
    %886 = ttir.empty() : tensor<128x64x197xbf16> loc(#loc46)
    %887 = "ttir.transpose"(%885, %886) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<128x197x64xbf16>, tensor<128x64x197xbf16>) -> tensor<128x64x197xbf16> loc(#loc46)
    %888 = ttir.empty() : tensor<128x197x197xbf16> loc(#loc1274)
    %889 = "ttir.matmul"(%873, %887, %888) <{transpose_a = false, transpose_b = false}> : (tensor<128x197x64xbf16>, tensor<128x64x197xbf16>, tensor<128x197x197xbf16>) -> tensor<128x197x197xbf16> loc(#loc1274)
    %890 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1275)
    %891 = "ttir.reshape"(%889, %890) <{shape = [8 : i32, 16 : i32, 197 : i32, 197 : i32]}> : (tensor<128x197x197xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1275)
    %892 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1276)
    %893 = "ttir.multiply"(%891, %arg52, %892) : (tensor<8x16x197x197xbf16>, tensor<1xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1276)
    %894 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1277)
    %895 = "ttir.add"(%893, %arg53, %894) : (tensor<8x16x197x197xbf16>, tensor<197x197xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1277)
    %896 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1278)
    %897 = "ttir.softmax"(%895, %896) <{dimension = -1 : si32}> : (tensor<8x16x197x197xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1278)
    %898 = ttir.empty() : tensor<128x197x197xbf16> loc(#loc1279)
    %899 = "ttir.reshape"(%897, %898) <{shape = [128 : i32, 197 : i32, 197 : i32]}> : (tensor<8x16x197x197xbf16>, tensor<128x197x197xbf16>) -> tensor<128x197x197xbf16> loc(#loc1279)
    %900 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1853)
    %901 = "ttir.matmul"(%861, %arg302, %900) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1853)
    %902 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1854)
    %903 = "ttir.reshape"(%901, %902) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1854)
    %904 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc47)
    %905 = "ttir.add"(%903, %arg303, %904) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc47)
    %906 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1281)
    %907 = "ttir.reshape"(%905, %906) <{shape = [8 : i32, 197 : i32, 16 : i32, 64 : i32]}> : (tensor<8x197x1024xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1281)
    %908 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1282)
    %909 = "ttir.transpose"(%907, %908) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x197x16x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1282)
    %910 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1283)
    %911 = "ttir.reshape"(%909, %910) <{shape = [128 : i32, 197 : i32, 64 : i32]}> : (tensor<8x16x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1283)
    %912 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1284)
    %913 = "ttir.matmul"(%899, %911, %912) <{transpose_a = false, transpose_b = false}> : (tensor<128x197x197xbf16>, tensor<128x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1284)
    %914 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1285)
    %915 = "ttir.reshape"(%913, %914) <{shape = [8 : i32, 16 : i32, 197 : i32, 64 : i32]}> : (tensor<128x197x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1285)
    %916 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1286)
    %917 = "ttir.transpose"(%915, %916) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x16x197x64xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1286)
    %918 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1855)
    %919 = "ttir.reshape"(%917, %918) <{shape = [1576 : i32, 1024 : i32]}> : (tensor<8x197x16x64xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1855)
    %920 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1856)
    %921 = "ttir.matmul"(%919, %arg304, %920) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1856)
    %922 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1857)
    %923 = "ttir.reshape"(%921, %922) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1857)
    %924 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc48)
    %925 = "ttir.add"(%923, %arg305, %924) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc48)
    %926 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc261)
    %927 = "ttir.add"(%925, %835, %926) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc261)
    %928 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc566)
    %929 = "ttir.sum"(%927, %928) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc566)
    %930 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc567)
    %931 = "ttir.multiply"(%arg54, %929, %930) : (tensor<8x197x1024xf32>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc567)
    %932 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc568)
    %933 = "ttir.subtract"(%927, %931, %932) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc568)
    %934 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc569)
    %935 = "ttir.multiply"(%933, %933, %934) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc569)
    %936 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc570)
    %937 = "ttir.sum"(%935, %936) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc570)
    %938 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc571)
    %939 = "ttir.multiply"(%arg55, %937, %938) : (tensor<8x197x1xf32>, tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc571)
    %940 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc572)
    %941 = "ttir.add"(%939, %arg56, %940) : (tensor<8x197x1xbf16>, tensor<8x197x1xf32>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc572)
    %942 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc573)
    %943 = "ttir.sqrt"(%941, %942) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc573)
    %944 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc574)
    %945 = "ttir.reciprocal"(%943, %944) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc574)
    %946 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc575)
    %947 = "ttir.multiply"(%933, %945, %946) : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc575)
    %948 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc576)
    %949 = "ttir.multiply"(%947, %arg306, %948) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc576)
    %950 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc577)
    %951 = "ttir.add"(%949, %arg307, %950) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc577)
    %952 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1288)
    %953 = "ttir.reshape"(%951, %952) <{shape = [1576 : i32, 1024 : i32]}> : (tensor<8x197x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1288)
    %954 = ttir.empty() : tensor<1576x4096xbf16> loc(#loc1289)
    %955 = "ttir.matmul"(%953, %arg308, %954) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x4096xbf16>, tensor<1576x4096xbf16>) -> tensor<1576x4096xbf16> loc(#loc1289)
    %956 = ttir.empty() : tensor<8x197x4096xbf16> loc(#loc1290)
    %957 = "ttir.reshape"(%955, %956) <{shape = [8 : i32, 197 : i32, 4096 : i32]}> : (tensor<1576x4096xbf16>, tensor<8x197x4096xbf16>) -> tensor<8x197x4096xbf16> loc(#loc1290)
    %958 = ttir.empty() : tensor<8x197x4096xbf16> loc(#loc49)
    %959 = "ttir.add"(%957, %arg309, %958) : (tensor<8x197x4096xbf16>, tensor<4096xbf16>, tensor<8x197x4096xbf16>) -> tensor<8x197x4096xbf16> loc(#loc49)
    %960 = ttir.empty() : tensor<8x197x4096xbf16> loc(#loc1291)
    %961 = "ttir.gelu"(%959, %960) : (tensor<8x197x4096xbf16>, tensor<8x197x4096xbf16>) -> tensor<8x197x4096xbf16> loc(#loc1291)
    %962 = ttir.empty() : tensor<1576x4096xbf16> loc(#loc1292)
    %963 = "ttir.reshape"(%961, %962) <{shape = [1576 : i32, 4096 : i32]}> : (tensor<8x197x4096xbf16>, tensor<1576x4096xbf16>) -> tensor<1576x4096xbf16> loc(#loc1292)
    %964 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1293)
    %965 = "ttir.matmul"(%963, %arg310, %964) <{transpose_a = false, transpose_b = false}> : (tensor<1576x4096xbf16>, tensor<4096x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1293)
    %966 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1294)
    %967 = "ttir.reshape"(%965, %966) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1294)
    %968 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc50)
    %969 = "ttir.add"(%967, %arg311, %968) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc50)
    %970 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc581)
    %971 = "ttir.add"(%969, %927, %970) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc581)
    %972 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc582)
    %973 = "ttir.sum"(%971, %972) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc582)
    %974 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc583)
    %975 = "ttir.multiply"(%arg57, %973, %974) : (tensor<8x197x1024xf32>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc583)
    %976 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc584)
    %977 = "ttir.subtract"(%971, %975, %976) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc584)
    %978 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc585)
    %979 = "ttir.multiply"(%977, %977, %978) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc585)
    %980 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc586)
    %981 = "ttir.sum"(%979, %980) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc586)
    %982 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc587)
    %983 = "ttir.multiply"(%arg58, %981, %982) : (tensor<8x197x1xf32>, tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc587)
    %984 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc588)
    %985 = "ttir.add"(%983, %arg59, %984) : (tensor<8x197x1xbf16>, tensor<8x197x1xf32>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc588)
    %986 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc589)
    %987 = "ttir.sqrt"(%985, %986) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc589)
    %988 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc590)
    %989 = "ttir.reciprocal"(%987, %988) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc590)
    %990 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc591)
    %991 = "ttir.multiply"(%977, %989, %990) : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc591)
    %992 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc592)
    %993 = "ttir.multiply"(%991, %arg312, %992) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc592)
    %994 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc593)
    %995 = "ttir.add"(%993, %arg313, %994) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc593)
    %996 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1858)
    %997 = "ttir.reshape"(%995, %996) <{shape = [1576 : i32, 1024 : i32]}> : (tensor<8x197x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1858)
    %998 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1859)
    %999 = "ttir.matmul"(%997, %arg314, %998) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1859)
    %1000 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1860)
    %1001 = "ttir.reshape"(%999, %1000) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1860)
    %1002 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc51)
    %1003 = "ttir.add"(%1001, %arg315, %1002) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc51)
    %1004 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1296)
    %1005 = "ttir.reshape"(%1003, %1004) <{shape = [8 : i32, 197 : i32, 16 : i32, 64 : i32]}> : (tensor<8x197x1024xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1296)
    %1006 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1297)
    %1007 = "ttir.transpose"(%1005, %1006) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x197x16x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1297)
    %1008 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1298)
    %1009 = "ttir.reshape"(%1007, %1008) <{shape = [128 : i32, 197 : i32, 64 : i32]}> : (tensor<8x16x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1298)
    %1010 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1861)
    %1011 = "ttir.matmul"(%997, %arg316, %1010) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1861)
    %1012 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1862)
    %1013 = "ttir.reshape"(%1011, %1012) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1862)
    %1014 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc52)
    %1015 = "ttir.add"(%1013, %arg317, %1014) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc52)
    %1016 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1300)
    %1017 = "ttir.reshape"(%1015, %1016) <{shape = [8 : i32, 197 : i32, 16 : i32, 64 : i32]}> : (tensor<8x197x1024xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1300)
    %1018 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1301)
    %1019 = "ttir.transpose"(%1017, %1018) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x197x16x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1301)
    %1020 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1302)
    %1021 = "ttir.reshape"(%1019, %1020) <{shape = [128 : i32, 197 : i32, 64 : i32]}> : (tensor<8x16x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1302)
    %1022 = ttir.empty() : tensor<128x64x197xbf16> loc(#loc53)
    %1023 = "ttir.transpose"(%1021, %1022) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<128x197x64xbf16>, tensor<128x64x197xbf16>) -> tensor<128x64x197xbf16> loc(#loc53)
    %1024 = ttir.empty() : tensor<128x197x197xbf16> loc(#loc1303)
    %1025 = "ttir.matmul"(%1009, %1023, %1024) <{transpose_a = false, transpose_b = false}> : (tensor<128x197x64xbf16>, tensor<128x64x197xbf16>, tensor<128x197x197xbf16>) -> tensor<128x197x197xbf16> loc(#loc1303)
    %1026 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1304)
    %1027 = "ttir.reshape"(%1025, %1026) <{shape = [8 : i32, 16 : i32, 197 : i32, 197 : i32]}> : (tensor<128x197x197xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1304)
    %1028 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1305)
    %1029 = "ttir.multiply"(%1027, %arg60, %1028) : (tensor<8x16x197x197xbf16>, tensor<1xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1305)
    %1030 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1306)
    %1031 = "ttir.add"(%1029, %arg61, %1030) : (tensor<8x16x197x197xbf16>, tensor<197x197xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1306)
    %1032 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1307)
    %1033 = "ttir.softmax"(%1031, %1032) <{dimension = -1 : si32}> : (tensor<8x16x197x197xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1307)
    %1034 = ttir.empty() : tensor<128x197x197xbf16> loc(#loc1308)
    %1035 = "ttir.reshape"(%1033, %1034) <{shape = [128 : i32, 197 : i32, 197 : i32]}> : (tensor<8x16x197x197xbf16>, tensor<128x197x197xbf16>) -> tensor<128x197x197xbf16> loc(#loc1308)
    %1036 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1863)
    %1037 = "ttir.matmul"(%997, %arg318, %1036) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1863)
    %1038 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1864)
    %1039 = "ttir.reshape"(%1037, %1038) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1864)
    %1040 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc54)
    %1041 = "ttir.add"(%1039, %arg319, %1040) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc54)
    %1042 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1310)
    %1043 = "ttir.reshape"(%1041, %1042) <{shape = [8 : i32, 197 : i32, 16 : i32, 64 : i32]}> : (tensor<8x197x1024xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1310)
    %1044 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1311)
    %1045 = "ttir.transpose"(%1043, %1044) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x197x16x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1311)
    %1046 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1312)
    %1047 = "ttir.reshape"(%1045, %1046) <{shape = [128 : i32, 197 : i32, 64 : i32]}> : (tensor<8x16x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1312)
    %1048 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1313)
    %1049 = "ttir.matmul"(%1035, %1047, %1048) <{transpose_a = false, transpose_b = false}> : (tensor<128x197x197xbf16>, tensor<128x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1313)
    %1050 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1314)
    %1051 = "ttir.reshape"(%1049, %1050) <{shape = [8 : i32, 16 : i32, 197 : i32, 64 : i32]}> : (tensor<128x197x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1314)
    %1052 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1315)
    %1053 = "ttir.transpose"(%1051, %1052) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x16x197x64xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1315)
    %1054 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1865)
    %1055 = "ttir.reshape"(%1053, %1054) <{shape = [1576 : i32, 1024 : i32]}> : (tensor<8x197x16x64xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1865)
    %1056 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1866)
    %1057 = "ttir.matmul"(%1055, %arg320, %1056) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1866)
    %1058 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1867)
    %1059 = "ttir.reshape"(%1057, %1058) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1867)
    %1060 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc55)
    %1061 = "ttir.add"(%1059, %arg321, %1060) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc55)
    %1062 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc267)
    %1063 = "ttir.add"(%1061, %971, %1062) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc267)
    %1064 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc596)
    %1065 = "ttir.sum"(%1063, %1064) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc596)
    %1066 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc597)
    %1067 = "ttir.multiply"(%arg62, %1065, %1066) : (tensor<8x197x1024xf32>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc597)
    %1068 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc598)
    %1069 = "ttir.subtract"(%1063, %1067, %1068) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc598)
    %1070 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc599)
    %1071 = "ttir.multiply"(%1069, %1069, %1070) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc599)
    %1072 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc600)
    %1073 = "ttir.sum"(%1071, %1072) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc600)
    %1074 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc601)
    %1075 = "ttir.multiply"(%arg63, %1073, %1074) : (tensor<8x197x1xf32>, tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc601)
    %1076 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc602)
    %1077 = "ttir.add"(%1075, %arg64, %1076) : (tensor<8x197x1xbf16>, tensor<8x197x1xf32>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc602)
    %1078 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc603)
    %1079 = "ttir.sqrt"(%1077, %1078) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc603)
    %1080 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc604)
    %1081 = "ttir.reciprocal"(%1079, %1080) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc604)
    %1082 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc605)
    %1083 = "ttir.multiply"(%1069, %1081, %1082) : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc605)
    %1084 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc606)
    %1085 = "ttir.multiply"(%1083, %arg322, %1084) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc606)
    %1086 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc607)
    %1087 = "ttir.add"(%1085, %arg323, %1086) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc607)
    %1088 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1317)
    %1089 = "ttir.reshape"(%1087, %1088) <{shape = [1576 : i32, 1024 : i32]}> : (tensor<8x197x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1317)
    %1090 = ttir.empty() : tensor<1576x4096xbf16> loc(#loc1318)
    %1091 = "ttir.matmul"(%1089, %arg324, %1090) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x4096xbf16>, tensor<1576x4096xbf16>) -> tensor<1576x4096xbf16> loc(#loc1318)
    %1092 = ttir.empty() : tensor<8x197x4096xbf16> loc(#loc1319)
    %1093 = "ttir.reshape"(%1091, %1092) <{shape = [8 : i32, 197 : i32, 4096 : i32]}> : (tensor<1576x4096xbf16>, tensor<8x197x4096xbf16>) -> tensor<8x197x4096xbf16> loc(#loc1319)
    %1094 = ttir.empty() : tensor<8x197x4096xbf16> loc(#loc56)
    %1095 = "ttir.add"(%1093, %arg325, %1094) : (tensor<8x197x4096xbf16>, tensor<4096xbf16>, tensor<8x197x4096xbf16>) -> tensor<8x197x4096xbf16> loc(#loc56)
    %1096 = ttir.empty() : tensor<8x197x4096xbf16> loc(#loc1320)
    %1097 = "ttir.gelu"(%1095, %1096) : (tensor<8x197x4096xbf16>, tensor<8x197x4096xbf16>) -> tensor<8x197x4096xbf16> loc(#loc1320)
    %1098 = ttir.empty() : tensor<1576x4096xbf16> loc(#loc1321)
    %1099 = "ttir.reshape"(%1097, %1098) <{shape = [1576 : i32, 4096 : i32]}> : (tensor<8x197x4096xbf16>, tensor<1576x4096xbf16>) -> tensor<1576x4096xbf16> loc(#loc1321)
    %1100 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1322)
    %1101 = "ttir.matmul"(%1099, %arg326, %1100) <{transpose_a = false, transpose_b = false}> : (tensor<1576x4096xbf16>, tensor<4096x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1322)
    %1102 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1323)
    %1103 = "ttir.reshape"(%1101, %1102) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1323)
    %1104 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc57)
    %1105 = "ttir.add"(%1103, %arg327, %1104) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc57)
    %1106 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc611)
    %1107 = "ttir.add"(%1105, %1063, %1106) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc611)
    %1108 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc612)
    %1109 = "ttir.sum"(%1107, %1108) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc612)
    %1110 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc613)
    %1111 = "ttir.multiply"(%arg65, %1109, %1110) : (tensor<8x197x1024xf32>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc613)
    %1112 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc614)
    %1113 = "ttir.subtract"(%1107, %1111, %1112) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc614)
    %1114 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc615)
    %1115 = "ttir.multiply"(%1113, %1113, %1114) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc615)
    %1116 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc616)
    %1117 = "ttir.sum"(%1115, %1116) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc616)
    %1118 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc617)
    %1119 = "ttir.multiply"(%arg66, %1117, %1118) : (tensor<8x197x1xf32>, tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc617)
    %1120 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc618)
    %1121 = "ttir.add"(%1119, %arg67, %1120) : (tensor<8x197x1xbf16>, tensor<8x197x1xf32>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc618)
    %1122 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc619)
    %1123 = "ttir.sqrt"(%1121, %1122) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc619)
    %1124 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc620)
    %1125 = "ttir.reciprocal"(%1123, %1124) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc620)
    %1126 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc621)
    %1127 = "ttir.multiply"(%1113, %1125, %1126) : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc621)
    %1128 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc622)
    %1129 = "ttir.multiply"(%1127, %arg328, %1128) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc622)
    %1130 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc623)
    %1131 = "ttir.add"(%1129, %arg329, %1130) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc623)
    %1132 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1868)
    %1133 = "ttir.reshape"(%1131, %1132) <{shape = [1576 : i32, 1024 : i32]}> : (tensor<8x197x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1868)
    %1134 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1869)
    %1135 = "ttir.matmul"(%1133, %arg330, %1134) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1869)
    %1136 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1870)
    %1137 = "ttir.reshape"(%1135, %1136) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1870)
    %1138 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc58)
    %1139 = "ttir.add"(%1137, %arg331, %1138) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc58)
    %1140 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1325)
    %1141 = "ttir.reshape"(%1139, %1140) <{shape = [8 : i32, 197 : i32, 16 : i32, 64 : i32]}> : (tensor<8x197x1024xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1325)
    %1142 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1326)
    %1143 = "ttir.transpose"(%1141, %1142) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x197x16x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1326)
    %1144 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1327)
    %1145 = "ttir.reshape"(%1143, %1144) <{shape = [128 : i32, 197 : i32, 64 : i32]}> : (tensor<8x16x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1327)
    %1146 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1871)
    %1147 = "ttir.matmul"(%1133, %arg332, %1146) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1871)
    %1148 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1872)
    %1149 = "ttir.reshape"(%1147, %1148) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1872)
    %1150 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc59)
    %1151 = "ttir.add"(%1149, %arg333, %1150) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc59)
    %1152 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1329)
    %1153 = "ttir.reshape"(%1151, %1152) <{shape = [8 : i32, 197 : i32, 16 : i32, 64 : i32]}> : (tensor<8x197x1024xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1329)
    %1154 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1330)
    %1155 = "ttir.transpose"(%1153, %1154) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x197x16x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1330)
    %1156 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1331)
    %1157 = "ttir.reshape"(%1155, %1156) <{shape = [128 : i32, 197 : i32, 64 : i32]}> : (tensor<8x16x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1331)
    %1158 = ttir.empty() : tensor<128x64x197xbf16> loc(#loc60)
    %1159 = "ttir.transpose"(%1157, %1158) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<128x197x64xbf16>, tensor<128x64x197xbf16>) -> tensor<128x64x197xbf16> loc(#loc60)
    %1160 = ttir.empty() : tensor<128x197x197xbf16> loc(#loc1332)
    %1161 = "ttir.matmul"(%1145, %1159, %1160) <{transpose_a = false, transpose_b = false}> : (tensor<128x197x64xbf16>, tensor<128x64x197xbf16>, tensor<128x197x197xbf16>) -> tensor<128x197x197xbf16> loc(#loc1332)
    %1162 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1333)
    %1163 = "ttir.reshape"(%1161, %1162) <{shape = [8 : i32, 16 : i32, 197 : i32, 197 : i32]}> : (tensor<128x197x197xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1333)
    %1164 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1334)
    %1165 = "ttir.multiply"(%1163, %arg68, %1164) : (tensor<8x16x197x197xbf16>, tensor<1xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1334)
    %1166 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1335)
    %1167 = "ttir.add"(%1165, %arg69, %1166) : (tensor<8x16x197x197xbf16>, tensor<197x197xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1335)
    %1168 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1336)
    %1169 = "ttir.softmax"(%1167, %1168) <{dimension = -1 : si32}> : (tensor<8x16x197x197xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1336)
    %1170 = ttir.empty() : tensor<128x197x197xbf16> loc(#loc1337)
    %1171 = "ttir.reshape"(%1169, %1170) <{shape = [128 : i32, 197 : i32, 197 : i32]}> : (tensor<8x16x197x197xbf16>, tensor<128x197x197xbf16>) -> tensor<128x197x197xbf16> loc(#loc1337)
    %1172 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1873)
    %1173 = "ttir.matmul"(%1133, %arg334, %1172) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1873)
    %1174 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1874)
    %1175 = "ttir.reshape"(%1173, %1174) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1874)
    %1176 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc61)
    %1177 = "ttir.add"(%1175, %arg335, %1176) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc61)
    %1178 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1339)
    %1179 = "ttir.reshape"(%1177, %1178) <{shape = [8 : i32, 197 : i32, 16 : i32, 64 : i32]}> : (tensor<8x197x1024xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1339)
    %1180 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1340)
    %1181 = "ttir.transpose"(%1179, %1180) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x197x16x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1340)
    %1182 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1341)
    %1183 = "ttir.reshape"(%1181, %1182) <{shape = [128 : i32, 197 : i32, 64 : i32]}> : (tensor<8x16x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1341)
    %1184 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1342)
    %1185 = "ttir.matmul"(%1171, %1183, %1184) <{transpose_a = false, transpose_b = false}> : (tensor<128x197x197xbf16>, tensor<128x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1342)
    %1186 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1343)
    %1187 = "ttir.reshape"(%1185, %1186) <{shape = [8 : i32, 16 : i32, 197 : i32, 64 : i32]}> : (tensor<128x197x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1343)
    %1188 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1344)
    %1189 = "ttir.transpose"(%1187, %1188) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x16x197x64xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1344)
    %1190 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1875)
    %1191 = "ttir.reshape"(%1189, %1190) <{shape = [1576 : i32, 1024 : i32]}> : (tensor<8x197x16x64xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1875)
    %1192 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1876)
    %1193 = "ttir.matmul"(%1191, %arg336, %1192) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1876)
    %1194 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1877)
    %1195 = "ttir.reshape"(%1193, %1194) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1877)
    %1196 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc62)
    %1197 = "ttir.add"(%1195, %arg337, %1196) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc62)
    %1198 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc273)
    %1199 = "ttir.add"(%1197, %1107, %1198) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc273)
    %1200 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc626)
    %1201 = "ttir.sum"(%1199, %1200) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc626)
    %1202 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc627)
    %1203 = "ttir.multiply"(%arg70, %1201, %1202) : (tensor<8x197x1024xf32>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc627)
    %1204 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc628)
    %1205 = "ttir.subtract"(%1199, %1203, %1204) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc628)
    %1206 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc629)
    %1207 = "ttir.multiply"(%1205, %1205, %1206) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc629)
    %1208 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc630)
    %1209 = "ttir.sum"(%1207, %1208) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc630)
    %1210 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc631)
    %1211 = "ttir.multiply"(%arg71, %1209, %1210) : (tensor<8x197x1xf32>, tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc631)
    %1212 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc632)
    %1213 = "ttir.add"(%1211, %arg72, %1212) : (tensor<8x197x1xbf16>, tensor<8x197x1xf32>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc632)
    %1214 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc633)
    %1215 = "ttir.sqrt"(%1213, %1214) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc633)
    %1216 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc634)
    %1217 = "ttir.reciprocal"(%1215, %1216) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc634)
    %1218 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc635)
    %1219 = "ttir.multiply"(%1205, %1217, %1218) : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc635)
    %1220 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc636)
    %1221 = "ttir.multiply"(%1219, %arg338, %1220) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc636)
    %1222 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc637)
    %1223 = "ttir.add"(%1221, %arg339, %1222) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc637)
    %1224 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1346)
    %1225 = "ttir.reshape"(%1223, %1224) <{shape = [1576 : i32, 1024 : i32]}> : (tensor<8x197x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1346)
    %1226 = ttir.empty() : tensor<1576x4096xbf16> loc(#loc1347)
    %1227 = "ttir.matmul"(%1225, %arg340, %1226) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x4096xbf16>, tensor<1576x4096xbf16>) -> tensor<1576x4096xbf16> loc(#loc1347)
    %1228 = ttir.empty() : tensor<8x197x4096xbf16> loc(#loc1348)
    %1229 = "ttir.reshape"(%1227, %1228) <{shape = [8 : i32, 197 : i32, 4096 : i32]}> : (tensor<1576x4096xbf16>, tensor<8x197x4096xbf16>) -> tensor<8x197x4096xbf16> loc(#loc1348)
    %1230 = ttir.empty() : tensor<8x197x4096xbf16> loc(#loc63)
    %1231 = "ttir.add"(%1229, %arg341, %1230) : (tensor<8x197x4096xbf16>, tensor<4096xbf16>, tensor<8x197x4096xbf16>) -> tensor<8x197x4096xbf16> loc(#loc63)
    %1232 = ttir.empty() : tensor<8x197x4096xbf16> loc(#loc1349)
    %1233 = "ttir.gelu"(%1231, %1232) : (tensor<8x197x4096xbf16>, tensor<8x197x4096xbf16>) -> tensor<8x197x4096xbf16> loc(#loc1349)
    %1234 = ttir.empty() : tensor<1576x4096xbf16> loc(#loc1350)
    %1235 = "ttir.reshape"(%1233, %1234) <{shape = [1576 : i32, 4096 : i32]}> : (tensor<8x197x4096xbf16>, tensor<1576x4096xbf16>) -> tensor<1576x4096xbf16> loc(#loc1350)
    %1236 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1351)
    %1237 = "ttir.matmul"(%1235, %arg342, %1236) <{transpose_a = false, transpose_b = false}> : (tensor<1576x4096xbf16>, tensor<4096x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1351)
    %1238 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1352)
    %1239 = "ttir.reshape"(%1237, %1238) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1352)
    %1240 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc64)
    %1241 = "ttir.add"(%1239, %arg343, %1240) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc64)
    %1242 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc641)
    %1243 = "ttir.add"(%1241, %1199, %1242) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc641)
    %1244 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc642)
    %1245 = "ttir.sum"(%1243, %1244) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc642)
    %1246 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc643)
    %1247 = "ttir.multiply"(%arg73, %1245, %1246) : (tensor<8x197x1024xf32>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc643)
    %1248 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc644)
    %1249 = "ttir.subtract"(%1243, %1247, %1248) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc644)
    %1250 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc645)
    %1251 = "ttir.multiply"(%1249, %1249, %1250) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc645)
    %1252 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc646)
    %1253 = "ttir.sum"(%1251, %1252) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc646)
    %1254 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc647)
    %1255 = "ttir.multiply"(%arg74, %1253, %1254) : (tensor<8x197x1xf32>, tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc647)
    %1256 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc648)
    %1257 = "ttir.add"(%1255, %arg75, %1256) : (tensor<8x197x1xbf16>, tensor<8x197x1xf32>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc648)
    %1258 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc649)
    %1259 = "ttir.sqrt"(%1257, %1258) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc649)
    %1260 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc650)
    %1261 = "ttir.reciprocal"(%1259, %1260) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc650)
    %1262 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc651)
    %1263 = "ttir.multiply"(%1249, %1261, %1262) : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc651)
    %1264 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc652)
    %1265 = "ttir.multiply"(%1263, %arg344, %1264) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc652)
    %1266 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc653)
    %1267 = "ttir.add"(%1265, %arg345, %1266) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc653)
    %1268 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1878)
    %1269 = "ttir.reshape"(%1267, %1268) <{shape = [1576 : i32, 1024 : i32]}> : (tensor<8x197x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1878)
    %1270 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1879)
    %1271 = "ttir.matmul"(%1269, %arg346, %1270) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1879)
    %1272 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1880)
    %1273 = "ttir.reshape"(%1271, %1272) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1880)
    %1274 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc65)
    %1275 = "ttir.add"(%1273, %arg347, %1274) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc65)
    %1276 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1354)
    %1277 = "ttir.reshape"(%1275, %1276) <{shape = [8 : i32, 197 : i32, 16 : i32, 64 : i32]}> : (tensor<8x197x1024xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1354)
    %1278 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1355)
    %1279 = "ttir.transpose"(%1277, %1278) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x197x16x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1355)
    %1280 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1356)
    %1281 = "ttir.reshape"(%1279, %1280) <{shape = [128 : i32, 197 : i32, 64 : i32]}> : (tensor<8x16x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1356)
    %1282 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1881)
    %1283 = "ttir.matmul"(%1269, %arg348, %1282) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1881)
    %1284 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1882)
    %1285 = "ttir.reshape"(%1283, %1284) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1882)
    %1286 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc66)
    %1287 = "ttir.add"(%1285, %arg349, %1286) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc66)
    %1288 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1358)
    %1289 = "ttir.reshape"(%1287, %1288) <{shape = [8 : i32, 197 : i32, 16 : i32, 64 : i32]}> : (tensor<8x197x1024xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1358)
    %1290 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1359)
    %1291 = "ttir.transpose"(%1289, %1290) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x197x16x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1359)
    %1292 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1360)
    %1293 = "ttir.reshape"(%1291, %1292) <{shape = [128 : i32, 197 : i32, 64 : i32]}> : (tensor<8x16x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1360)
    %1294 = ttir.empty() : tensor<128x64x197xbf16> loc(#loc67)
    %1295 = "ttir.transpose"(%1293, %1294) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<128x197x64xbf16>, tensor<128x64x197xbf16>) -> tensor<128x64x197xbf16> loc(#loc67)
    %1296 = ttir.empty() : tensor<128x197x197xbf16> loc(#loc1361)
    %1297 = "ttir.matmul"(%1281, %1295, %1296) <{transpose_a = false, transpose_b = false}> : (tensor<128x197x64xbf16>, tensor<128x64x197xbf16>, tensor<128x197x197xbf16>) -> tensor<128x197x197xbf16> loc(#loc1361)
    %1298 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1362)
    %1299 = "ttir.reshape"(%1297, %1298) <{shape = [8 : i32, 16 : i32, 197 : i32, 197 : i32]}> : (tensor<128x197x197xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1362)
    %1300 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1363)
    %1301 = "ttir.multiply"(%1299, %arg76, %1300) : (tensor<8x16x197x197xbf16>, tensor<1xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1363)
    %1302 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1364)
    %1303 = "ttir.add"(%1301, %arg77, %1302) : (tensor<8x16x197x197xbf16>, tensor<197x197xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1364)
    %1304 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1365)
    %1305 = "ttir.softmax"(%1303, %1304) <{dimension = -1 : si32}> : (tensor<8x16x197x197xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1365)
    %1306 = ttir.empty() : tensor<128x197x197xbf16> loc(#loc1366)
    %1307 = "ttir.reshape"(%1305, %1306) <{shape = [128 : i32, 197 : i32, 197 : i32]}> : (tensor<8x16x197x197xbf16>, tensor<128x197x197xbf16>) -> tensor<128x197x197xbf16> loc(#loc1366)
    %1308 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1883)
    %1309 = "ttir.matmul"(%1269, %arg350, %1308) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1883)
    %1310 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1884)
    %1311 = "ttir.reshape"(%1309, %1310) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1884)
    %1312 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc68)
    %1313 = "ttir.add"(%1311, %arg351, %1312) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc68)
    %1314 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1368)
    %1315 = "ttir.reshape"(%1313, %1314) <{shape = [8 : i32, 197 : i32, 16 : i32, 64 : i32]}> : (tensor<8x197x1024xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1368)
    %1316 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1369)
    %1317 = "ttir.transpose"(%1315, %1316) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x197x16x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1369)
    %1318 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1370)
    %1319 = "ttir.reshape"(%1317, %1318) <{shape = [128 : i32, 197 : i32, 64 : i32]}> : (tensor<8x16x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1370)
    %1320 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1371)
    %1321 = "ttir.matmul"(%1307, %1319, %1320) <{transpose_a = false, transpose_b = false}> : (tensor<128x197x197xbf16>, tensor<128x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1371)
    %1322 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1372)
    %1323 = "ttir.reshape"(%1321, %1322) <{shape = [8 : i32, 16 : i32, 197 : i32, 64 : i32]}> : (tensor<128x197x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1372)
    %1324 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1373)
    %1325 = "ttir.transpose"(%1323, %1324) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x16x197x64xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1373)
    %1326 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1885)
    %1327 = "ttir.reshape"(%1325, %1326) <{shape = [1576 : i32, 1024 : i32]}> : (tensor<8x197x16x64xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1885)
    %1328 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1886)
    %1329 = "ttir.matmul"(%1327, %arg352, %1328) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1886)
    %1330 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1887)
    %1331 = "ttir.reshape"(%1329, %1330) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1887)
    %1332 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc69)
    %1333 = "ttir.add"(%1331, %arg353, %1332) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc69)
    %1334 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc279)
    %1335 = "ttir.add"(%1333, %1243, %1334) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc279)
    %1336 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc656)
    %1337 = "ttir.sum"(%1335, %1336) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc656)
    %1338 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc657)
    %1339 = "ttir.multiply"(%arg78, %1337, %1338) : (tensor<8x197x1024xf32>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc657)
    %1340 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc658)
    %1341 = "ttir.subtract"(%1335, %1339, %1340) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc658)
    %1342 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc659)
    %1343 = "ttir.multiply"(%1341, %1341, %1342) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc659)
    %1344 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc660)
    %1345 = "ttir.sum"(%1343, %1344) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc660)
    %1346 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc661)
    %1347 = "ttir.multiply"(%arg79, %1345, %1346) : (tensor<8x197x1xf32>, tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc661)
    %1348 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc662)
    %1349 = "ttir.add"(%1347, %arg80, %1348) : (tensor<8x197x1xbf16>, tensor<8x197x1xf32>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc662)
    %1350 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc663)
    %1351 = "ttir.sqrt"(%1349, %1350) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc663)
    %1352 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc664)
    %1353 = "ttir.reciprocal"(%1351, %1352) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc664)
    %1354 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc665)
    %1355 = "ttir.multiply"(%1341, %1353, %1354) : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc665)
    %1356 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc666)
    %1357 = "ttir.multiply"(%1355, %arg354, %1356) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc666)
    %1358 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc667)
    %1359 = "ttir.add"(%1357, %arg355, %1358) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc667)
    %1360 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1375)
    %1361 = "ttir.reshape"(%1359, %1360) <{shape = [1576 : i32, 1024 : i32]}> : (tensor<8x197x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1375)
    %1362 = ttir.empty() : tensor<1576x4096xbf16> loc(#loc1376)
    %1363 = "ttir.matmul"(%1361, %arg356, %1362) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x4096xbf16>, tensor<1576x4096xbf16>) -> tensor<1576x4096xbf16> loc(#loc1376)
    %1364 = ttir.empty() : tensor<8x197x4096xbf16> loc(#loc1377)
    %1365 = "ttir.reshape"(%1363, %1364) <{shape = [8 : i32, 197 : i32, 4096 : i32]}> : (tensor<1576x4096xbf16>, tensor<8x197x4096xbf16>) -> tensor<8x197x4096xbf16> loc(#loc1377)
    %1366 = ttir.empty() : tensor<8x197x4096xbf16> loc(#loc70)
    %1367 = "ttir.add"(%1365, %arg357, %1366) : (tensor<8x197x4096xbf16>, tensor<4096xbf16>, tensor<8x197x4096xbf16>) -> tensor<8x197x4096xbf16> loc(#loc70)
    %1368 = ttir.empty() : tensor<8x197x4096xbf16> loc(#loc1378)
    %1369 = "ttir.gelu"(%1367, %1368) : (tensor<8x197x4096xbf16>, tensor<8x197x4096xbf16>) -> tensor<8x197x4096xbf16> loc(#loc1378)
    %1370 = ttir.empty() : tensor<1576x4096xbf16> loc(#loc1379)
    %1371 = "ttir.reshape"(%1369, %1370) <{shape = [1576 : i32, 4096 : i32]}> : (tensor<8x197x4096xbf16>, tensor<1576x4096xbf16>) -> tensor<1576x4096xbf16> loc(#loc1379)
    %1372 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1380)
    %1373 = "ttir.matmul"(%1371, %arg358, %1372) <{transpose_a = false, transpose_b = false}> : (tensor<1576x4096xbf16>, tensor<4096x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1380)
    %1374 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1381)
    %1375 = "ttir.reshape"(%1373, %1374) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1381)
    %1376 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc71)
    %1377 = "ttir.add"(%1375, %arg359, %1376) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc71)
    %1378 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc671)
    %1379 = "ttir.add"(%1377, %1335, %1378) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc671)
    %1380 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc672)
    %1381 = "ttir.sum"(%1379, %1380) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc672)
    %1382 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc673)
    %1383 = "ttir.multiply"(%arg81, %1381, %1382) : (tensor<8x197x1024xf32>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc673)
    %1384 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc674)
    %1385 = "ttir.subtract"(%1379, %1383, %1384) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc674)
    %1386 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc675)
    %1387 = "ttir.multiply"(%1385, %1385, %1386) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc675)
    %1388 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc676)
    %1389 = "ttir.sum"(%1387, %1388) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc676)
    %1390 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc677)
    %1391 = "ttir.multiply"(%arg82, %1389, %1390) : (tensor<8x197x1xf32>, tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc677)
    %1392 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc678)
    %1393 = "ttir.add"(%1391, %arg83, %1392) : (tensor<8x197x1xbf16>, tensor<8x197x1xf32>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc678)
    %1394 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc679)
    %1395 = "ttir.sqrt"(%1393, %1394) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc679)
    %1396 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc680)
    %1397 = "ttir.reciprocal"(%1395, %1396) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc680)
    %1398 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc681)
    %1399 = "ttir.multiply"(%1385, %1397, %1398) : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc681)
    %1400 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc682)
    %1401 = "ttir.multiply"(%1399, %arg360, %1400) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc682)
    %1402 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc683)
    %1403 = "ttir.add"(%1401, %arg361, %1402) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc683)
    %1404 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1888)
    %1405 = "ttir.reshape"(%1403, %1404) <{shape = [1576 : i32, 1024 : i32]}> : (tensor<8x197x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1888)
    %1406 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1889)
    %1407 = "ttir.matmul"(%1405, %arg362, %1406) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1889)
    %1408 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1890)
    %1409 = "ttir.reshape"(%1407, %1408) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1890)
    %1410 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc72)
    %1411 = "ttir.add"(%1409, %arg363, %1410) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc72)
    %1412 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1383)
    %1413 = "ttir.reshape"(%1411, %1412) <{shape = [8 : i32, 197 : i32, 16 : i32, 64 : i32]}> : (tensor<8x197x1024xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1383)
    %1414 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1384)
    %1415 = "ttir.transpose"(%1413, %1414) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x197x16x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1384)
    %1416 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1385)
    %1417 = "ttir.reshape"(%1415, %1416) <{shape = [128 : i32, 197 : i32, 64 : i32]}> : (tensor<8x16x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1385)
    %1418 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1891)
    %1419 = "ttir.matmul"(%1405, %arg364, %1418) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1891)
    %1420 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1892)
    %1421 = "ttir.reshape"(%1419, %1420) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1892)
    %1422 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc73)
    %1423 = "ttir.add"(%1421, %arg365, %1422) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc73)
    %1424 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1387)
    %1425 = "ttir.reshape"(%1423, %1424) <{shape = [8 : i32, 197 : i32, 16 : i32, 64 : i32]}> : (tensor<8x197x1024xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1387)
    %1426 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1388)
    %1427 = "ttir.transpose"(%1425, %1426) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x197x16x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1388)
    %1428 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1389)
    %1429 = "ttir.reshape"(%1427, %1428) <{shape = [128 : i32, 197 : i32, 64 : i32]}> : (tensor<8x16x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1389)
    %1430 = ttir.empty() : tensor<128x64x197xbf16> loc(#loc74)
    %1431 = "ttir.transpose"(%1429, %1430) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<128x197x64xbf16>, tensor<128x64x197xbf16>) -> tensor<128x64x197xbf16> loc(#loc74)
    %1432 = ttir.empty() : tensor<128x197x197xbf16> loc(#loc1390)
    %1433 = "ttir.matmul"(%1417, %1431, %1432) <{transpose_a = false, transpose_b = false}> : (tensor<128x197x64xbf16>, tensor<128x64x197xbf16>, tensor<128x197x197xbf16>) -> tensor<128x197x197xbf16> loc(#loc1390)
    %1434 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1391)
    %1435 = "ttir.reshape"(%1433, %1434) <{shape = [8 : i32, 16 : i32, 197 : i32, 197 : i32]}> : (tensor<128x197x197xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1391)
    %1436 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1392)
    %1437 = "ttir.multiply"(%1435, %arg84, %1436) : (tensor<8x16x197x197xbf16>, tensor<1xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1392)
    %1438 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1393)
    %1439 = "ttir.add"(%1437, %arg85, %1438) : (tensor<8x16x197x197xbf16>, tensor<197x197xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1393)
    %1440 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1394)
    %1441 = "ttir.softmax"(%1439, %1440) <{dimension = -1 : si32}> : (tensor<8x16x197x197xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1394)
    %1442 = ttir.empty() : tensor<128x197x197xbf16> loc(#loc1395)
    %1443 = "ttir.reshape"(%1441, %1442) <{shape = [128 : i32, 197 : i32, 197 : i32]}> : (tensor<8x16x197x197xbf16>, tensor<128x197x197xbf16>) -> tensor<128x197x197xbf16> loc(#loc1395)
    %1444 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1893)
    %1445 = "ttir.matmul"(%1405, %arg366, %1444) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1893)
    %1446 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1894)
    %1447 = "ttir.reshape"(%1445, %1446) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1894)
    %1448 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc75)
    %1449 = "ttir.add"(%1447, %arg367, %1448) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc75)
    %1450 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1397)
    %1451 = "ttir.reshape"(%1449, %1450) <{shape = [8 : i32, 197 : i32, 16 : i32, 64 : i32]}> : (tensor<8x197x1024xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1397)
    %1452 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1398)
    %1453 = "ttir.transpose"(%1451, %1452) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x197x16x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1398)
    %1454 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1399)
    %1455 = "ttir.reshape"(%1453, %1454) <{shape = [128 : i32, 197 : i32, 64 : i32]}> : (tensor<8x16x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1399)
    %1456 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1400)
    %1457 = "ttir.matmul"(%1443, %1455, %1456) <{transpose_a = false, transpose_b = false}> : (tensor<128x197x197xbf16>, tensor<128x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1400)
    %1458 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1401)
    %1459 = "ttir.reshape"(%1457, %1458) <{shape = [8 : i32, 16 : i32, 197 : i32, 64 : i32]}> : (tensor<128x197x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1401)
    %1460 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1402)
    %1461 = "ttir.transpose"(%1459, %1460) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x16x197x64xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1402)
    %1462 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1895)
    %1463 = "ttir.reshape"(%1461, %1462) <{shape = [1576 : i32, 1024 : i32]}> : (tensor<8x197x16x64xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1895)
    %1464 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1896)
    %1465 = "ttir.matmul"(%1463, %arg368, %1464) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1896)
    %1466 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1897)
    %1467 = "ttir.reshape"(%1465, %1466) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1897)
    %1468 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc76)
    %1469 = "ttir.add"(%1467, %arg369, %1468) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc76)
    %1470 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc285)
    %1471 = "ttir.add"(%1469, %1379, %1470) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc285)
    %1472 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc686)
    %1473 = "ttir.sum"(%1471, %1472) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc686)
    %1474 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc687)
    %1475 = "ttir.multiply"(%arg86, %1473, %1474) : (tensor<8x197x1024xf32>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc687)
    %1476 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc688)
    %1477 = "ttir.subtract"(%1471, %1475, %1476) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc688)
    %1478 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc689)
    %1479 = "ttir.multiply"(%1477, %1477, %1478) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc689)
    %1480 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc690)
    %1481 = "ttir.sum"(%1479, %1480) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc690)
    %1482 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc691)
    %1483 = "ttir.multiply"(%arg87, %1481, %1482) : (tensor<8x197x1xf32>, tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc691)
    %1484 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc692)
    %1485 = "ttir.add"(%1483, %arg88, %1484) : (tensor<8x197x1xbf16>, tensor<8x197x1xf32>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc692)
    %1486 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc693)
    %1487 = "ttir.sqrt"(%1485, %1486) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc693)
    %1488 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc694)
    %1489 = "ttir.reciprocal"(%1487, %1488) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc694)
    %1490 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc695)
    %1491 = "ttir.multiply"(%1477, %1489, %1490) : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc695)
    %1492 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc696)
    %1493 = "ttir.multiply"(%1491, %arg370, %1492) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc696)
    %1494 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc697)
    %1495 = "ttir.add"(%1493, %arg371, %1494) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc697)
    %1496 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1404)
    %1497 = "ttir.reshape"(%1495, %1496) <{shape = [1576 : i32, 1024 : i32]}> : (tensor<8x197x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1404)
    %1498 = ttir.empty() : tensor<1576x4096xbf16> loc(#loc1405)
    %1499 = "ttir.matmul"(%1497, %arg372, %1498) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x4096xbf16>, tensor<1576x4096xbf16>) -> tensor<1576x4096xbf16> loc(#loc1405)
    %1500 = ttir.empty() : tensor<8x197x4096xbf16> loc(#loc1406)
    %1501 = "ttir.reshape"(%1499, %1500) <{shape = [8 : i32, 197 : i32, 4096 : i32]}> : (tensor<1576x4096xbf16>, tensor<8x197x4096xbf16>) -> tensor<8x197x4096xbf16> loc(#loc1406)
    %1502 = ttir.empty() : tensor<8x197x4096xbf16> loc(#loc77)
    %1503 = "ttir.add"(%1501, %arg373, %1502) : (tensor<8x197x4096xbf16>, tensor<4096xbf16>, tensor<8x197x4096xbf16>) -> tensor<8x197x4096xbf16> loc(#loc77)
    %1504 = ttir.empty() : tensor<8x197x4096xbf16> loc(#loc1407)
    %1505 = "ttir.gelu"(%1503, %1504) : (tensor<8x197x4096xbf16>, tensor<8x197x4096xbf16>) -> tensor<8x197x4096xbf16> loc(#loc1407)
    %1506 = ttir.empty() : tensor<1576x4096xbf16> loc(#loc1408)
    %1507 = "ttir.reshape"(%1505, %1506) <{shape = [1576 : i32, 4096 : i32]}> : (tensor<8x197x4096xbf16>, tensor<1576x4096xbf16>) -> tensor<1576x4096xbf16> loc(#loc1408)
    %1508 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1409)
    %1509 = "ttir.matmul"(%1507, %arg374, %1508) <{transpose_a = false, transpose_b = false}> : (tensor<1576x4096xbf16>, tensor<4096x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1409)
    %1510 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1410)
    %1511 = "ttir.reshape"(%1509, %1510) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1410)
    %1512 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc78)
    %1513 = "ttir.add"(%1511, %arg375, %1512) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc78)
    %1514 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc701)
    %1515 = "ttir.add"(%1513, %1471, %1514) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc701)
    %1516 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc702)
    %1517 = "ttir.sum"(%1515, %1516) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc702)
    %1518 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc703)
    %1519 = "ttir.multiply"(%arg89, %1517, %1518) : (tensor<8x197x1024xf32>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc703)
    %1520 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc704)
    %1521 = "ttir.subtract"(%1515, %1519, %1520) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc704)
    %1522 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc705)
    %1523 = "ttir.multiply"(%1521, %1521, %1522) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc705)
    %1524 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc706)
    %1525 = "ttir.sum"(%1523, %1524) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc706)
    %1526 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc707)
    %1527 = "ttir.multiply"(%arg90, %1525, %1526) : (tensor<8x197x1xf32>, tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc707)
    %1528 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc708)
    %1529 = "ttir.add"(%1527, %arg91, %1528) : (tensor<8x197x1xbf16>, tensor<8x197x1xf32>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc708)
    %1530 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc709)
    %1531 = "ttir.sqrt"(%1529, %1530) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc709)
    %1532 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc710)
    %1533 = "ttir.reciprocal"(%1531, %1532) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc710)
    %1534 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc711)
    %1535 = "ttir.multiply"(%1521, %1533, %1534) : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc711)
    %1536 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc712)
    %1537 = "ttir.multiply"(%1535, %arg376, %1536) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc712)
    %1538 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc713)
    %1539 = "ttir.add"(%1537, %arg377, %1538) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc713)
    %1540 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1898)
    %1541 = "ttir.reshape"(%1539, %1540) <{shape = [1576 : i32, 1024 : i32]}> : (tensor<8x197x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1898)
    %1542 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1899)
    %1543 = "ttir.matmul"(%1541, %arg378, %1542) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1899)
    %1544 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1900)
    %1545 = "ttir.reshape"(%1543, %1544) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1900)
    %1546 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc79)
    %1547 = "ttir.add"(%1545, %arg379, %1546) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc79)
    %1548 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1412)
    %1549 = "ttir.reshape"(%1547, %1548) <{shape = [8 : i32, 197 : i32, 16 : i32, 64 : i32]}> : (tensor<8x197x1024xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1412)
    %1550 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1413)
    %1551 = "ttir.transpose"(%1549, %1550) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x197x16x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1413)
    %1552 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1414)
    %1553 = "ttir.reshape"(%1551, %1552) <{shape = [128 : i32, 197 : i32, 64 : i32]}> : (tensor<8x16x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1414)
    %1554 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1901)
    %1555 = "ttir.matmul"(%1541, %arg380, %1554) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1901)
    %1556 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1902)
    %1557 = "ttir.reshape"(%1555, %1556) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1902)
    %1558 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc80)
    %1559 = "ttir.add"(%1557, %arg381, %1558) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc80)
    %1560 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1416)
    %1561 = "ttir.reshape"(%1559, %1560) <{shape = [8 : i32, 197 : i32, 16 : i32, 64 : i32]}> : (tensor<8x197x1024xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1416)
    %1562 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1417)
    %1563 = "ttir.transpose"(%1561, %1562) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x197x16x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1417)
    %1564 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1418)
    %1565 = "ttir.reshape"(%1563, %1564) <{shape = [128 : i32, 197 : i32, 64 : i32]}> : (tensor<8x16x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1418)
    %1566 = ttir.empty() : tensor<128x64x197xbf16> loc(#loc81)
    %1567 = "ttir.transpose"(%1565, %1566) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<128x197x64xbf16>, tensor<128x64x197xbf16>) -> tensor<128x64x197xbf16> loc(#loc81)
    %1568 = ttir.empty() : tensor<128x197x197xbf16> loc(#loc1419)
    %1569 = "ttir.matmul"(%1553, %1567, %1568) <{transpose_a = false, transpose_b = false}> : (tensor<128x197x64xbf16>, tensor<128x64x197xbf16>, tensor<128x197x197xbf16>) -> tensor<128x197x197xbf16> loc(#loc1419)
    %1570 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1420)
    %1571 = "ttir.reshape"(%1569, %1570) <{shape = [8 : i32, 16 : i32, 197 : i32, 197 : i32]}> : (tensor<128x197x197xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1420)
    %1572 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1421)
    %1573 = "ttir.multiply"(%1571, %arg92, %1572) : (tensor<8x16x197x197xbf16>, tensor<1xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1421)
    %1574 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1422)
    %1575 = "ttir.add"(%1573, %arg93, %1574) : (tensor<8x16x197x197xbf16>, tensor<197x197xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1422)
    %1576 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1423)
    %1577 = "ttir.softmax"(%1575, %1576) <{dimension = -1 : si32}> : (tensor<8x16x197x197xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1423)
    %1578 = ttir.empty() : tensor<128x197x197xbf16> loc(#loc1424)
    %1579 = "ttir.reshape"(%1577, %1578) <{shape = [128 : i32, 197 : i32, 197 : i32]}> : (tensor<8x16x197x197xbf16>, tensor<128x197x197xbf16>) -> tensor<128x197x197xbf16> loc(#loc1424)
    %1580 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1903)
    %1581 = "ttir.matmul"(%1541, %arg382, %1580) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1903)
    %1582 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1904)
    %1583 = "ttir.reshape"(%1581, %1582) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1904)
    %1584 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc82)
    %1585 = "ttir.add"(%1583, %arg383, %1584) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc82)
    %1586 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1426)
    %1587 = "ttir.reshape"(%1585, %1586) <{shape = [8 : i32, 197 : i32, 16 : i32, 64 : i32]}> : (tensor<8x197x1024xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1426)
    %1588 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1427)
    %1589 = "ttir.transpose"(%1587, %1588) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x197x16x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1427)
    %1590 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1428)
    %1591 = "ttir.reshape"(%1589, %1590) <{shape = [128 : i32, 197 : i32, 64 : i32]}> : (tensor<8x16x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1428)
    %1592 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1429)
    %1593 = "ttir.matmul"(%1579, %1591, %1592) <{transpose_a = false, transpose_b = false}> : (tensor<128x197x197xbf16>, tensor<128x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1429)
    %1594 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1430)
    %1595 = "ttir.reshape"(%1593, %1594) <{shape = [8 : i32, 16 : i32, 197 : i32, 64 : i32]}> : (tensor<128x197x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1430)
    %1596 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1431)
    %1597 = "ttir.transpose"(%1595, %1596) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x16x197x64xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1431)
    %1598 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1905)
    %1599 = "ttir.reshape"(%1597, %1598) <{shape = [1576 : i32, 1024 : i32]}> : (tensor<8x197x16x64xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1905)
    %1600 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1906)
    %1601 = "ttir.matmul"(%1599, %arg384, %1600) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1906)
    %1602 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1907)
    %1603 = "ttir.reshape"(%1601, %1602) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1907)
    %1604 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc83)
    %1605 = "ttir.add"(%1603, %arg385, %1604) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc83)
    %1606 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc291)
    %1607 = "ttir.add"(%1605, %1515, %1606) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc291)
    %1608 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc716)
    %1609 = "ttir.sum"(%1607, %1608) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc716)
    %1610 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc717)
    %1611 = "ttir.multiply"(%arg94, %1609, %1610) : (tensor<8x197x1024xf32>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc717)
    %1612 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc718)
    %1613 = "ttir.subtract"(%1607, %1611, %1612) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc718)
    %1614 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc719)
    %1615 = "ttir.multiply"(%1613, %1613, %1614) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc719)
    %1616 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc720)
    %1617 = "ttir.sum"(%1615, %1616) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc720)
    %1618 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc721)
    %1619 = "ttir.multiply"(%arg95, %1617, %1618) : (tensor<8x197x1xf32>, tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc721)
    %1620 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc722)
    %1621 = "ttir.add"(%1619, %arg96, %1620) : (tensor<8x197x1xbf16>, tensor<8x197x1xf32>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc722)
    %1622 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc723)
    %1623 = "ttir.sqrt"(%1621, %1622) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc723)
    %1624 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc724)
    %1625 = "ttir.reciprocal"(%1623, %1624) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc724)
    %1626 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc725)
    %1627 = "ttir.multiply"(%1613, %1625, %1626) : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc725)
    %1628 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc726)
    %1629 = "ttir.multiply"(%1627, %arg386, %1628) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc726)
    %1630 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc727)
    %1631 = "ttir.add"(%1629, %arg387, %1630) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc727)
    %1632 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1433)
    %1633 = "ttir.reshape"(%1631, %1632) <{shape = [1576 : i32, 1024 : i32]}> : (tensor<8x197x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1433)
    %1634 = ttir.empty() : tensor<1576x4096xbf16> loc(#loc1434)
    %1635 = "ttir.matmul"(%1633, %arg388, %1634) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x4096xbf16>, tensor<1576x4096xbf16>) -> tensor<1576x4096xbf16> loc(#loc1434)
    %1636 = ttir.empty() : tensor<8x197x4096xbf16> loc(#loc1435)
    %1637 = "ttir.reshape"(%1635, %1636) <{shape = [8 : i32, 197 : i32, 4096 : i32]}> : (tensor<1576x4096xbf16>, tensor<8x197x4096xbf16>) -> tensor<8x197x4096xbf16> loc(#loc1435)
    %1638 = ttir.empty() : tensor<8x197x4096xbf16> loc(#loc84)
    %1639 = "ttir.add"(%1637, %arg389, %1638) : (tensor<8x197x4096xbf16>, tensor<4096xbf16>, tensor<8x197x4096xbf16>) -> tensor<8x197x4096xbf16> loc(#loc84)
    %1640 = ttir.empty() : tensor<8x197x4096xbf16> loc(#loc1436)
    %1641 = "ttir.gelu"(%1639, %1640) : (tensor<8x197x4096xbf16>, tensor<8x197x4096xbf16>) -> tensor<8x197x4096xbf16> loc(#loc1436)
    %1642 = ttir.empty() : tensor<1576x4096xbf16> loc(#loc1437)
    %1643 = "ttir.reshape"(%1641, %1642) <{shape = [1576 : i32, 4096 : i32]}> : (tensor<8x197x4096xbf16>, tensor<1576x4096xbf16>) -> tensor<1576x4096xbf16> loc(#loc1437)
    %1644 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1438)
    %1645 = "ttir.matmul"(%1643, %arg390, %1644) <{transpose_a = false, transpose_b = false}> : (tensor<1576x4096xbf16>, tensor<4096x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1438)
    %1646 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1439)
    %1647 = "ttir.reshape"(%1645, %1646) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1439)
    %1648 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc85)
    %1649 = "ttir.add"(%1647, %arg391, %1648) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc85)
    %1650 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc731)
    %1651 = "ttir.add"(%1649, %1607, %1650) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc731)
    %1652 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc732)
    %1653 = "ttir.sum"(%1651, %1652) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc732)
    %1654 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc733)
    %1655 = "ttir.multiply"(%arg97, %1653, %1654) : (tensor<8x197x1024xf32>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc733)
    %1656 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc734)
    %1657 = "ttir.subtract"(%1651, %1655, %1656) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc734)
    %1658 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc735)
    %1659 = "ttir.multiply"(%1657, %1657, %1658) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc735)
    %1660 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc736)
    %1661 = "ttir.sum"(%1659, %1660) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc736)
    %1662 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc737)
    %1663 = "ttir.multiply"(%arg98, %1661, %1662) : (tensor<8x197x1xf32>, tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc737)
    %1664 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc738)
    %1665 = "ttir.add"(%1663, %arg99, %1664) : (tensor<8x197x1xbf16>, tensor<8x197x1xf32>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc738)
    %1666 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc739)
    %1667 = "ttir.sqrt"(%1665, %1666) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc739)
    %1668 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc740)
    %1669 = "ttir.reciprocal"(%1667, %1668) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc740)
    %1670 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc741)
    %1671 = "ttir.multiply"(%1657, %1669, %1670) : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc741)
    %1672 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc742)
    %1673 = "ttir.multiply"(%1671, %arg392, %1672) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc742)
    %1674 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc743)
    %1675 = "ttir.add"(%1673, %arg393, %1674) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc743)
    %1676 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1908)
    %1677 = "ttir.reshape"(%1675, %1676) <{shape = [1576 : i32, 1024 : i32]}> : (tensor<8x197x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1908)
    %1678 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1909)
    %1679 = "ttir.matmul"(%1677, %arg394, %1678) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1909)
    %1680 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1910)
    %1681 = "ttir.reshape"(%1679, %1680) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1910)
    %1682 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc86)
    %1683 = "ttir.add"(%1681, %arg395, %1682) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc86)
    %1684 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1441)
    %1685 = "ttir.reshape"(%1683, %1684) <{shape = [8 : i32, 197 : i32, 16 : i32, 64 : i32]}> : (tensor<8x197x1024xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1441)
    %1686 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1442)
    %1687 = "ttir.transpose"(%1685, %1686) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x197x16x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1442)
    %1688 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1443)
    %1689 = "ttir.reshape"(%1687, %1688) <{shape = [128 : i32, 197 : i32, 64 : i32]}> : (tensor<8x16x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1443)
    %1690 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1911)
    %1691 = "ttir.matmul"(%1677, %arg396, %1690) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1911)
    %1692 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1912)
    %1693 = "ttir.reshape"(%1691, %1692) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1912)
    %1694 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc87)
    %1695 = "ttir.add"(%1693, %arg397, %1694) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc87)
    %1696 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1445)
    %1697 = "ttir.reshape"(%1695, %1696) <{shape = [8 : i32, 197 : i32, 16 : i32, 64 : i32]}> : (tensor<8x197x1024xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1445)
    %1698 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1446)
    %1699 = "ttir.transpose"(%1697, %1698) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x197x16x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1446)
    %1700 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1447)
    %1701 = "ttir.reshape"(%1699, %1700) <{shape = [128 : i32, 197 : i32, 64 : i32]}> : (tensor<8x16x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1447)
    %1702 = ttir.empty() : tensor<128x64x197xbf16> loc(#loc88)
    %1703 = "ttir.transpose"(%1701, %1702) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<128x197x64xbf16>, tensor<128x64x197xbf16>) -> tensor<128x64x197xbf16> loc(#loc88)
    %1704 = ttir.empty() : tensor<128x197x197xbf16> loc(#loc1448)
    %1705 = "ttir.matmul"(%1689, %1703, %1704) <{transpose_a = false, transpose_b = false}> : (tensor<128x197x64xbf16>, tensor<128x64x197xbf16>, tensor<128x197x197xbf16>) -> tensor<128x197x197xbf16> loc(#loc1448)
    %1706 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1449)
    %1707 = "ttir.reshape"(%1705, %1706) <{shape = [8 : i32, 16 : i32, 197 : i32, 197 : i32]}> : (tensor<128x197x197xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1449)
    %1708 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1450)
    %1709 = "ttir.multiply"(%1707, %arg100, %1708) : (tensor<8x16x197x197xbf16>, tensor<1xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1450)
    %1710 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1451)
    %1711 = "ttir.add"(%1709, %arg101, %1710) : (tensor<8x16x197x197xbf16>, tensor<197x197xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1451)
    %1712 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1452)
    %1713 = "ttir.softmax"(%1711, %1712) <{dimension = -1 : si32}> : (tensor<8x16x197x197xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1452)
    %1714 = ttir.empty() : tensor<128x197x197xbf16> loc(#loc1453)
    %1715 = "ttir.reshape"(%1713, %1714) <{shape = [128 : i32, 197 : i32, 197 : i32]}> : (tensor<8x16x197x197xbf16>, tensor<128x197x197xbf16>) -> tensor<128x197x197xbf16> loc(#loc1453)
    %1716 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1913)
    %1717 = "ttir.matmul"(%1677, %arg398, %1716) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1913)
    %1718 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1914)
    %1719 = "ttir.reshape"(%1717, %1718) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1914)
    %1720 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc89)
    %1721 = "ttir.add"(%1719, %arg399, %1720) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc89)
    %1722 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1455)
    %1723 = "ttir.reshape"(%1721, %1722) <{shape = [8 : i32, 197 : i32, 16 : i32, 64 : i32]}> : (tensor<8x197x1024xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1455)
    %1724 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1456)
    %1725 = "ttir.transpose"(%1723, %1724) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x197x16x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1456)
    %1726 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1457)
    %1727 = "ttir.reshape"(%1725, %1726) <{shape = [128 : i32, 197 : i32, 64 : i32]}> : (tensor<8x16x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1457)
    %1728 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1458)
    %1729 = "ttir.matmul"(%1715, %1727, %1728) <{transpose_a = false, transpose_b = false}> : (tensor<128x197x197xbf16>, tensor<128x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1458)
    %1730 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1459)
    %1731 = "ttir.reshape"(%1729, %1730) <{shape = [8 : i32, 16 : i32, 197 : i32, 64 : i32]}> : (tensor<128x197x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1459)
    %1732 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1460)
    %1733 = "ttir.transpose"(%1731, %1732) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x16x197x64xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1460)
    %1734 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1915)
    %1735 = "ttir.reshape"(%1733, %1734) <{shape = [1576 : i32, 1024 : i32]}> : (tensor<8x197x16x64xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1915)
    %1736 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1916)
    %1737 = "ttir.matmul"(%1735, %arg400, %1736) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1916)
    %1738 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1917)
    %1739 = "ttir.reshape"(%1737, %1738) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1917)
    %1740 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc90)
    %1741 = "ttir.add"(%1739, %arg401, %1740) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc90)
    %1742 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc297)
    %1743 = "ttir.add"(%1741, %1651, %1742) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc297)
    %1744 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc746)
    %1745 = "ttir.sum"(%1743, %1744) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc746)
    %1746 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc747)
    %1747 = "ttir.multiply"(%arg102, %1745, %1746) : (tensor<8x197x1024xf32>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc747)
    %1748 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc748)
    %1749 = "ttir.subtract"(%1743, %1747, %1748) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc748)
    %1750 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc749)
    %1751 = "ttir.multiply"(%1749, %1749, %1750) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc749)
    %1752 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc750)
    %1753 = "ttir.sum"(%1751, %1752) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc750)
    %1754 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc751)
    %1755 = "ttir.multiply"(%arg103, %1753, %1754) : (tensor<8x197x1xf32>, tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc751)
    %1756 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc752)
    %1757 = "ttir.add"(%1755, %arg104, %1756) : (tensor<8x197x1xbf16>, tensor<8x197x1xf32>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc752)
    %1758 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc753)
    %1759 = "ttir.sqrt"(%1757, %1758) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc753)
    %1760 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc754)
    %1761 = "ttir.reciprocal"(%1759, %1760) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc754)
    %1762 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc755)
    %1763 = "ttir.multiply"(%1749, %1761, %1762) : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc755)
    %1764 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc756)
    %1765 = "ttir.multiply"(%1763, %arg402, %1764) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc756)
    %1766 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc757)
    %1767 = "ttir.add"(%1765, %arg403, %1766) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc757)
    %1768 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1462)
    %1769 = "ttir.reshape"(%1767, %1768) <{shape = [1576 : i32, 1024 : i32]}> : (tensor<8x197x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1462)
    %1770 = ttir.empty() : tensor<1576x4096xbf16> loc(#loc1463)
    %1771 = "ttir.matmul"(%1769, %arg404, %1770) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x4096xbf16>, tensor<1576x4096xbf16>) -> tensor<1576x4096xbf16> loc(#loc1463)
    %1772 = ttir.empty() : tensor<8x197x4096xbf16> loc(#loc1464)
    %1773 = "ttir.reshape"(%1771, %1772) <{shape = [8 : i32, 197 : i32, 4096 : i32]}> : (tensor<1576x4096xbf16>, tensor<8x197x4096xbf16>) -> tensor<8x197x4096xbf16> loc(#loc1464)
    %1774 = ttir.empty() : tensor<8x197x4096xbf16> loc(#loc91)
    %1775 = "ttir.add"(%1773, %arg405, %1774) : (tensor<8x197x4096xbf16>, tensor<4096xbf16>, tensor<8x197x4096xbf16>) -> tensor<8x197x4096xbf16> loc(#loc91)
    %1776 = ttir.empty() : tensor<8x197x4096xbf16> loc(#loc1465)
    %1777 = "ttir.gelu"(%1775, %1776) : (tensor<8x197x4096xbf16>, tensor<8x197x4096xbf16>) -> tensor<8x197x4096xbf16> loc(#loc1465)
    %1778 = ttir.empty() : tensor<1576x4096xbf16> loc(#loc1466)
    %1779 = "ttir.reshape"(%1777, %1778) <{shape = [1576 : i32, 4096 : i32]}> : (tensor<8x197x4096xbf16>, tensor<1576x4096xbf16>) -> tensor<1576x4096xbf16> loc(#loc1466)
    %1780 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1467)
    %1781 = "ttir.matmul"(%1779, %arg406, %1780) <{transpose_a = false, transpose_b = false}> : (tensor<1576x4096xbf16>, tensor<4096x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1467)
    %1782 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1468)
    %1783 = "ttir.reshape"(%1781, %1782) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1468)
    %1784 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc92)
    %1785 = "ttir.add"(%1783, %arg407, %1784) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc92)
    %1786 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc761)
    %1787 = "ttir.add"(%1785, %1743, %1786) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc761)
    %1788 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc762)
    %1789 = "ttir.sum"(%1787, %1788) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc762)
    %1790 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc763)
    %1791 = "ttir.multiply"(%arg105, %1789, %1790) : (tensor<8x197x1024xf32>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc763)
    %1792 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc764)
    %1793 = "ttir.subtract"(%1787, %1791, %1792) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc764)
    %1794 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc765)
    %1795 = "ttir.multiply"(%1793, %1793, %1794) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc765)
    %1796 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc766)
    %1797 = "ttir.sum"(%1795, %1796) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc766)
    %1798 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc767)
    %1799 = "ttir.multiply"(%arg106, %1797, %1798) : (tensor<8x197x1xf32>, tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc767)
    %1800 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc768)
    %1801 = "ttir.add"(%1799, %arg107, %1800) : (tensor<8x197x1xbf16>, tensor<8x197x1xf32>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc768)
    %1802 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc769)
    %1803 = "ttir.sqrt"(%1801, %1802) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc769)
    %1804 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc770)
    %1805 = "ttir.reciprocal"(%1803, %1804) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc770)
    %1806 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc771)
    %1807 = "ttir.multiply"(%1793, %1805, %1806) : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc771)
    %1808 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc772)
    %1809 = "ttir.multiply"(%1807, %arg408, %1808) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc772)
    %1810 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc773)
    %1811 = "ttir.add"(%1809, %arg409, %1810) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc773)
    %1812 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1918)
    %1813 = "ttir.reshape"(%1811, %1812) <{shape = [1576 : i32, 1024 : i32]}> : (tensor<8x197x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1918)
    %1814 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1919)
    %1815 = "ttir.matmul"(%1813, %arg410, %1814) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1919)
    %1816 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1920)
    %1817 = "ttir.reshape"(%1815, %1816) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1920)
    %1818 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc93)
    %1819 = "ttir.add"(%1817, %arg411, %1818) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc93)
    %1820 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1470)
    %1821 = "ttir.reshape"(%1819, %1820) <{shape = [8 : i32, 197 : i32, 16 : i32, 64 : i32]}> : (tensor<8x197x1024xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1470)
    %1822 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1471)
    %1823 = "ttir.transpose"(%1821, %1822) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x197x16x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1471)
    %1824 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1472)
    %1825 = "ttir.reshape"(%1823, %1824) <{shape = [128 : i32, 197 : i32, 64 : i32]}> : (tensor<8x16x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1472)
    %1826 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1921)
    %1827 = "ttir.matmul"(%1813, %arg412, %1826) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1921)
    %1828 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1922)
    %1829 = "ttir.reshape"(%1827, %1828) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1922)
    %1830 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc94)
    %1831 = "ttir.add"(%1829, %arg413, %1830) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc94)
    %1832 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1474)
    %1833 = "ttir.reshape"(%1831, %1832) <{shape = [8 : i32, 197 : i32, 16 : i32, 64 : i32]}> : (tensor<8x197x1024xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1474)
    %1834 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1475)
    %1835 = "ttir.transpose"(%1833, %1834) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x197x16x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1475)
    %1836 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1476)
    %1837 = "ttir.reshape"(%1835, %1836) <{shape = [128 : i32, 197 : i32, 64 : i32]}> : (tensor<8x16x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1476)
    %1838 = ttir.empty() : tensor<128x64x197xbf16> loc(#loc95)
    %1839 = "ttir.transpose"(%1837, %1838) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<128x197x64xbf16>, tensor<128x64x197xbf16>) -> tensor<128x64x197xbf16> loc(#loc95)
    %1840 = ttir.empty() : tensor<128x197x197xbf16> loc(#loc1477)
    %1841 = "ttir.matmul"(%1825, %1839, %1840) <{transpose_a = false, transpose_b = false}> : (tensor<128x197x64xbf16>, tensor<128x64x197xbf16>, tensor<128x197x197xbf16>) -> tensor<128x197x197xbf16> loc(#loc1477)
    %1842 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1478)
    %1843 = "ttir.reshape"(%1841, %1842) <{shape = [8 : i32, 16 : i32, 197 : i32, 197 : i32]}> : (tensor<128x197x197xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1478)
    %1844 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1479)
    %1845 = "ttir.multiply"(%1843, %arg108, %1844) : (tensor<8x16x197x197xbf16>, tensor<1xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1479)
    %1846 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1480)
    %1847 = "ttir.add"(%1845, %arg109, %1846) : (tensor<8x16x197x197xbf16>, tensor<197x197xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1480)
    %1848 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1481)
    %1849 = "ttir.softmax"(%1847, %1848) <{dimension = -1 : si32}> : (tensor<8x16x197x197xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1481)
    %1850 = ttir.empty() : tensor<128x197x197xbf16> loc(#loc1482)
    %1851 = "ttir.reshape"(%1849, %1850) <{shape = [128 : i32, 197 : i32, 197 : i32]}> : (tensor<8x16x197x197xbf16>, tensor<128x197x197xbf16>) -> tensor<128x197x197xbf16> loc(#loc1482)
    %1852 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1923)
    %1853 = "ttir.matmul"(%1813, %arg414, %1852) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1923)
    %1854 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1924)
    %1855 = "ttir.reshape"(%1853, %1854) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1924)
    %1856 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc96)
    %1857 = "ttir.add"(%1855, %arg415, %1856) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc96)
    %1858 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1484)
    %1859 = "ttir.reshape"(%1857, %1858) <{shape = [8 : i32, 197 : i32, 16 : i32, 64 : i32]}> : (tensor<8x197x1024xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1484)
    %1860 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1485)
    %1861 = "ttir.transpose"(%1859, %1860) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x197x16x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1485)
    %1862 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1486)
    %1863 = "ttir.reshape"(%1861, %1862) <{shape = [128 : i32, 197 : i32, 64 : i32]}> : (tensor<8x16x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1486)
    %1864 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1487)
    %1865 = "ttir.matmul"(%1851, %1863, %1864) <{transpose_a = false, transpose_b = false}> : (tensor<128x197x197xbf16>, tensor<128x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1487)
    %1866 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1488)
    %1867 = "ttir.reshape"(%1865, %1866) <{shape = [8 : i32, 16 : i32, 197 : i32, 64 : i32]}> : (tensor<128x197x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1488)
    %1868 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1489)
    %1869 = "ttir.transpose"(%1867, %1868) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x16x197x64xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1489)
    %1870 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1925)
    %1871 = "ttir.reshape"(%1869, %1870) <{shape = [1576 : i32, 1024 : i32]}> : (tensor<8x197x16x64xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1925)
    %1872 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1926)
    %1873 = "ttir.matmul"(%1871, %arg416, %1872) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1926)
    %1874 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1927)
    %1875 = "ttir.reshape"(%1873, %1874) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1927)
    %1876 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc97)
    %1877 = "ttir.add"(%1875, %arg417, %1876) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc97)
    %1878 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc303)
    %1879 = "ttir.add"(%1877, %1787, %1878) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc303)
    %1880 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc776)
    %1881 = "ttir.sum"(%1879, %1880) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc776)
    %1882 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc777)
    %1883 = "ttir.multiply"(%arg110, %1881, %1882) : (tensor<8x197x1024xf32>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc777)
    %1884 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc778)
    %1885 = "ttir.subtract"(%1879, %1883, %1884) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc778)
    %1886 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc779)
    %1887 = "ttir.multiply"(%1885, %1885, %1886) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc779)
    %1888 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc780)
    %1889 = "ttir.sum"(%1887, %1888) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc780)
    %1890 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc781)
    %1891 = "ttir.multiply"(%arg111, %1889, %1890) : (tensor<8x197x1xf32>, tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc781)
    %1892 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc782)
    %1893 = "ttir.add"(%1891, %arg112, %1892) : (tensor<8x197x1xbf16>, tensor<8x197x1xf32>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc782)
    %1894 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc783)
    %1895 = "ttir.sqrt"(%1893, %1894) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc783)
    %1896 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc784)
    %1897 = "ttir.reciprocal"(%1895, %1896) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc784)
    %1898 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc785)
    %1899 = "ttir.multiply"(%1885, %1897, %1898) : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc785)
    %1900 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc786)
    %1901 = "ttir.multiply"(%1899, %arg418, %1900) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc786)
    %1902 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc787)
    %1903 = "ttir.add"(%1901, %arg419, %1902) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc787)
    %1904 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1491)
    %1905 = "ttir.reshape"(%1903, %1904) <{shape = [1576 : i32, 1024 : i32]}> : (tensor<8x197x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1491)
    %1906 = ttir.empty() : tensor<1576x4096xbf16> loc(#loc1492)
    %1907 = "ttir.matmul"(%1905, %arg420, %1906) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x4096xbf16>, tensor<1576x4096xbf16>) -> tensor<1576x4096xbf16> loc(#loc1492)
    %1908 = ttir.empty() : tensor<8x197x4096xbf16> loc(#loc1493)
    %1909 = "ttir.reshape"(%1907, %1908) <{shape = [8 : i32, 197 : i32, 4096 : i32]}> : (tensor<1576x4096xbf16>, tensor<8x197x4096xbf16>) -> tensor<8x197x4096xbf16> loc(#loc1493)
    %1910 = ttir.empty() : tensor<8x197x4096xbf16> loc(#loc98)
    %1911 = "ttir.add"(%1909, %arg421, %1910) : (tensor<8x197x4096xbf16>, tensor<4096xbf16>, tensor<8x197x4096xbf16>) -> tensor<8x197x4096xbf16> loc(#loc98)
    %1912 = ttir.empty() : tensor<8x197x4096xbf16> loc(#loc1494)
    %1913 = "ttir.gelu"(%1911, %1912) : (tensor<8x197x4096xbf16>, tensor<8x197x4096xbf16>) -> tensor<8x197x4096xbf16> loc(#loc1494)
    %1914 = ttir.empty() : tensor<1576x4096xbf16> loc(#loc1495)
    %1915 = "ttir.reshape"(%1913, %1914) <{shape = [1576 : i32, 4096 : i32]}> : (tensor<8x197x4096xbf16>, tensor<1576x4096xbf16>) -> tensor<1576x4096xbf16> loc(#loc1495)
    %1916 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1496)
    %1917 = "ttir.matmul"(%1915, %arg422, %1916) <{transpose_a = false, transpose_b = false}> : (tensor<1576x4096xbf16>, tensor<4096x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1496)
    %1918 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1497)
    %1919 = "ttir.reshape"(%1917, %1918) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1497)
    %1920 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc99)
    %1921 = "ttir.add"(%1919, %arg423, %1920) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc99)
    %1922 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc791)
    %1923 = "ttir.add"(%1921, %1879, %1922) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc791)
    %1924 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc792)
    %1925 = "ttir.sum"(%1923, %1924) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc792)
    %1926 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc793)
    %1927 = "ttir.multiply"(%arg113, %1925, %1926) : (tensor<8x197x1024xf32>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc793)
    %1928 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc794)
    %1929 = "ttir.subtract"(%1923, %1927, %1928) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc794)
    %1930 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc795)
    %1931 = "ttir.multiply"(%1929, %1929, %1930) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc795)
    %1932 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc796)
    %1933 = "ttir.sum"(%1931, %1932) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc796)
    %1934 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc797)
    %1935 = "ttir.multiply"(%arg114, %1933, %1934) : (tensor<8x197x1xf32>, tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc797)
    %1936 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc798)
    %1937 = "ttir.add"(%1935, %arg115, %1936) : (tensor<8x197x1xbf16>, tensor<8x197x1xf32>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc798)
    %1938 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc799)
    %1939 = "ttir.sqrt"(%1937, %1938) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc799)
    %1940 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc800)
    %1941 = "ttir.reciprocal"(%1939, %1940) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc800)
    %1942 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc801)
    %1943 = "ttir.multiply"(%1929, %1941, %1942) : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc801)
    %1944 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc802)
    %1945 = "ttir.multiply"(%1943, %arg424, %1944) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc802)
    %1946 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc803)
    %1947 = "ttir.add"(%1945, %arg425, %1946) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc803)
    %1948 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1928)
    %1949 = "ttir.reshape"(%1947, %1948) <{shape = [1576 : i32, 1024 : i32]}> : (tensor<8x197x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1928)
    %1950 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1929)
    %1951 = "ttir.matmul"(%1949, %arg426, %1950) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1929)
    %1952 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1930)
    %1953 = "ttir.reshape"(%1951, %1952) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1930)
    %1954 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc100)
    %1955 = "ttir.add"(%1953, %arg427, %1954) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc100)
    %1956 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1499)
    %1957 = "ttir.reshape"(%1955, %1956) <{shape = [8 : i32, 197 : i32, 16 : i32, 64 : i32]}> : (tensor<8x197x1024xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1499)
    %1958 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1500)
    %1959 = "ttir.transpose"(%1957, %1958) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x197x16x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1500)
    %1960 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1501)
    %1961 = "ttir.reshape"(%1959, %1960) <{shape = [128 : i32, 197 : i32, 64 : i32]}> : (tensor<8x16x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1501)
    %1962 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1931)
    %1963 = "ttir.matmul"(%1949, %arg428, %1962) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1931)
    %1964 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1932)
    %1965 = "ttir.reshape"(%1963, %1964) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1932)
    %1966 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc101)
    %1967 = "ttir.add"(%1965, %arg429, %1966) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc101)
    %1968 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1503)
    %1969 = "ttir.reshape"(%1967, %1968) <{shape = [8 : i32, 197 : i32, 16 : i32, 64 : i32]}> : (tensor<8x197x1024xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1503)
    %1970 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1504)
    %1971 = "ttir.transpose"(%1969, %1970) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x197x16x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1504)
    %1972 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1505)
    %1973 = "ttir.reshape"(%1971, %1972) <{shape = [128 : i32, 197 : i32, 64 : i32]}> : (tensor<8x16x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1505)
    %1974 = ttir.empty() : tensor<128x64x197xbf16> loc(#loc102)
    %1975 = "ttir.transpose"(%1973, %1974) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<128x197x64xbf16>, tensor<128x64x197xbf16>) -> tensor<128x64x197xbf16> loc(#loc102)
    %1976 = ttir.empty() : tensor<128x197x197xbf16> loc(#loc1506)
    %1977 = "ttir.matmul"(%1961, %1975, %1976) <{transpose_a = false, transpose_b = false}> : (tensor<128x197x64xbf16>, tensor<128x64x197xbf16>, tensor<128x197x197xbf16>) -> tensor<128x197x197xbf16> loc(#loc1506)
    %1978 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1507)
    %1979 = "ttir.reshape"(%1977, %1978) <{shape = [8 : i32, 16 : i32, 197 : i32, 197 : i32]}> : (tensor<128x197x197xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1507)
    %1980 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1508)
    %1981 = "ttir.multiply"(%1979, %arg116, %1980) : (tensor<8x16x197x197xbf16>, tensor<1xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1508)
    %1982 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1509)
    %1983 = "ttir.add"(%1981, %arg117, %1982) : (tensor<8x16x197x197xbf16>, tensor<197x197xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1509)
    %1984 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1510)
    %1985 = "ttir.softmax"(%1983, %1984) <{dimension = -1 : si32}> : (tensor<8x16x197x197xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1510)
    %1986 = ttir.empty() : tensor<128x197x197xbf16> loc(#loc1511)
    %1987 = "ttir.reshape"(%1985, %1986) <{shape = [128 : i32, 197 : i32, 197 : i32]}> : (tensor<8x16x197x197xbf16>, tensor<128x197x197xbf16>) -> tensor<128x197x197xbf16> loc(#loc1511)
    %1988 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1933)
    %1989 = "ttir.matmul"(%1949, %arg430, %1988) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1933)
    %1990 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1934)
    %1991 = "ttir.reshape"(%1989, %1990) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1934)
    %1992 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc103)
    %1993 = "ttir.add"(%1991, %arg431, %1992) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc103)
    %1994 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1513)
    %1995 = "ttir.reshape"(%1993, %1994) <{shape = [8 : i32, 197 : i32, 16 : i32, 64 : i32]}> : (tensor<8x197x1024xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1513)
    %1996 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1514)
    %1997 = "ttir.transpose"(%1995, %1996) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x197x16x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1514)
    %1998 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1515)
    %1999 = "ttir.reshape"(%1997, %1998) <{shape = [128 : i32, 197 : i32, 64 : i32]}> : (tensor<8x16x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1515)
    %2000 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1516)
    %2001 = "ttir.matmul"(%1987, %1999, %2000) <{transpose_a = false, transpose_b = false}> : (tensor<128x197x197xbf16>, tensor<128x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1516)
    %2002 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1517)
    %2003 = "ttir.reshape"(%2001, %2002) <{shape = [8 : i32, 16 : i32, 197 : i32, 64 : i32]}> : (tensor<128x197x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1517)
    %2004 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1518)
    %2005 = "ttir.transpose"(%2003, %2004) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x16x197x64xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1518)
    %2006 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1935)
    %2007 = "ttir.reshape"(%2005, %2006) <{shape = [1576 : i32, 1024 : i32]}> : (tensor<8x197x16x64xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1935)
    %2008 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1936)
    %2009 = "ttir.matmul"(%2007, %arg432, %2008) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1936)
    %2010 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1937)
    %2011 = "ttir.reshape"(%2009, %2010) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1937)
    %2012 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc104)
    %2013 = "ttir.add"(%2011, %arg433, %2012) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc104)
    %2014 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc309)
    %2015 = "ttir.add"(%2013, %1923, %2014) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc309)
    %2016 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc806)
    %2017 = "ttir.sum"(%2015, %2016) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc806)
    %2018 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc807)
    %2019 = "ttir.multiply"(%arg118, %2017, %2018) : (tensor<8x197x1024xf32>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc807)
    %2020 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc808)
    %2021 = "ttir.subtract"(%2015, %2019, %2020) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc808)
    %2022 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc809)
    %2023 = "ttir.multiply"(%2021, %2021, %2022) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc809)
    %2024 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc810)
    %2025 = "ttir.sum"(%2023, %2024) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc810)
    %2026 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc811)
    %2027 = "ttir.multiply"(%arg119, %2025, %2026) : (tensor<8x197x1xf32>, tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc811)
    %2028 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc812)
    %2029 = "ttir.add"(%2027, %arg120, %2028) : (tensor<8x197x1xbf16>, tensor<8x197x1xf32>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc812)
    %2030 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc813)
    %2031 = "ttir.sqrt"(%2029, %2030) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc813)
    %2032 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc814)
    %2033 = "ttir.reciprocal"(%2031, %2032) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc814)
    %2034 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc815)
    %2035 = "ttir.multiply"(%2021, %2033, %2034) : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc815)
    %2036 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc816)
    %2037 = "ttir.multiply"(%2035, %arg434, %2036) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc816)
    %2038 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc817)
    %2039 = "ttir.add"(%2037, %arg435, %2038) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc817)
    %2040 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1520)
    %2041 = "ttir.reshape"(%2039, %2040) <{shape = [1576 : i32, 1024 : i32]}> : (tensor<8x197x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1520)
    %2042 = ttir.empty() : tensor<1576x4096xbf16> loc(#loc1521)
    %2043 = "ttir.matmul"(%2041, %arg436, %2042) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x4096xbf16>, tensor<1576x4096xbf16>) -> tensor<1576x4096xbf16> loc(#loc1521)
    %2044 = ttir.empty() : tensor<8x197x4096xbf16> loc(#loc1522)
    %2045 = "ttir.reshape"(%2043, %2044) <{shape = [8 : i32, 197 : i32, 4096 : i32]}> : (tensor<1576x4096xbf16>, tensor<8x197x4096xbf16>) -> tensor<8x197x4096xbf16> loc(#loc1522)
    %2046 = ttir.empty() : tensor<8x197x4096xbf16> loc(#loc105)
    %2047 = "ttir.add"(%2045, %arg437, %2046) : (tensor<8x197x4096xbf16>, tensor<4096xbf16>, tensor<8x197x4096xbf16>) -> tensor<8x197x4096xbf16> loc(#loc105)
    %2048 = ttir.empty() : tensor<8x197x4096xbf16> loc(#loc1523)
    %2049 = "ttir.gelu"(%2047, %2048) : (tensor<8x197x4096xbf16>, tensor<8x197x4096xbf16>) -> tensor<8x197x4096xbf16> loc(#loc1523)
    %2050 = ttir.empty() : tensor<1576x4096xbf16> loc(#loc1524)
    %2051 = "ttir.reshape"(%2049, %2050) <{shape = [1576 : i32, 4096 : i32]}> : (tensor<8x197x4096xbf16>, tensor<1576x4096xbf16>) -> tensor<1576x4096xbf16> loc(#loc1524)
    %2052 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1525)
    %2053 = "ttir.matmul"(%2051, %arg438, %2052) <{transpose_a = false, transpose_b = false}> : (tensor<1576x4096xbf16>, tensor<4096x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1525)
    %2054 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1526)
    %2055 = "ttir.reshape"(%2053, %2054) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1526)
    %2056 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc106)
    %2057 = "ttir.add"(%2055, %arg439, %2056) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc106)
    %2058 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc821)
    %2059 = "ttir.add"(%2057, %2015, %2058) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc821)
    %2060 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc822)
    %2061 = "ttir.sum"(%2059, %2060) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc822)
    %2062 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc823)
    %2063 = "ttir.multiply"(%arg121, %2061, %2062) : (tensor<8x197x1024xf32>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc823)
    %2064 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc824)
    %2065 = "ttir.subtract"(%2059, %2063, %2064) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc824)
    %2066 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc825)
    %2067 = "ttir.multiply"(%2065, %2065, %2066) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc825)
    %2068 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc826)
    %2069 = "ttir.sum"(%2067, %2068) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc826)
    %2070 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc827)
    %2071 = "ttir.multiply"(%arg122, %2069, %2070) : (tensor<8x197x1xf32>, tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc827)
    %2072 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc828)
    %2073 = "ttir.add"(%2071, %arg123, %2072) : (tensor<8x197x1xbf16>, tensor<8x197x1xf32>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc828)
    %2074 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc829)
    %2075 = "ttir.sqrt"(%2073, %2074) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc829)
    %2076 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc830)
    %2077 = "ttir.reciprocal"(%2075, %2076) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc830)
    %2078 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc831)
    %2079 = "ttir.multiply"(%2065, %2077, %2078) : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc831)
    %2080 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc832)
    %2081 = "ttir.multiply"(%2079, %arg440, %2080) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc832)
    %2082 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc833)
    %2083 = "ttir.add"(%2081, %arg441, %2082) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc833)
    %2084 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1938)
    %2085 = "ttir.reshape"(%2083, %2084) <{shape = [1576 : i32, 1024 : i32]}> : (tensor<8x197x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1938)
    %2086 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1939)
    %2087 = "ttir.matmul"(%2085, %arg442, %2086) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1939)
    %2088 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1940)
    %2089 = "ttir.reshape"(%2087, %2088) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1940)
    %2090 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc107)
    %2091 = "ttir.add"(%2089, %arg443, %2090) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc107)
    %2092 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1528)
    %2093 = "ttir.reshape"(%2091, %2092) <{shape = [8 : i32, 197 : i32, 16 : i32, 64 : i32]}> : (tensor<8x197x1024xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1528)
    %2094 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1529)
    %2095 = "ttir.transpose"(%2093, %2094) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x197x16x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1529)
    %2096 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1530)
    %2097 = "ttir.reshape"(%2095, %2096) <{shape = [128 : i32, 197 : i32, 64 : i32]}> : (tensor<8x16x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1530)
    %2098 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1941)
    %2099 = "ttir.matmul"(%2085, %arg444, %2098) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1941)
    %2100 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1942)
    %2101 = "ttir.reshape"(%2099, %2100) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1942)
    %2102 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc108)
    %2103 = "ttir.add"(%2101, %arg445, %2102) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc108)
    %2104 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1532)
    %2105 = "ttir.reshape"(%2103, %2104) <{shape = [8 : i32, 197 : i32, 16 : i32, 64 : i32]}> : (tensor<8x197x1024xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1532)
    %2106 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1533)
    %2107 = "ttir.transpose"(%2105, %2106) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x197x16x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1533)
    %2108 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1534)
    %2109 = "ttir.reshape"(%2107, %2108) <{shape = [128 : i32, 197 : i32, 64 : i32]}> : (tensor<8x16x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1534)
    %2110 = ttir.empty() : tensor<128x64x197xbf16> loc(#loc109)
    %2111 = "ttir.transpose"(%2109, %2110) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<128x197x64xbf16>, tensor<128x64x197xbf16>) -> tensor<128x64x197xbf16> loc(#loc109)
    %2112 = ttir.empty() : tensor<128x197x197xbf16> loc(#loc1535)
    %2113 = "ttir.matmul"(%2097, %2111, %2112) <{transpose_a = false, transpose_b = false}> : (tensor<128x197x64xbf16>, tensor<128x64x197xbf16>, tensor<128x197x197xbf16>) -> tensor<128x197x197xbf16> loc(#loc1535)
    %2114 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1536)
    %2115 = "ttir.reshape"(%2113, %2114) <{shape = [8 : i32, 16 : i32, 197 : i32, 197 : i32]}> : (tensor<128x197x197xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1536)
    %2116 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1537)
    %2117 = "ttir.multiply"(%2115, %arg124, %2116) : (tensor<8x16x197x197xbf16>, tensor<1xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1537)
    %2118 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1538)
    %2119 = "ttir.add"(%2117, %arg125, %2118) : (tensor<8x16x197x197xbf16>, tensor<197x197xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1538)
    %2120 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1539)
    %2121 = "ttir.softmax"(%2119, %2120) <{dimension = -1 : si32}> : (tensor<8x16x197x197xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1539)
    %2122 = ttir.empty() : tensor<128x197x197xbf16> loc(#loc1540)
    %2123 = "ttir.reshape"(%2121, %2122) <{shape = [128 : i32, 197 : i32, 197 : i32]}> : (tensor<8x16x197x197xbf16>, tensor<128x197x197xbf16>) -> tensor<128x197x197xbf16> loc(#loc1540)
    %2124 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1943)
    %2125 = "ttir.matmul"(%2085, %arg446, %2124) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1943)
    %2126 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1944)
    %2127 = "ttir.reshape"(%2125, %2126) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1944)
    %2128 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc110)
    %2129 = "ttir.add"(%2127, %arg447, %2128) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc110)
    %2130 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1542)
    %2131 = "ttir.reshape"(%2129, %2130) <{shape = [8 : i32, 197 : i32, 16 : i32, 64 : i32]}> : (tensor<8x197x1024xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1542)
    %2132 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1543)
    %2133 = "ttir.transpose"(%2131, %2132) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x197x16x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1543)
    %2134 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1544)
    %2135 = "ttir.reshape"(%2133, %2134) <{shape = [128 : i32, 197 : i32, 64 : i32]}> : (tensor<8x16x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1544)
    %2136 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1545)
    %2137 = "ttir.matmul"(%2123, %2135, %2136) <{transpose_a = false, transpose_b = false}> : (tensor<128x197x197xbf16>, tensor<128x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1545)
    %2138 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1546)
    %2139 = "ttir.reshape"(%2137, %2138) <{shape = [8 : i32, 16 : i32, 197 : i32, 64 : i32]}> : (tensor<128x197x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1546)
    %2140 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1547)
    %2141 = "ttir.transpose"(%2139, %2140) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x16x197x64xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1547)
    %2142 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1945)
    %2143 = "ttir.reshape"(%2141, %2142) <{shape = [1576 : i32, 1024 : i32]}> : (tensor<8x197x16x64xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1945)
    %2144 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1946)
    %2145 = "ttir.matmul"(%2143, %arg448, %2144) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1946)
    %2146 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1947)
    %2147 = "ttir.reshape"(%2145, %2146) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1947)
    %2148 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc111)
    %2149 = "ttir.add"(%2147, %arg449, %2148) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc111)
    %2150 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc315)
    %2151 = "ttir.add"(%2149, %2059, %2150) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc315)
    %2152 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc836)
    %2153 = "ttir.sum"(%2151, %2152) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc836)
    %2154 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc837)
    %2155 = "ttir.multiply"(%arg126, %2153, %2154) : (tensor<8x197x1024xf32>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc837)
    %2156 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc838)
    %2157 = "ttir.subtract"(%2151, %2155, %2156) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc838)
    %2158 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc839)
    %2159 = "ttir.multiply"(%2157, %2157, %2158) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc839)
    %2160 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc840)
    %2161 = "ttir.sum"(%2159, %2160) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc840)
    %2162 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc841)
    %2163 = "ttir.multiply"(%arg127, %2161, %2162) : (tensor<8x197x1xf32>, tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc841)
    %2164 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc842)
    %2165 = "ttir.add"(%2163, %arg128, %2164) : (tensor<8x197x1xbf16>, tensor<8x197x1xf32>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc842)
    %2166 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc843)
    %2167 = "ttir.sqrt"(%2165, %2166) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc843)
    %2168 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc844)
    %2169 = "ttir.reciprocal"(%2167, %2168) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc844)
    %2170 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc845)
    %2171 = "ttir.multiply"(%2157, %2169, %2170) : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc845)
    %2172 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc846)
    %2173 = "ttir.multiply"(%2171, %arg450, %2172) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc846)
    %2174 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc847)
    %2175 = "ttir.add"(%2173, %arg451, %2174) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc847)
    %2176 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1549)
    %2177 = "ttir.reshape"(%2175, %2176) <{shape = [1576 : i32, 1024 : i32]}> : (tensor<8x197x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1549)
    %2178 = ttir.empty() : tensor<1576x4096xbf16> loc(#loc1550)
    %2179 = "ttir.matmul"(%2177, %arg452, %2178) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x4096xbf16>, tensor<1576x4096xbf16>) -> tensor<1576x4096xbf16> loc(#loc1550)
    %2180 = ttir.empty() : tensor<8x197x4096xbf16> loc(#loc1551)
    %2181 = "ttir.reshape"(%2179, %2180) <{shape = [8 : i32, 197 : i32, 4096 : i32]}> : (tensor<1576x4096xbf16>, tensor<8x197x4096xbf16>) -> tensor<8x197x4096xbf16> loc(#loc1551)
    %2182 = ttir.empty() : tensor<8x197x4096xbf16> loc(#loc112)
    %2183 = "ttir.add"(%2181, %arg453, %2182) : (tensor<8x197x4096xbf16>, tensor<4096xbf16>, tensor<8x197x4096xbf16>) -> tensor<8x197x4096xbf16> loc(#loc112)
    %2184 = ttir.empty() : tensor<8x197x4096xbf16> loc(#loc1552)
    %2185 = "ttir.gelu"(%2183, %2184) : (tensor<8x197x4096xbf16>, tensor<8x197x4096xbf16>) -> tensor<8x197x4096xbf16> loc(#loc1552)
    %2186 = ttir.empty() : tensor<1576x4096xbf16> loc(#loc1553)
    %2187 = "ttir.reshape"(%2185, %2186) <{shape = [1576 : i32, 4096 : i32]}> : (tensor<8x197x4096xbf16>, tensor<1576x4096xbf16>) -> tensor<1576x4096xbf16> loc(#loc1553)
    %2188 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1554)
    %2189 = "ttir.matmul"(%2187, %arg454, %2188) <{transpose_a = false, transpose_b = false}> : (tensor<1576x4096xbf16>, tensor<4096x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1554)
    %2190 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1555)
    %2191 = "ttir.reshape"(%2189, %2190) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1555)
    %2192 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc113)
    %2193 = "ttir.add"(%2191, %arg455, %2192) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc113)
    %2194 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc851)
    %2195 = "ttir.add"(%2193, %2151, %2194) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc851)
    %2196 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc852)
    %2197 = "ttir.sum"(%2195, %2196) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc852)
    %2198 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc853)
    %2199 = "ttir.multiply"(%arg129, %2197, %2198) : (tensor<8x197x1024xf32>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc853)
    %2200 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc854)
    %2201 = "ttir.subtract"(%2195, %2199, %2200) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc854)
    %2202 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc855)
    %2203 = "ttir.multiply"(%2201, %2201, %2202) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc855)
    %2204 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc856)
    %2205 = "ttir.sum"(%2203, %2204) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc856)
    %2206 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc857)
    %2207 = "ttir.multiply"(%arg130, %2205, %2206) : (tensor<8x197x1xf32>, tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc857)
    %2208 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc858)
    %2209 = "ttir.add"(%2207, %arg131, %2208) : (tensor<8x197x1xbf16>, tensor<8x197x1xf32>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc858)
    %2210 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc859)
    %2211 = "ttir.sqrt"(%2209, %2210) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc859)
    %2212 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc860)
    %2213 = "ttir.reciprocal"(%2211, %2212) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc860)
    %2214 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc861)
    %2215 = "ttir.multiply"(%2201, %2213, %2214) : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc861)
    %2216 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc862)
    %2217 = "ttir.multiply"(%2215, %arg456, %2216) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc862)
    %2218 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc863)
    %2219 = "ttir.add"(%2217, %arg457, %2218) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc863)
    %2220 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1948)
    %2221 = "ttir.reshape"(%2219, %2220) <{shape = [1576 : i32, 1024 : i32]}> : (tensor<8x197x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1948)
    %2222 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1949)
    %2223 = "ttir.matmul"(%2221, %arg458, %2222) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1949)
    %2224 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1950)
    %2225 = "ttir.reshape"(%2223, %2224) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1950)
    %2226 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc114)
    %2227 = "ttir.add"(%2225, %arg459, %2226) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc114)
    %2228 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1557)
    %2229 = "ttir.reshape"(%2227, %2228) <{shape = [8 : i32, 197 : i32, 16 : i32, 64 : i32]}> : (tensor<8x197x1024xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1557)
    %2230 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1558)
    %2231 = "ttir.transpose"(%2229, %2230) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x197x16x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1558)
    %2232 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1559)
    %2233 = "ttir.reshape"(%2231, %2232) <{shape = [128 : i32, 197 : i32, 64 : i32]}> : (tensor<8x16x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1559)
    %2234 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1951)
    %2235 = "ttir.matmul"(%2221, %arg460, %2234) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1951)
    %2236 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1952)
    %2237 = "ttir.reshape"(%2235, %2236) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1952)
    %2238 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc115)
    %2239 = "ttir.add"(%2237, %arg461, %2238) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc115)
    %2240 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1561)
    %2241 = "ttir.reshape"(%2239, %2240) <{shape = [8 : i32, 197 : i32, 16 : i32, 64 : i32]}> : (tensor<8x197x1024xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1561)
    %2242 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1562)
    %2243 = "ttir.transpose"(%2241, %2242) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x197x16x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1562)
    %2244 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1563)
    %2245 = "ttir.reshape"(%2243, %2244) <{shape = [128 : i32, 197 : i32, 64 : i32]}> : (tensor<8x16x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1563)
    %2246 = ttir.empty() : tensor<128x64x197xbf16> loc(#loc116)
    %2247 = "ttir.transpose"(%2245, %2246) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<128x197x64xbf16>, tensor<128x64x197xbf16>) -> tensor<128x64x197xbf16> loc(#loc116)
    %2248 = ttir.empty() : tensor<128x197x197xbf16> loc(#loc1564)
    %2249 = "ttir.matmul"(%2233, %2247, %2248) <{transpose_a = false, transpose_b = false}> : (tensor<128x197x64xbf16>, tensor<128x64x197xbf16>, tensor<128x197x197xbf16>) -> tensor<128x197x197xbf16> loc(#loc1564)
    %2250 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1565)
    %2251 = "ttir.reshape"(%2249, %2250) <{shape = [8 : i32, 16 : i32, 197 : i32, 197 : i32]}> : (tensor<128x197x197xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1565)
    %2252 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1566)
    %2253 = "ttir.multiply"(%2251, %arg132, %2252) : (tensor<8x16x197x197xbf16>, tensor<1xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1566)
    %2254 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1567)
    %2255 = "ttir.add"(%2253, %arg133, %2254) : (tensor<8x16x197x197xbf16>, tensor<197x197xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1567)
    %2256 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1568)
    %2257 = "ttir.softmax"(%2255, %2256) <{dimension = -1 : si32}> : (tensor<8x16x197x197xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1568)
    %2258 = ttir.empty() : tensor<128x197x197xbf16> loc(#loc1569)
    %2259 = "ttir.reshape"(%2257, %2258) <{shape = [128 : i32, 197 : i32, 197 : i32]}> : (tensor<8x16x197x197xbf16>, tensor<128x197x197xbf16>) -> tensor<128x197x197xbf16> loc(#loc1569)
    %2260 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1953)
    %2261 = "ttir.matmul"(%2221, %arg462, %2260) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1953)
    %2262 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1954)
    %2263 = "ttir.reshape"(%2261, %2262) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1954)
    %2264 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc117)
    %2265 = "ttir.add"(%2263, %arg463, %2264) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc117)
    %2266 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1571)
    %2267 = "ttir.reshape"(%2265, %2266) <{shape = [8 : i32, 197 : i32, 16 : i32, 64 : i32]}> : (tensor<8x197x1024xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1571)
    %2268 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1572)
    %2269 = "ttir.transpose"(%2267, %2268) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x197x16x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1572)
    %2270 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1573)
    %2271 = "ttir.reshape"(%2269, %2270) <{shape = [128 : i32, 197 : i32, 64 : i32]}> : (tensor<8x16x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1573)
    %2272 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1574)
    %2273 = "ttir.matmul"(%2259, %2271, %2272) <{transpose_a = false, transpose_b = false}> : (tensor<128x197x197xbf16>, tensor<128x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1574)
    %2274 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1575)
    %2275 = "ttir.reshape"(%2273, %2274) <{shape = [8 : i32, 16 : i32, 197 : i32, 64 : i32]}> : (tensor<128x197x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1575)
    %2276 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1576)
    %2277 = "ttir.transpose"(%2275, %2276) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x16x197x64xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1576)
    %2278 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1955)
    %2279 = "ttir.reshape"(%2277, %2278) <{shape = [1576 : i32, 1024 : i32]}> : (tensor<8x197x16x64xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1955)
    %2280 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1956)
    %2281 = "ttir.matmul"(%2279, %arg464, %2280) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1956)
    %2282 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1957)
    %2283 = "ttir.reshape"(%2281, %2282) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1957)
    %2284 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc118)
    %2285 = "ttir.add"(%2283, %arg465, %2284) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc118)
    %2286 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc321)
    %2287 = "ttir.add"(%2285, %2195, %2286) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc321)
    %2288 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc866)
    %2289 = "ttir.sum"(%2287, %2288) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc866)
    %2290 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc867)
    %2291 = "ttir.multiply"(%arg134, %2289, %2290) : (tensor<8x197x1024xf32>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc867)
    %2292 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc868)
    %2293 = "ttir.subtract"(%2287, %2291, %2292) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc868)
    %2294 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc869)
    %2295 = "ttir.multiply"(%2293, %2293, %2294) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc869)
    %2296 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc870)
    %2297 = "ttir.sum"(%2295, %2296) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc870)
    %2298 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc871)
    %2299 = "ttir.multiply"(%arg135, %2297, %2298) : (tensor<8x197x1xf32>, tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc871)
    %2300 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc872)
    %2301 = "ttir.add"(%2299, %arg136, %2300) : (tensor<8x197x1xbf16>, tensor<8x197x1xf32>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc872)
    %2302 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc873)
    %2303 = "ttir.sqrt"(%2301, %2302) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc873)
    %2304 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc874)
    %2305 = "ttir.reciprocal"(%2303, %2304) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc874)
    %2306 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc875)
    %2307 = "ttir.multiply"(%2293, %2305, %2306) : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc875)
    %2308 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc876)
    %2309 = "ttir.multiply"(%2307, %arg466, %2308) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc876)
    %2310 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc877)
    %2311 = "ttir.add"(%2309, %arg467, %2310) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc877)
    %2312 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1578)
    %2313 = "ttir.reshape"(%2311, %2312) <{shape = [1576 : i32, 1024 : i32]}> : (tensor<8x197x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1578)
    %2314 = ttir.empty() : tensor<1576x4096xbf16> loc(#loc1579)
    %2315 = "ttir.matmul"(%2313, %arg468, %2314) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x4096xbf16>, tensor<1576x4096xbf16>) -> tensor<1576x4096xbf16> loc(#loc1579)
    %2316 = ttir.empty() : tensor<8x197x4096xbf16> loc(#loc1580)
    %2317 = "ttir.reshape"(%2315, %2316) <{shape = [8 : i32, 197 : i32, 4096 : i32]}> : (tensor<1576x4096xbf16>, tensor<8x197x4096xbf16>) -> tensor<8x197x4096xbf16> loc(#loc1580)
    %2318 = ttir.empty() : tensor<8x197x4096xbf16> loc(#loc119)
    %2319 = "ttir.add"(%2317, %arg469, %2318) : (tensor<8x197x4096xbf16>, tensor<4096xbf16>, tensor<8x197x4096xbf16>) -> tensor<8x197x4096xbf16> loc(#loc119)
    %2320 = ttir.empty() : tensor<8x197x4096xbf16> loc(#loc1581)
    %2321 = "ttir.gelu"(%2319, %2320) : (tensor<8x197x4096xbf16>, tensor<8x197x4096xbf16>) -> tensor<8x197x4096xbf16> loc(#loc1581)
    %2322 = ttir.empty() : tensor<1576x4096xbf16> loc(#loc1582)
    %2323 = "ttir.reshape"(%2321, %2322) <{shape = [1576 : i32, 4096 : i32]}> : (tensor<8x197x4096xbf16>, tensor<1576x4096xbf16>) -> tensor<1576x4096xbf16> loc(#loc1582)
    %2324 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1583)
    %2325 = "ttir.matmul"(%2323, %arg470, %2324) <{transpose_a = false, transpose_b = false}> : (tensor<1576x4096xbf16>, tensor<4096x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1583)
    %2326 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1584)
    %2327 = "ttir.reshape"(%2325, %2326) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1584)
    %2328 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc120)
    %2329 = "ttir.add"(%2327, %arg471, %2328) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc120)
    %2330 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc881)
    %2331 = "ttir.add"(%2329, %2287, %2330) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc881)
    %2332 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc882)
    %2333 = "ttir.sum"(%2331, %2332) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc882)
    %2334 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc883)
    %2335 = "ttir.multiply"(%arg137, %2333, %2334) : (tensor<8x197x1024xf32>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc883)
    %2336 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc884)
    %2337 = "ttir.subtract"(%2331, %2335, %2336) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc884)
    %2338 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc885)
    %2339 = "ttir.multiply"(%2337, %2337, %2338) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc885)
    %2340 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc886)
    %2341 = "ttir.sum"(%2339, %2340) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc886)
    %2342 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc887)
    %2343 = "ttir.multiply"(%arg138, %2341, %2342) : (tensor<8x197x1xf32>, tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc887)
    %2344 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc888)
    %2345 = "ttir.add"(%2343, %arg139, %2344) : (tensor<8x197x1xbf16>, tensor<8x197x1xf32>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc888)
    %2346 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc889)
    %2347 = "ttir.sqrt"(%2345, %2346) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc889)
    %2348 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc890)
    %2349 = "ttir.reciprocal"(%2347, %2348) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc890)
    %2350 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc891)
    %2351 = "ttir.multiply"(%2337, %2349, %2350) : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc891)
    %2352 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc892)
    %2353 = "ttir.multiply"(%2351, %arg472, %2352) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc892)
    %2354 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc893)
    %2355 = "ttir.add"(%2353, %arg473, %2354) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc893)
    %2356 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1958)
    %2357 = "ttir.reshape"(%2355, %2356) <{shape = [1576 : i32, 1024 : i32]}> : (tensor<8x197x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1958)
    %2358 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1959)
    %2359 = "ttir.matmul"(%2357, %arg474, %2358) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1959)
    %2360 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1960)
    %2361 = "ttir.reshape"(%2359, %2360) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1960)
    %2362 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc121)
    %2363 = "ttir.add"(%2361, %arg475, %2362) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc121)
    %2364 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1586)
    %2365 = "ttir.reshape"(%2363, %2364) <{shape = [8 : i32, 197 : i32, 16 : i32, 64 : i32]}> : (tensor<8x197x1024xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1586)
    %2366 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1587)
    %2367 = "ttir.transpose"(%2365, %2366) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x197x16x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1587)
    %2368 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1588)
    %2369 = "ttir.reshape"(%2367, %2368) <{shape = [128 : i32, 197 : i32, 64 : i32]}> : (tensor<8x16x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1588)
    %2370 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1961)
    %2371 = "ttir.matmul"(%2357, %arg476, %2370) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1961)
    %2372 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1962)
    %2373 = "ttir.reshape"(%2371, %2372) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1962)
    %2374 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc122)
    %2375 = "ttir.add"(%2373, %arg477, %2374) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc122)
    %2376 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1590)
    %2377 = "ttir.reshape"(%2375, %2376) <{shape = [8 : i32, 197 : i32, 16 : i32, 64 : i32]}> : (tensor<8x197x1024xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1590)
    %2378 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1591)
    %2379 = "ttir.transpose"(%2377, %2378) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x197x16x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1591)
    %2380 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1592)
    %2381 = "ttir.reshape"(%2379, %2380) <{shape = [128 : i32, 197 : i32, 64 : i32]}> : (tensor<8x16x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1592)
    %2382 = ttir.empty() : tensor<128x64x197xbf16> loc(#loc123)
    %2383 = "ttir.transpose"(%2381, %2382) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<128x197x64xbf16>, tensor<128x64x197xbf16>) -> tensor<128x64x197xbf16> loc(#loc123)
    %2384 = ttir.empty() : tensor<128x197x197xbf16> loc(#loc1593)
    %2385 = "ttir.matmul"(%2369, %2383, %2384) <{transpose_a = false, transpose_b = false}> : (tensor<128x197x64xbf16>, tensor<128x64x197xbf16>, tensor<128x197x197xbf16>) -> tensor<128x197x197xbf16> loc(#loc1593)
    %2386 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1594)
    %2387 = "ttir.reshape"(%2385, %2386) <{shape = [8 : i32, 16 : i32, 197 : i32, 197 : i32]}> : (tensor<128x197x197xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1594)
    %2388 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1595)
    %2389 = "ttir.multiply"(%2387, %arg140, %2388) : (tensor<8x16x197x197xbf16>, tensor<1xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1595)
    %2390 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1596)
    %2391 = "ttir.add"(%2389, %arg141, %2390) : (tensor<8x16x197x197xbf16>, tensor<197x197xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1596)
    %2392 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1597)
    %2393 = "ttir.softmax"(%2391, %2392) <{dimension = -1 : si32}> : (tensor<8x16x197x197xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1597)
    %2394 = ttir.empty() : tensor<128x197x197xbf16> loc(#loc1598)
    %2395 = "ttir.reshape"(%2393, %2394) <{shape = [128 : i32, 197 : i32, 197 : i32]}> : (tensor<8x16x197x197xbf16>, tensor<128x197x197xbf16>) -> tensor<128x197x197xbf16> loc(#loc1598)
    %2396 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1963)
    %2397 = "ttir.matmul"(%2357, %arg478, %2396) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1963)
    %2398 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1964)
    %2399 = "ttir.reshape"(%2397, %2398) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1964)
    %2400 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc124)
    %2401 = "ttir.add"(%2399, %arg479, %2400) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc124)
    %2402 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1600)
    %2403 = "ttir.reshape"(%2401, %2402) <{shape = [8 : i32, 197 : i32, 16 : i32, 64 : i32]}> : (tensor<8x197x1024xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1600)
    %2404 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1601)
    %2405 = "ttir.transpose"(%2403, %2404) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x197x16x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1601)
    %2406 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1602)
    %2407 = "ttir.reshape"(%2405, %2406) <{shape = [128 : i32, 197 : i32, 64 : i32]}> : (tensor<8x16x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1602)
    %2408 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1603)
    %2409 = "ttir.matmul"(%2395, %2407, %2408) <{transpose_a = false, transpose_b = false}> : (tensor<128x197x197xbf16>, tensor<128x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1603)
    %2410 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1604)
    %2411 = "ttir.reshape"(%2409, %2410) <{shape = [8 : i32, 16 : i32, 197 : i32, 64 : i32]}> : (tensor<128x197x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1604)
    %2412 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1605)
    %2413 = "ttir.transpose"(%2411, %2412) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x16x197x64xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1605)
    %2414 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1965)
    %2415 = "ttir.reshape"(%2413, %2414) <{shape = [1576 : i32, 1024 : i32]}> : (tensor<8x197x16x64xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1965)
    %2416 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1966)
    %2417 = "ttir.matmul"(%2415, %arg480, %2416) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1966)
    %2418 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1967)
    %2419 = "ttir.reshape"(%2417, %2418) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1967)
    %2420 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc125)
    %2421 = "ttir.add"(%2419, %arg481, %2420) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc125)
    %2422 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc327)
    %2423 = "ttir.add"(%2421, %2331, %2422) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc327)
    %2424 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc896)
    %2425 = "ttir.sum"(%2423, %2424) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc896)
    %2426 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc897)
    %2427 = "ttir.multiply"(%arg142, %2425, %2426) : (tensor<8x197x1024xf32>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc897)
    %2428 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc898)
    %2429 = "ttir.subtract"(%2423, %2427, %2428) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc898)
    %2430 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc899)
    %2431 = "ttir.multiply"(%2429, %2429, %2430) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc899)
    %2432 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc900)
    %2433 = "ttir.sum"(%2431, %2432) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc900)
    %2434 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc901)
    %2435 = "ttir.multiply"(%arg143, %2433, %2434) : (tensor<8x197x1xf32>, tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc901)
    %2436 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc902)
    %2437 = "ttir.add"(%2435, %arg144, %2436) : (tensor<8x197x1xbf16>, tensor<8x197x1xf32>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc902)
    %2438 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc903)
    %2439 = "ttir.sqrt"(%2437, %2438) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc903)
    %2440 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc904)
    %2441 = "ttir.reciprocal"(%2439, %2440) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc904)
    %2442 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc905)
    %2443 = "ttir.multiply"(%2429, %2441, %2442) : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc905)
    %2444 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc906)
    %2445 = "ttir.multiply"(%2443, %arg482, %2444) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc906)
    %2446 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc907)
    %2447 = "ttir.add"(%2445, %arg483, %2446) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc907)
    %2448 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1607)
    %2449 = "ttir.reshape"(%2447, %2448) <{shape = [1576 : i32, 1024 : i32]}> : (tensor<8x197x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1607)
    %2450 = ttir.empty() : tensor<1576x4096xbf16> loc(#loc1608)
    %2451 = "ttir.matmul"(%2449, %arg484, %2450) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x4096xbf16>, tensor<1576x4096xbf16>) -> tensor<1576x4096xbf16> loc(#loc1608)
    %2452 = ttir.empty() : tensor<8x197x4096xbf16> loc(#loc1609)
    %2453 = "ttir.reshape"(%2451, %2452) <{shape = [8 : i32, 197 : i32, 4096 : i32]}> : (tensor<1576x4096xbf16>, tensor<8x197x4096xbf16>) -> tensor<8x197x4096xbf16> loc(#loc1609)
    %2454 = ttir.empty() : tensor<8x197x4096xbf16> loc(#loc126)
    %2455 = "ttir.add"(%2453, %arg485, %2454) : (tensor<8x197x4096xbf16>, tensor<4096xbf16>, tensor<8x197x4096xbf16>) -> tensor<8x197x4096xbf16> loc(#loc126)
    %2456 = ttir.empty() : tensor<8x197x4096xbf16> loc(#loc1610)
    %2457 = "ttir.gelu"(%2455, %2456) : (tensor<8x197x4096xbf16>, tensor<8x197x4096xbf16>) -> tensor<8x197x4096xbf16> loc(#loc1610)
    %2458 = ttir.empty() : tensor<1576x4096xbf16> loc(#loc1611)
    %2459 = "ttir.reshape"(%2457, %2458) <{shape = [1576 : i32, 4096 : i32]}> : (tensor<8x197x4096xbf16>, tensor<1576x4096xbf16>) -> tensor<1576x4096xbf16> loc(#loc1611)
    %2460 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1612)
    %2461 = "ttir.matmul"(%2459, %arg486, %2460) <{transpose_a = false, transpose_b = false}> : (tensor<1576x4096xbf16>, tensor<4096x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1612)
    %2462 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1613)
    %2463 = "ttir.reshape"(%2461, %2462) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1613)
    %2464 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc127)
    %2465 = "ttir.add"(%2463, %arg487, %2464) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc127)
    %2466 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc911)
    %2467 = "ttir.add"(%2465, %2423, %2466) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc911)
    %2468 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc912)
    %2469 = "ttir.sum"(%2467, %2468) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc912)
    %2470 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc913)
    %2471 = "ttir.multiply"(%arg145, %2469, %2470) : (tensor<8x197x1024xf32>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc913)
    %2472 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc914)
    %2473 = "ttir.subtract"(%2467, %2471, %2472) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc914)
    %2474 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc915)
    %2475 = "ttir.multiply"(%2473, %2473, %2474) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc915)
    %2476 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc916)
    %2477 = "ttir.sum"(%2475, %2476) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc916)
    %2478 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc917)
    %2479 = "ttir.multiply"(%arg146, %2477, %2478) : (tensor<8x197x1xf32>, tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc917)
    %2480 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc918)
    %2481 = "ttir.add"(%2479, %arg147, %2480) : (tensor<8x197x1xbf16>, tensor<8x197x1xf32>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc918)
    %2482 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc919)
    %2483 = "ttir.sqrt"(%2481, %2482) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc919)
    %2484 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc920)
    %2485 = "ttir.reciprocal"(%2483, %2484) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc920)
    %2486 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc921)
    %2487 = "ttir.multiply"(%2473, %2485, %2486) : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc921)
    %2488 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc922)
    %2489 = "ttir.multiply"(%2487, %arg488, %2488) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc922)
    %2490 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc923)
    %2491 = "ttir.add"(%2489, %arg489, %2490) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc923)
    %2492 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1968)
    %2493 = "ttir.reshape"(%2491, %2492) <{shape = [1576 : i32, 1024 : i32]}> : (tensor<8x197x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1968)
    %2494 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1969)
    %2495 = "ttir.matmul"(%2493, %arg490, %2494) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1969)
    %2496 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1970)
    %2497 = "ttir.reshape"(%2495, %2496) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1970)
    %2498 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc128)
    %2499 = "ttir.add"(%2497, %arg491, %2498) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc128)
    %2500 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1615)
    %2501 = "ttir.reshape"(%2499, %2500) <{shape = [8 : i32, 197 : i32, 16 : i32, 64 : i32]}> : (tensor<8x197x1024xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1615)
    %2502 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1616)
    %2503 = "ttir.transpose"(%2501, %2502) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x197x16x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1616)
    %2504 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1617)
    %2505 = "ttir.reshape"(%2503, %2504) <{shape = [128 : i32, 197 : i32, 64 : i32]}> : (tensor<8x16x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1617)
    %2506 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1971)
    %2507 = "ttir.matmul"(%2493, %arg492, %2506) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1971)
    %2508 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1972)
    %2509 = "ttir.reshape"(%2507, %2508) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1972)
    %2510 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc129)
    %2511 = "ttir.add"(%2509, %arg493, %2510) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc129)
    %2512 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1619)
    %2513 = "ttir.reshape"(%2511, %2512) <{shape = [8 : i32, 197 : i32, 16 : i32, 64 : i32]}> : (tensor<8x197x1024xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1619)
    %2514 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1620)
    %2515 = "ttir.transpose"(%2513, %2514) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x197x16x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1620)
    %2516 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1621)
    %2517 = "ttir.reshape"(%2515, %2516) <{shape = [128 : i32, 197 : i32, 64 : i32]}> : (tensor<8x16x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1621)
    %2518 = ttir.empty() : tensor<128x64x197xbf16> loc(#loc130)
    %2519 = "ttir.transpose"(%2517, %2518) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<128x197x64xbf16>, tensor<128x64x197xbf16>) -> tensor<128x64x197xbf16> loc(#loc130)
    %2520 = ttir.empty() : tensor<128x197x197xbf16> loc(#loc1622)
    %2521 = "ttir.matmul"(%2505, %2519, %2520) <{transpose_a = false, transpose_b = false}> : (tensor<128x197x64xbf16>, tensor<128x64x197xbf16>, tensor<128x197x197xbf16>) -> tensor<128x197x197xbf16> loc(#loc1622)
    %2522 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1623)
    %2523 = "ttir.reshape"(%2521, %2522) <{shape = [8 : i32, 16 : i32, 197 : i32, 197 : i32]}> : (tensor<128x197x197xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1623)
    %2524 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1624)
    %2525 = "ttir.multiply"(%2523, %arg148, %2524) : (tensor<8x16x197x197xbf16>, tensor<1xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1624)
    %2526 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1625)
    %2527 = "ttir.add"(%2525, %arg149, %2526) : (tensor<8x16x197x197xbf16>, tensor<197x197xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1625)
    %2528 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1626)
    %2529 = "ttir.softmax"(%2527, %2528) <{dimension = -1 : si32}> : (tensor<8x16x197x197xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1626)
    %2530 = ttir.empty() : tensor<128x197x197xbf16> loc(#loc1627)
    %2531 = "ttir.reshape"(%2529, %2530) <{shape = [128 : i32, 197 : i32, 197 : i32]}> : (tensor<8x16x197x197xbf16>, tensor<128x197x197xbf16>) -> tensor<128x197x197xbf16> loc(#loc1627)
    %2532 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1973)
    %2533 = "ttir.matmul"(%2493, %arg494, %2532) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1973)
    %2534 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1974)
    %2535 = "ttir.reshape"(%2533, %2534) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1974)
    %2536 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc131)
    %2537 = "ttir.add"(%2535, %arg495, %2536) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc131)
    %2538 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1629)
    %2539 = "ttir.reshape"(%2537, %2538) <{shape = [8 : i32, 197 : i32, 16 : i32, 64 : i32]}> : (tensor<8x197x1024xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1629)
    %2540 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1630)
    %2541 = "ttir.transpose"(%2539, %2540) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x197x16x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1630)
    %2542 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1631)
    %2543 = "ttir.reshape"(%2541, %2542) <{shape = [128 : i32, 197 : i32, 64 : i32]}> : (tensor<8x16x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1631)
    %2544 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1632)
    %2545 = "ttir.matmul"(%2531, %2543, %2544) <{transpose_a = false, transpose_b = false}> : (tensor<128x197x197xbf16>, tensor<128x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1632)
    %2546 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1633)
    %2547 = "ttir.reshape"(%2545, %2546) <{shape = [8 : i32, 16 : i32, 197 : i32, 64 : i32]}> : (tensor<128x197x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1633)
    %2548 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1634)
    %2549 = "ttir.transpose"(%2547, %2548) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x16x197x64xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1634)
    %2550 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1975)
    %2551 = "ttir.reshape"(%2549, %2550) <{shape = [1576 : i32, 1024 : i32]}> : (tensor<8x197x16x64xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1975)
    %2552 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1976)
    %2553 = "ttir.matmul"(%2551, %arg496, %2552) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1976)
    %2554 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1977)
    %2555 = "ttir.reshape"(%2553, %2554) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1977)
    %2556 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc132)
    %2557 = "ttir.add"(%2555, %arg497, %2556) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc132)
    %2558 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc333)
    %2559 = "ttir.add"(%2557, %2467, %2558) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc333)
    %2560 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc926)
    %2561 = "ttir.sum"(%2559, %2560) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc926)
    %2562 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc927)
    %2563 = "ttir.multiply"(%arg150, %2561, %2562) : (tensor<8x197x1024xf32>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc927)
    %2564 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc928)
    %2565 = "ttir.subtract"(%2559, %2563, %2564) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc928)
    %2566 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc929)
    %2567 = "ttir.multiply"(%2565, %2565, %2566) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc929)
    %2568 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc930)
    %2569 = "ttir.sum"(%2567, %2568) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc930)
    %2570 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc931)
    %2571 = "ttir.multiply"(%arg151, %2569, %2570) : (tensor<8x197x1xf32>, tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc931)
    %2572 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc932)
    %2573 = "ttir.add"(%2571, %arg152, %2572) : (tensor<8x197x1xbf16>, tensor<8x197x1xf32>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc932)
    %2574 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc933)
    %2575 = "ttir.sqrt"(%2573, %2574) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc933)
    %2576 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc934)
    %2577 = "ttir.reciprocal"(%2575, %2576) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc934)
    %2578 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc935)
    %2579 = "ttir.multiply"(%2565, %2577, %2578) : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc935)
    %2580 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc936)
    %2581 = "ttir.multiply"(%2579, %arg498, %2580) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc936)
    %2582 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc937)
    %2583 = "ttir.add"(%2581, %arg499, %2582) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc937)
    %2584 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1636)
    %2585 = "ttir.reshape"(%2583, %2584) <{shape = [1576 : i32, 1024 : i32]}> : (tensor<8x197x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1636)
    %2586 = ttir.empty() : tensor<1576x4096xbf16> loc(#loc1637)
    %2587 = "ttir.matmul"(%2585, %arg500, %2586) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x4096xbf16>, tensor<1576x4096xbf16>) -> tensor<1576x4096xbf16> loc(#loc1637)
    %2588 = ttir.empty() : tensor<8x197x4096xbf16> loc(#loc1638)
    %2589 = "ttir.reshape"(%2587, %2588) <{shape = [8 : i32, 197 : i32, 4096 : i32]}> : (tensor<1576x4096xbf16>, tensor<8x197x4096xbf16>) -> tensor<8x197x4096xbf16> loc(#loc1638)
    %2590 = ttir.empty() : tensor<8x197x4096xbf16> loc(#loc133)
    %2591 = "ttir.add"(%2589, %arg501, %2590) : (tensor<8x197x4096xbf16>, tensor<4096xbf16>, tensor<8x197x4096xbf16>) -> tensor<8x197x4096xbf16> loc(#loc133)
    %2592 = ttir.empty() : tensor<8x197x4096xbf16> loc(#loc1639)
    %2593 = "ttir.gelu"(%2591, %2592) : (tensor<8x197x4096xbf16>, tensor<8x197x4096xbf16>) -> tensor<8x197x4096xbf16> loc(#loc1639)
    %2594 = ttir.empty() : tensor<1576x4096xbf16> loc(#loc1640)
    %2595 = "ttir.reshape"(%2593, %2594) <{shape = [1576 : i32, 4096 : i32]}> : (tensor<8x197x4096xbf16>, tensor<1576x4096xbf16>) -> tensor<1576x4096xbf16> loc(#loc1640)
    %2596 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1641)
    %2597 = "ttir.matmul"(%2595, %arg502, %2596) <{transpose_a = false, transpose_b = false}> : (tensor<1576x4096xbf16>, tensor<4096x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1641)
    %2598 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1642)
    %2599 = "ttir.reshape"(%2597, %2598) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1642)
    %2600 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc134)
    %2601 = "ttir.add"(%2599, %arg503, %2600) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc134)
    %2602 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc941)
    %2603 = "ttir.add"(%2601, %2559, %2602) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc941)
    %2604 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc942)
    %2605 = "ttir.sum"(%2603, %2604) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc942)
    %2606 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc943)
    %2607 = "ttir.multiply"(%arg153, %2605, %2606) : (tensor<8x197x1024xf32>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc943)
    %2608 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc944)
    %2609 = "ttir.subtract"(%2603, %2607, %2608) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc944)
    %2610 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc945)
    %2611 = "ttir.multiply"(%2609, %2609, %2610) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc945)
    %2612 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc946)
    %2613 = "ttir.sum"(%2611, %2612) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc946)
    %2614 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc947)
    %2615 = "ttir.multiply"(%arg154, %2613, %2614) : (tensor<8x197x1xf32>, tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc947)
    %2616 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc948)
    %2617 = "ttir.add"(%2615, %arg155, %2616) : (tensor<8x197x1xbf16>, tensor<8x197x1xf32>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc948)
    %2618 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc949)
    %2619 = "ttir.sqrt"(%2617, %2618) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc949)
    %2620 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc950)
    %2621 = "ttir.reciprocal"(%2619, %2620) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc950)
    %2622 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc951)
    %2623 = "ttir.multiply"(%2609, %2621, %2622) : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc951)
    %2624 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc952)
    %2625 = "ttir.multiply"(%2623, %arg504, %2624) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc952)
    %2626 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc953)
    %2627 = "ttir.add"(%2625, %arg505, %2626) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc953)
    %2628 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1978)
    %2629 = "ttir.reshape"(%2627, %2628) <{shape = [1576 : i32, 1024 : i32]}> : (tensor<8x197x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1978)
    %2630 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1979)
    %2631 = "ttir.matmul"(%2629, %arg506, %2630) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1979)
    %2632 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1980)
    %2633 = "ttir.reshape"(%2631, %2632) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1980)
    %2634 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc135)
    %2635 = "ttir.add"(%2633, %arg507, %2634) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc135)
    %2636 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1644)
    %2637 = "ttir.reshape"(%2635, %2636) <{shape = [8 : i32, 197 : i32, 16 : i32, 64 : i32]}> : (tensor<8x197x1024xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1644)
    %2638 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1645)
    %2639 = "ttir.transpose"(%2637, %2638) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x197x16x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1645)
    %2640 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1646)
    %2641 = "ttir.reshape"(%2639, %2640) <{shape = [128 : i32, 197 : i32, 64 : i32]}> : (tensor<8x16x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1646)
    %2642 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1981)
    %2643 = "ttir.matmul"(%2629, %arg508, %2642) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1981)
    %2644 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1982)
    %2645 = "ttir.reshape"(%2643, %2644) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1982)
    %2646 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc136)
    %2647 = "ttir.add"(%2645, %arg509, %2646) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc136)
    %2648 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1648)
    %2649 = "ttir.reshape"(%2647, %2648) <{shape = [8 : i32, 197 : i32, 16 : i32, 64 : i32]}> : (tensor<8x197x1024xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1648)
    %2650 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1649)
    %2651 = "ttir.transpose"(%2649, %2650) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x197x16x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1649)
    %2652 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1650)
    %2653 = "ttir.reshape"(%2651, %2652) <{shape = [128 : i32, 197 : i32, 64 : i32]}> : (tensor<8x16x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1650)
    %2654 = ttir.empty() : tensor<128x64x197xbf16> loc(#loc137)
    %2655 = "ttir.transpose"(%2653, %2654) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<128x197x64xbf16>, tensor<128x64x197xbf16>) -> tensor<128x64x197xbf16> loc(#loc137)
    %2656 = ttir.empty() : tensor<128x197x197xbf16> loc(#loc1651)
    %2657 = "ttir.matmul"(%2641, %2655, %2656) <{transpose_a = false, transpose_b = false}> : (tensor<128x197x64xbf16>, tensor<128x64x197xbf16>, tensor<128x197x197xbf16>) -> tensor<128x197x197xbf16> loc(#loc1651)
    %2658 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1652)
    %2659 = "ttir.reshape"(%2657, %2658) <{shape = [8 : i32, 16 : i32, 197 : i32, 197 : i32]}> : (tensor<128x197x197xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1652)
    %2660 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1653)
    %2661 = "ttir.multiply"(%2659, %arg156, %2660) : (tensor<8x16x197x197xbf16>, tensor<1xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1653)
    %2662 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1654)
    %2663 = "ttir.add"(%2661, %arg157, %2662) : (tensor<8x16x197x197xbf16>, tensor<197x197xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1654)
    %2664 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1655)
    %2665 = "ttir.softmax"(%2663, %2664) <{dimension = -1 : si32}> : (tensor<8x16x197x197xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1655)
    %2666 = ttir.empty() : tensor<128x197x197xbf16> loc(#loc1656)
    %2667 = "ttir.reshape"(%2665, %2666) <{shape = [128 : i32, 197 : i32, 197 : i32]}> : (tensor<8x16x197x197xbf16>, tensor<128x197x197xbf16>) -> tensor<128x197x197xbf16> loc(#loc1656)
    %2668 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1983)
    %2669 = "ttir.matmul"(%2629, %arg510, %2668) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1983)
    %2670 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1984)
    %2671 = "ttir.reshape"(%2669, %2670) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1984)
    %2672 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc138)
    %2673 = "ttir.add"(%2671, %arg511, %2672) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc138)
    %2674 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1658)
    %2675 = "ttir.reshape"(%2673, %2674) <{shape = [8 : i32, 197 : i32, 16 : i32, 64 : i32]}> : (tensor<8x197x1024xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1658)
    %2676 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1659)
    %2677 = "ttir.transpose"(%2675, %2676) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x197x16x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1659)
    %2678 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1660)
    %2679 = "ttir.reshape"(%2677, %2678) <{shape = [128 : i32, 197 : i32, 64 : i32]}> : (tensor<8x16x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1660)
    %2680 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1661)
    %2681 = "ttir.matmul"(%2667, %2679, %2680) <{transpose_a = false, transpose_b = false}> : (tensor<128x197x197xbf16>, tensor<128x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1661)
    %2682 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1662)
    %2683 = "ttir.reshape"(%2681, %2682) <{shape = [8 : i32, 16 : i32, 197 : i32, 64 : i32]}> : (tensor<128x197x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1662)
    %2684 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1663)
    %2685 = "ttir.transpose"(%2683, %2684) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x16x197x64xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1663)
    %2686 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1985)
    %2687 = "ttir.reshape"(%2685, %2686) <{shape = [1576 : i32, 1024 : i32]}> : (tensor<8x197x16x64xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1985)
    %2688 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1986)
    %2689 = "ttir.matmul"(%2687, %arg512, %2688) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1986)
    %2690 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1987)
    %2691 = "ttir.reshape"(%2689, %2690) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1987)
    %2692 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc139)
    %2693 = "ttir.add"(%2691, %arg513, %2692) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc139)
    %2694 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc339)
    %2695 = "ttir.add"(%2693, %2603, %2694) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc339)
    %2696 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc956)
    %2697 = "ttir.sum"(%2695, %2696) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc956)
    %2698 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc957)
    %2699 = "ttir.multiply"(%arg158, %2697, %2698) : (tensor<8x197x1024xf32>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc957)
    %2700 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc958)
    %2701 = "ttir.subtract"(%2695, %2699, %2700) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc958)
    %2702 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc959)
    %2703 = "ttir.multiply"(%2701, %2701, %2702) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc959)
    %2704 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc960)
    %2705 = "ttir.sum"(%2703, %2704) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc960)
    %2706 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc961)
    %2707 = "ttir.multiply"(%arg159, %2705, %2706) : (tensor<8x197x1xf32>, tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc961)
    %2708 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc962)
    %2709 = "ttir.add"(%2707, %arg160, %2708) : (tensor<8x197x1xbf16>, tensor<8x197x1xf32>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc962)
    %2710 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc963)
    %2711 = "ttir.sqrt"(%2709, %2710) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc963)
    %2712 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc964)
    %2713 = "ttir.reciprocal"(%2711, %2712) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc964)
    %2714 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc965)
    %2715 = "ttir.multiply"(%2701, %2713, %2714) : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc965)
    %2716 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc966)
    %2717 = "ttir.multiply"(%2715, %arg514, %2716) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc966)
    %2718 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc967)
    %2719 = "ttir.add"(%2717, %arg515, %2718) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc967)
    %2720 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1665)
    %2721 = "ttir.reshape"(%2719, %2720) <{shape = [1576 : i32, 1024 : i32]}> : (tensor<8x197x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1665)
    %2722 = ttir.empty() : tensor<1576x4096xbf16> loc(#loc1666)
    %2723 = "ttir.matmul"(%2721, %arg516, %2722) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x4096xbf16>, tensor<1576x4096xbf16>) -> tensor<1576x4096xbf16> loc(#loc1666)
    %2724 = ttir.empty() : tensor<8x197x4096xbf16> loc(#loc1667)
    %2725 = "ttir.reshape"(%2723, %2724) <{shape = [8 : i32, 197 : i32, 4096 : i32]}> : (tensor<1576x4096xbf16>, tensor<8x197x4096xbf16>) -> tensor<8x197x4096xbf16> loc(#loc1667)
    %2726 = ttir.empty() : tensor<8x197x4096xbf16> loc(#loc140)
    %2727 = "ttir.add"(%2725, %arg517, %2726) : (tensor<8x197x4096xbf16>, tensor<4096xbf16>, tensor<8x197x4096xbf16>) -> tensor<8x197x4096xbf16> loc(#loc140)
    %2728 = ttir.empty() : tensor<8x197x4096xbf16> loc(#loc1668)
    %2729 = "ttir.gelu"(%2727, %2728) : (tensor<8x197x4096xbf16>, tensor<8x197x4096xbf16>) -> tensor<8x197x4096xbf16> loc(#loc1668)
    %2730 = ttir.empty() : tensor<1576x4096xbf16> loc(#loc1669)
    %2731 = "ttir.reshape"(%2729, %2730) <{shape = [1576 : i32, 4096 : i32]}> : (tensor<8x197x4096xbf16>, tensor<1576x4096xbf16>) -> tensor<1576x4096xbf16> loc(#loc1669)
    %2732 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1670)
    %2733 = "ttir.matmul"(%2731, %arg518, %2732) <{transpose_a = false, transpose_b = false}> : (tensor<1576x4096xbf16>, tensor<4096x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1670)
    %2734 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1671)
    %2735 = "ttir.reshape"(%2733, %2734) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1671)
    %2736 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc141)
    %2737 = "ttir.add"(%2735, %arg519, %2736) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc141)
    %2738 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc971)
    %2739 = "ttir.add"(%2737, %2695, %2738) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc971)
    %2740 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc972)
    %2741 = "ttir.sum"(%2739, %2740) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc972)
    %2742 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc973)
    %2743 = "ttir.multiply"(%arg161, %2741, %2742) : (tensor<8x197x1024xf32>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc973)
    %2744 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc974)
    %2745 = "ttir.subtract"(%2739, %2743, %2744) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc974)
    %2746 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc975)
    %2747 = "ttir.multiply"(%2745, %2745, %2746) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc975)
    %2748 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc976)
    %2749 = "ttir.sum"(%2747, %2748) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc976)
    %2750 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc977)
    %2751 = "ttir.multiply"(%arg162, %2749, %2750) : (tensor<8x197x1xf32>, tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc977)
    %2752 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc978)
    %2753 = "ttir.add"(%2751, %arg163, %2752) : (tensor<8x197x1xbf16>, tensor<8x197x1xf32>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc978)
    %2754 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc979)
    %2755 = "ttir.sqrt"(%2753, %2754) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc979)
    %2756 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc980)
    %2757 = "ttir.reciprocal"(%2755, %2756) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc980)
    %2758 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc981)
    %2759 = "ttir.multiply"(%2745, %2757, %2758) : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc981)
    %2760 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc982)
    %2761 = "ttir.multiply"(%2759, %arg520, %2760) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc982)
    %2762 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc983)
    %2763 = "ttir.add"(%2761, %arg521, %2762) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc983)
    %2764 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1988)
    %2765 = "ttir.reshape"(%2763, %2764) <{shape = [1576 : i32, 1024 : i32]}> : (tensor<8x197x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1988)
    %2766 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1989)
    %2767 = "ttir.matmul"(%2765, %arg522, %2766) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1989)
    %2768 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1990)
    %2769 = "ttir.reshape"(%2767, %2768) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1990)
    %2770 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc142)
    %2771 = "ttir.add"(%2769, %arg523, %2770) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc142)
    %2772 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1673)
    %2773 = "ttir.reshape"(%2771, %2772) <{shape = [8 : i32, 197 : i32, 16 : i32, 64 : i32]}> : (tensor<8x197x1024xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1673)
    %2774 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1674)
    %2775 = "ttir.transpose"(%2773, %2774) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x197x16x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1674)
    %2776 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1675)
    %2777 = "ttir.reshape"(%2775, %2776) <{shape = [128 : i32, 197 : i32, 64 : i32]}> : (tensor<8x16x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1675)
    %2778 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1991)
    %2779 = "ttir.matmul"(%2765, %arg524, %2778) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1991)
    %2780 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1992)
    %2781 = "ttir.reshape"(%2779, %2780) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1992)
    %2782 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc143)
    %2783 = "ttir.add"(%2781, %arg525, %2782) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc143)
    %2784 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1677)
    %2785 = "ttir.reshape"(%2783, %2784) <{shape = [8 : i32, 197 : i32, 16 : i32, 64 : i32]}> : (tensor<8x197x1024xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1677)
    %2786 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1678)
    %2787 = "ttir.transpose"(%2785, %2786) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x197x16x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1678)
    %2788 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1679)
    %2789 = "ttir.reshape"(%2787, %2788) <{shape = [128 : i32, 197 : i32, 64 : i32]}> : (tensor<8x16x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1679)
    %2790 = ttir.empty() : tensor<128x64x197xbf16> loc(#loc144)
    %2791 = "ttir.transpose"(%2789, %2790) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<128x197x64xbf16>, tensor<128x64x197xbf16>) -> tensor<128x64x197xbf16> loc(#loc144)
    %2792 = ttir.empty() : tensor<128x197x197xbf16> loc(#loc1680)
    %2793 = "ttir.matmul"(%2777, %2791, %2792) <{transpose_a = false, transpose_b = false}> : (tensor<128x197x64xbf16>, tensor<128x64x197xbf16>, tensor<128x197x197xbf16>) -> tensor<128x197x197xbf16> loc(#loc1680)
    %2794 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1681)
    %2795 = "ttir.reshape"(%2793, %2794) <{shape = [8 : i32, 16 : i32, 197 : i32, 197 : i32]}> : (tensor<128x197x197xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1681)
    %2796 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1682)
    %2797 = "ttir.multiply"(%2795, %arg164, %2796) : (tensor<8x16x197x197xbf16>, tensor<1xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1682)
    %2798 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1683)
    %2799 = "ttir.add"(%2797, %arg165, %2798) : (tensor<8x16x197x197xbf16>, tensor<197x197xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1683)
    %2800 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1684)
    %2801 = "ttir.softmax"(%2799, %2800) <{dimension = -1 : si32}> : (tensor<8x16x197x197xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1684)
    %2802 = ttir.empty() : tensor<128x197x197xbf16> loc(#loc1685)
    %2803 = "ttir.reshape"(%2801, %2802) <{shape = [128 : i32, 197 : i32, 197 : i32]}> : (tensor<8x16x197x197xbf16>, tensor<128x197x197xbf16>) -> tensor<128x197x197xbf16> loc(#loc1685)
    %2804 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1993)
    %2805 = "ttir.matmul"(%2765, %arg526, %2804) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1993)
    %2806 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1994)
    %2807 = "ttir.reshape"(%2805, %2806) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1994)
    %2808 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc145)
    %2809 = "ttir.add"(%2807, %arg527, %2808) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc145)
    %2810 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1687)
    %2811 = "ttir.reshape"(%2809, %2810) <{shape = [8 : i32, 197 : i32, 16 : i32, 64 : i32]}> : (tensor<8x197x1024xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1687)
    %2812 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1688)
    %2813 = "ttir.transpose"(%2811, %2812) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x197x16x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1688)
    %2814 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1689)
    %2815 = "ttir.reshape"(%2813, %2814) <{shape = [128 : i32, 197 : i32, 64 : i32]}> : (tensor<8x16x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1689)
    %2816 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1690)
    %2817 = "ttir.matmul"(%2803, %2815, %2816) <{transpose_a = false, transpose_b = false}> : (tensor<128x197x197xbf16>, tensor<128x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1690)
    %2818 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1691)
    %2819 = "ttir.reshape"(%2817, %2818) <{shape = [8 : i32, 16 : i32, 197 : i32, 64 : i32]}> : (tensor<128x197x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1691)
    %2820 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1692)
    %2821 = "ttir.transpose"(%2819, %2820) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x16x197x64xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1692)
    %2822 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1995)
    %2823 = "ttir.reshape"(%2821, %2822) <{shape = [1576 : i32, 1024 : i32]}> : (tensor<8x197x16x64xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1995)
    %2824 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1996)
    %2825 = "ttir.matmul"(%2823, %arg528, %2824) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1996)
    %2826 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1997)
    %2827 = "ttir.reshape"(%2825, %2826) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1997)
    %2828 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc146)
    %2829 = "ttir.add"(%2827, %arg529, %2828) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc146)
    %2830 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc345)
    %2831 = "ttir.add"(%2829, %2739, %2830) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc345)
    %2832 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc986)
    %2833 = "ttir.sum"(%2831, %2832) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc986)
    %2834 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc987)
    %2835 = "ttir.multiply"(%arg166, %2833, %2834) : (tensor<8x197x1024xf32>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc987)
    %2836 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc988)
    %2837 = "ttir.subtract"(%2831, %2835, %2836) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc988)
    %2838 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc989)
    %2839 = "ttir.multiply"(%2837, %2837, %2838) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc989)
    %2840 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc990)
    %2841 = "ttir.sum"(%2839, %2840) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc990)
    %2842 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc991)
    %2843 = "ttir.multiply"(%arg167, %2841, %2842) : (tensor<8x197x1xf32>, tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc991)
    %2844 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc992)
    %2845 = "ttir.add"(%2843, %arg168, %2844) : (tensor<8x197x1xbf16>, tensor<8x197x1xf32>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc992)
    %2846 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc993)
    %2847 = "ttir.sqrt"(%2845, %2846) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc993)
    %2848 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc994)
    %2849 = "ttir.reciprocal"(%2847, %2848) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc994)
    %2850 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc995)
    %2851 = "ttir.multiply"(%2837, %2849, %2850) : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc995)
    %2852 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc996)
    %2853 = "ttir.multiply"(%2851, %arg530, %2852) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc996)
    %2854 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc997)
    %2855 = "ttir.add"(%2853, %arg531, %2854) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc997)
    %2856 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1694)
    %2857 = "ttir.reshape"(%2855, %2856) <{shape = [1576 : i32, 1024 : i32]}> : (tensor<8x197x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1694)
    %2858 = ttir.empty() : tensor<1576x4096xbf16> loc(#loc1695)
    %2859 = "ttir.matmul"(%2857, %arg532, %2858) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x4096xbf16>, tensor<1576x4096xbf16>) -> tensor<1576x4096xbf16> loc(#loc1695)
    %2860 = ttir.empty() : tensor<8x197x4096xbf16> loc(#loc1696)
    %2861 = "ttir.reshape"(%2859, %2860) <{shape = [8 : i32, 197 : i32, 4096 : i32]}> : (tensor<1576x4096xbf16>, tensor<8x197x4096xbf16>) -> tensor<8x197x4096xbf16> loc(#loc1696)
    %2862 = ttir.empty() : tensor<8x197x4096xbf16> loc(#loc147)
    %2863 = "ttir.add"(%2861, %arg533, %2862) : (tensor<8x197x4096xbf16>, tensor<4096xbf16>, tensor<8x197x4096xbf16>) -> tensor<8x197x4096xbf16> loc(#loc147)
    %2864 = ttir.empty() : tensor<8x197x4096xbf16> loc(#loc1697)
    %2865 = "ttir.gelu"(%2863, %2864) : (tensor<8x197x4096xbf16>, tensor<8x197x4096xbf16>) -> tensor<8x197x4096xbf16> loc(#loc1697)
    %2866 = ttir.empty() : tensor<1576x4096xbf16> loc(#loc1698)
    %2867 = "ttir.reshape"(%2865, %2866) <{shape = [1576 : i32, 4096 : i32]}> : (tensor<8x197x4096xbf16>, tensor<1576x4096xbf16>) -> tensor<1576x4096xbf16> loc(#loc1698)
    %2868 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1699)
    %2869 = "ttir.matmul"(%2867, %arg534, %2868) <{transpose_a = false, transpose_b = false}> : (tensor<1576x4096xbf16>, tensor<4096x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1699)
    %2870 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1700)
    %2871 = "ttir.reshape"(%2869, %2870) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1700)
    %2872 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc148)
    %2873 = "ttir.add"(%2871, %arg535, %2872) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc148)
    %2874 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1001)
    %2875 = "ttir.add"(%2873, %2831, %2874) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1001)
    %2876 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc1002)
    %2877 = "ttir.sum"(%2875, %2876) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc1002)
    %2878 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1003)
    %2879 = "ttir.multiply"(%arg169, %2877, %2878) : (tensor<8x197x1024xf32>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1003)
    %2880 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1004)
    %2881 = "ttir.subtract"(%2875, %2879, %2880) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1004)
    %2882 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1005)
    %2883 = "ttir.multiply"(%2881, %2881, %2882) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1005)
    %2884 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc1006)
    %2885 = "ttir.sum"(%2883, %2884) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc1006)
    %2886 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc1007)
    %2887 = "ttir.multiply"(%arg170, %2885, %2886) : (tensor<8x197x1xf32>, tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc1007)
    %2888 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc1008)
    %2889 = "ttir.add"(%2887, %arg171, %2888) : (tensor<8x197x1xbf16>, tensor<8x197x1xf32>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc1008)
    %2890 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc1009)
    %2891 = "ttir.sqrt"(%2889, %2890) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc1009)
    %2892 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc1010)
    %2893 = "ttir.reciprocal"(%2891, %2892) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc1010)
    %2894 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1011)
    %2895 = "ttir.multiply"(%2881, %2893, %2894) : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1011)
    %2896 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1012)
    %2897 = "ttir.multiply"(%2895, %arg536, %2896) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1012)
    %2898 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1013)
    %2899 = "ttir.add"(%2897, %arg537, %2898) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1013)
    %2900 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1998)
    %2901 = "ttir.reshape"(%2899, %2900) <{shape = [1576 : i32, 1024 : i32]}> : (tensor<8x197x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1998)
    %2902 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1999)
    %2903 = "ttir.matmul"(%2901, %arg538, %2902) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1999)
    %2904 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc2000)
    %2905 = "ttir.reshape"(%2903, %2904) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc2000)
    %2906 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc149)
    %2907 = "ttir.add"(%2905, %arg539, %2906) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc149)
    %2908 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1702)
    %2909 = "ttir.reshape"(%2907, %2908) <{shape = [8 : i32, 197 : i32, 16 : i32, 64 : i32]}> : (tensor<8x197x1024xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1702)
    %2910 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1703)
    %2911 = "ttir.transpose"(%2909, %2910) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x197x16x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1703)
    %2912 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1704)
    %2913 = "ttir.reshape"(%2911, %2912) <{shape = [128 : i32, 197 : i32, 64 : i32]}> : (tensor<8x16x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1704)
    %2914 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc2001)
    %2915 = "ttir.matmul"(%2901, %arg540, %2914) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc2001)
    %2916 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc2002)
    %2917 = "ttir.reshape"(%2915, %2916) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc2002)
    %2918 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc150)
    %2919 = "ttir.add"(%2917, %arg541, %2918) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc150)
    %2920 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1706)
    %2921 = "ttir.reshape"(%2919, %2920) <{shape = [8 : i32, 197 : i32, 16 : i32, 64 : i32]}> : (tensor<8x197x1024xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1706)
    %2922 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1707)
    %2923 = "ttir.transpose"(%2921, %2922) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x197x16x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1707)
    %2924 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1708)
    %2925 = "ttir.reshape"(%2923, %2924) <{shape = [128 : i32, 197 : i32, 64 : i32]}> : (tensor<8x16x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1708)
    %2926 = ttir.empty() : tensor<128x64x197xbf16> loc(#loc151)
    %2927 = "ttir.transpose"(%2925, %2926) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<128x197x64xbf16>, tensor<128x64x197xbf16>) -> tensor<128x64x197xbf16> loc(#loc151)
    %2928 = ttir.empty() : tensor<128x197x197xbf16> loc(#loc1709)
    %2929 = "ttir.matmul"(%2913, %2927, %2928) <{transpose_a = false, transpose_b = false}> : (tensor<128x197x64xbf16>, tensor<128x64x197xbf16>, tensor<128x197x197xbf16>) -> tensor<128x197x197xbf16> loc(#loc1709)
    %2930 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1710)
    %2931 = "ttir.reshape"(%2929, %2930) <{shape = [8 : i32, 16 : i32, 197 : i32, 197 : i32]}> : (tensor<128x197x197xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1710)
    %2932 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1711)
    %2933 = "ttir.multiply"(%2931, %arg172, %2932) : (tensor<8x16x197x197xbf16>, tensor<1xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1711)
    %2934 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1712)
    %2935 = "ttir.add"(%2933, %arg173, %2934) : (tensor<8x16x197x197xbf16>, tensor<197x197xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1712)
    %2936 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1713)
    %2937 = "ttir.softmax"(%2935, %2936) <{dimension = -1 : si32}> : (tensor<8x16x197x197xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1713)
    %2938 = ttir.empty() : tensor<128x197x197xbf16> loc(#loc1714)
    %2939 = "ttir.reshape"(%2937, %2938) <{shape = [128 : i32, 197 : i32, 197 : i32]}> : (tensor<8x16x197x197xbf16>, tensor<128x197x197xbf16>) -> tensor<128x197x197xbf16> loc(#loc1714)
    %2940 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc2003)
    %2941 = "ttir.matmul"(%2901, %arg542, %2940) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc2003)
    %2942 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc2004)
    %2943 = "ttir.reshape"(%2941, %2942) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc2004)
    %2944 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc152)
    %2945 = "ttir.add"(%2943, %arg543, %2944) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc152)
    %2946 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1716)
    %2947 = "ttir.reshape"(%2945, %2946) <{shape = [8 : i32, 197 : i32, 16 : i32, 64 : i32]}> : (tensor<8x197x1024xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1716)
    %2948 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1717)
    %2949 = "ttir.transpose"(%2947, %2948) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x197x16x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1717)
    %2950 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1718)
    %2951 = "ttir.reshape"(%2949, %2950) <{shape = [128 : i32, 197 : i32, 64 : i32]}> : (tensor<8x16x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1718)
    %2952 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1719)
    %2953 = "ttir.matmul"(%2939, %2951, %2952) <{transpose_a = false, transpose_b = false}> : (tensor<128x197x197xbf16>, tensor<128x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1719)
    %2954 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1720)
    %2955 = "ttir.reshape"(%2953, %2954) <{shape = [8 : i32, 16 : i32, 197 : i32, 64 : i32]}> : (tensor<128x197x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1720)
    %2956 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1721)
    %2957 = "ttir.transpose"(%2955, %2956) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x16x197x64xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1721)
    %2958 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc2005)
    %2959 = "ttir.reshape"(%2957, %2958) <{shape = [1576 : i32, 1024 : i32]}> : (tensor<8x197x16x64xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc2005)
    %2960 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc2006)
    %2961 = "ttir.matmul"(%2959, %arg544, %2960) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc2006)
    %2962 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc2007)
    %2963 = "ttir.reshape"(%2961, %2962) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc2007)
    %2964 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc153)
    %2965 = "ttir.add"(%2963, %arg545, %2964) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc153)
    %2966 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc351)
    %2967 = "ttir.add"(%2965, %2875, %2966) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc351)
    %2968 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc1016)
    %2969 = "ttir.sum"(%2967, %2968) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc1016)
    %2970 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1017)
    %2971 = "ttir.multiply"(%arg174, %2969, %2970) : (tensor<8x197x1024xf32>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1017)
    %2972 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1018)
    %2973 = "ttir.subtract"(%2967, %2971, %2972) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1018)
    %2974 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1019)
    %2975 = "ttir.multiply"(%2973, %2973, %2974) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1019)
    %2976 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc1020)
    %2977 = "ttir.sum"(%2975, %2976) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc1020)
    %2978 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc1021)
    %2979 = "ttir.multiply"(%arg175, %2977, %2978) : (tensor<8x197x1xf32>, tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc1021)
    %2980 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc1022)
    %2981 = "ttir.add"(%2979, %arg176, %2980) : (tensor<8x197x1xbf16>, tensor<8x197x1xf32>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc1022)
    %2982 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc1023)
    %2983 = "ttir.sqrt"(%2981, %2982) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc1023)
    %2984 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc1024)
    %2985 = "ttir.reciprocal"(%2983, %2984) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc1024)
    %2986 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1025)
    %2987 = "ttir.multiply"(%2973, %2985, %2986) : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1025)
    %2988 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1026)
    %2989 = "ttir.multiply"(%2987, %arg546, %2988) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1026)
    %2990 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1027)
    %2991 = "ttir.add"(%2989, %arg547, %2990) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1027)
    %2992 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1723)
    %2993 = "ttir.reshape"(%2991, %2992) <{shape = [1576 : i32, 1024 : i32]}> : (tensor<8x197x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1723)
    %2994 = ttir.empty() : tensor<1576x4096xbf16> loc(#loc1724)
    %2995 = "ttir.matmul"(%2993, %arg548, %2994) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x4096xbf16>, tensor<1576x4096xbf16>) -> tensor<1576x4096xbf16> loc(#loc1724)
    %2996 = ttir.empty() : tensor<8x197x4096xbf16> loc(#loc1725)
    %2997 = "ttir.reshape"(%2995, %2996) <{shape = [8 : i32, 197 : i32, 4096 : i32]}> : (tensor<1576x4096xbf16>, tensor<8x197x4096xbf16>) -> tensor<8x197x4096xbf16> loc(#loc1725)
    %2998 = ttir.empty() : tensor<8x197x4096xbf16> loc(#loc154)
    %2999 = "ttir.add"(%2997, %arg549, %2998) : (tensor<8x197x4096xbf16>, tensor<4096xbf16>, tensor<8x197x4096xbf16>) -> tensor<8x197x4096xbf16> loc(#loc154)
    %3000 = ttir.empty() : tensor<8x197x4096xbf16> loc(#loc1726)
    %3001 = "ttir.gelu"(%2999, %3000) : (tensor<8x197x4096xbf16>, tensor<8x197x4096xbf16>) -> tensor<8x197x4096xbf16> loc(#loc1726)
    %3002 = ttir.empty() : tensor<1576x4096xbf16> loc(#loc1727)
    %3003 = "ttir.reshape"(%3001, %3002) <{shape = [1576 : i32, 4096 : i32]}> : (tensor<8x197x4096xbf16>, tensor<1576x4096xbf16>) -> tensor<1576x4096xbf16> loc(#loc1727)
    %3004 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1728)
    %3005 = "ttir.matmul"(%3003, %arg550, %3004) <{transpose_a = false, transpose_b = false}> : (tensor<1576x4096xbf16>, tensor<4096x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1728)
    %3006 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1729)
    %3007 = "ttir.reshape"(%3005, %3006) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1729)
    %3008 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc155)
    %3009 = "ttir.add"(%3007, %arg551, %3008) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc155)
    %3010 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1031)
    %3011 = "ttir.add"(%3009, %2967, %3010) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1031)
    %3012 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc1032)
    %3013 = "ttir.sum"(%3011, %3012) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc1032)
    %3014 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1033)
    %3015 = "ttir.multiply"(%arg177, %3013, %3014) : (tensor<8x197x1024xf32>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1033)
    %3016 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1034)
    %3017 = "ttir.subtract"(%3011, %3015, %3016) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1034)
    %3018 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1035)
    %3019 = "ttir.multiply"(%3017, %3017, %3018) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1035)
    %3020 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc1036)
    %3021 = "ttir.sum"(%3019, %3020) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc1036)
    %3022 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc1037)
    %3023 = "ttir.multiply"(%arg178, %3021, %3022) : (tensor<8x197x1xf32>, tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc1037)
    %3024 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc1038)
    %3025 = "ttir.add"(%3023, %arg179, %3024) : (tensor<8x197x1xbf16>, tensor<8x197x1xf32>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc1038)
    %3026 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc1039)
    %3027 = "ttir.sqrt"(%3025, %3026) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc1039)
    %3028 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc1040)
    %3029 = "ttir.reciprocal"(%3027, %3028) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc1040)
    %3030 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1041)
    %3031 = "ttir.multiply"(%3017, %3029, %3030) : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1041)
    %3032 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1042)
    %3033 = "ttir.multiply"(%3031, %arg552, %3032) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1042)
    %3034 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1043)
    %3035 = "ttir.add"(%3033, %arg553, %3034) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1043)
    %3036 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc2008)
    %3037 = "ttir.reshape"(%3035, %3036) <{shape = [1576 : i32, 1024 : i32]}> : (tensor<8x197x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc2008)
    %3038 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc2009)
    %3039 = "ttir.matmul"(%3037, %arg554, %3038) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc2009)
    %3040 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc2010)
    %3041 = "ttir.reshape"(%3039, %3040) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc2010)
    %3042 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc156)
    %3043 = "ttir.add"(%3041, %arg555, %3042) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc156)
    %3044 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1731)
    %3045 = "ttir.reshape"(%3043, %3044) <{shape = [8 : i32, 197 : i32, 16 : i32, 64 : i32]}> : (tensor<8x197x1024xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1731)
    %3046 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1732)
    %3047 = "ttir.transpose"(%3045, %3046) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x197x16x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1732)
    %3048 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1733)
    %3049 = "ttir.reshape"(%3047, %3048) <{shape = [128 : i32, 197 : i32, 64 : i32]}> : (tensor<8x16x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1733)
    %3050 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc2011)
    %3051 = "ttir.matmul"(%3037, %arg556, %3050) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc2011)
    %3052 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc2012)
    %3053 = "ttir.reshape"(%3051, %3052) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc2012)
    %3054 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc157)
    %3055 = "ttir.add"(%3053, %arg557, %3054) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc157)
    %3056 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1735)
    %3057 = "ttir.reshape"(%3055, %3056) <{shape = [8 : i32, 197 : i32, 16 : i32, 64 : i32]}> : (tensor<8x197x1024xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1735)
    %3058 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1736)
    %3059 = "ttir.transpose"(%3057, %3058) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x197x16x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1736)
    %3060 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1737)
    %3061 = "ttir.reshape"(%3059, %3060) <{shape = [128 : i32, 197 : i32, 64 : i32]}> : (tensor<8x16x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1737)
    %3062 = ttir.empty() : tensor<128x64x197xbf16> loc(#loc158)
    %3063 = "ttir.transpose"(%3061, %3062) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<128x197x64xbf16>, tensor<128x64x197xbf16>) -> tensor<128x64x197xbf16> loc(#loc158)
    %3064 = ttir.empty() : tensor<128x197x197xbf16> loc(#loc1738)
    %3065 = "ttir.matmul"(%3049, %3063, %3064) <{transpose_a = false, transpose_b = false}> : (tensor<128x197x64xbf16>, tensor<128x64x197xbf16>, tensor<128x197x197xbf16>) -> tensor<128x197x197xbf16> loc(#loc1738)
    %3066 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1739)
    %3067 = "ttir.reshape"(%3065, %3066) <{shape = [8 : i32, 16 : i32, 197 : i32, 197 : i32]}> : (tensor<128x197x197xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1739)
    %3068 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1740)
    %3069 = "ttir.multiply"(%3067, %arg180, %3068) : (tensor<8x16x197x197xbf16>, tensor<1xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1740)
    %3070 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1741)
    %3071 = "ttir.add"(%3069, %arg181, %3070) : (tensor<8x16x197x197xbf16>, tensor<197x197xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1741)
    %3072 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1742)
    %3073 = "ttir.softmax"(%3071, %3072) <{dimension = -1 : si32}> : (tensor<8x16x197x197xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1742)
    %3074 = ttir.empty() : tensor<128x197x197xbf16> loc(#loc1743)
    %3075 = "ttir.reshape"(%3073, %3074) <{shape = [128 : i32, 197 : i32, 197 : i32]}> : (tensor<8x16x197x197xbf16>, tensor<128x197x197xbf16>) -> tensor<128x197x197xbf16> loc(#loc1743)
    %3076 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc2013)
    %3077 = "ttir.matmul"(%3037, %arg558, %3076) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc2013)
    %3078 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc2014)
    %3079 = "ttir.reshape"(%3077, %3078) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc2014)
    %3080 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc159)
    %3081 = "ttir.add"(%3079, %arg559, %3080) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc159)
    %3082 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1745)
    %3083 = "ttir.reshape"(%3081, %3082) <{shape = [8 : i32, 197 : i32, 16 : i32, 64 : i32]}> : (tensor<8x197x1024xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1745)
    %3084 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1746)
    %3085 = "ttir.transpose"(%3083, %3084) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x197x16x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1746)
    %3086 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1747)
    %3087 = "ttir.reshape"(%3085, %3086) <{shape = [128 : i32, 197 : i32, 64 : i32]}> : (tensor<8x16x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1747)
    %3088 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1748)
    %3089 = "ttir.matmul"(%3075, %3087, %3088) <{transpose_a = false, transpose_b = false}> : (tensor<128x197x197xbf16>, tensor<128x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1748)
    %3090 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1749)
    %3091 = "ttir.reshape"(%3089, %3090) <{shape = [8 : i32, 16 : i32, 197 : i32, 64 : i32]}> : (tensor<128x197x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1749)
    %3092 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1750)
    %3093 = "ttir.transpose"(%3091, %3092) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x16x197x64xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1750)
    %3094 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc2015)
    %3095 = "ttir.reshape"(%3093, %3094) <{shape = [1576 : i32, 1024 : i32]}> : (tensor<8x197x16x64xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc2015)
    %3096 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc2016)
    %3097 = "ttir.matmul"(%3095, %arg560, %3096) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc2016)
    %3098 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc2017)
    %3099 = "ttir.reshape"(%3097, %3098) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc2017)
    %3100 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc160)
    %3101 = "ttir.add"(%3099, %arg561, %3100) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc160)
    %3102 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc357)
    %3103 = "ttir.add"(%3101, %3011, %3102) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc357)
    %3104 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc1046)
    %3105 = "ttir.sum"(%3103, %3104) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc1046)
    %3106 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1047)
    %3107 = "ttir.multiply"(%arg182, %3105, %3106) : (tensor<8x197x1024xf32>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1047)
    %3108 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1048)
    %3109 = "ttir.subtract"(%3103, %3107, %3108) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1048)
    %3110 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1049)
    %3111 = "ttir.multiply"(%3109, %3109, %3110) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1049)
    %3112 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc1050)
    %3113 = "ttir.sum"(%3111, %3112) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc1050)
    %3114 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc1051)
    %3115 = "ttir.multiply"(%arg183, %3113, %3114) : (tensor<8x197x1xf32>, tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc1051)
    %3116 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc1052)
    %3117 = "ttir.add"(%3115, %arg184, %3116) : (tensor<8x197x1xbf16>, tensor<8x197x1xf32>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc1052)
    %3118 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc1053)
    %3119 = "ttir.sqrt"(%3117, %3118) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc1053)
    %3120 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc1054)
    %3121 = "ttir.reciprocal"(%3119, %3120) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc1054)
    %3122 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1055)
    %3123 = "ttir.multiply"(%3109, %3121, %3122) : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1055)
    %3124 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1056)
    %3125 = "ttir.multiply"(%3123, %arg562, %3124) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1056)
    %3126 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1057)
    %3127 = "ttir.add"(%3125, %arg563, %3126) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1057)
    %3128 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1752)
    %3129 = "ttir.reshape"(%3127, %3128) <{shape = [1576 : i32, 1024 : i32]}> : (tensor<8x197x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1752)
    %3130 = ttir.empty() : tensor<1576x4096xbf16> loc(#loc1753)
    %3131 = "ttir.matmul"(%3129, %arg564, %3130) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x4096xbf16>, tensor<1576x4096xbf16>) -> tensor<1576x4096xbf16> loc(#loc1753)
    %3132 = ttir.empty() : tensor<8x197x4096xbf16> loc(#loc1754)
    %3133 = "ttir.reshape"(%3131, %3132) <{shape = [8 : i32, 197 : i32, 4096 : i32]}> : (tensor<1576x4096xbf16>, tensor<8x197x4096xbf16>) -> tensor<8x197x4096xbf16> loc(#loc1754)
    %3134 = ttir.empty() : tensor<8x197x4096xbf16> loc(#loc161)
    %3135 = "ttir.add"(%3133, %arg565, %3134) : (tensor<8x197x4096xbf16>, tensor<4096xbf16>, tensor<8x197x4096xbf16>) -> tensor<8x197x4096xbf16> loc(#loc161)
    %3136 = ttir.empty() : tensor<8x197x4096xbf16> loc(#loc1755)
    %3137 = "ttir.gelu"(%3135, %3136) : (tensor<8x197x4096xbf16>, tensor<8x197x4096xbf16>) -> tensor<8x197x4096xbf16> loc(#loc1755)
    %3138 = ttir.empty() : tensor<1576x4096xbf16> loc(#loc1756)
    %3139 = "ttir.reshape"(%3137, %3138) <{shape = [1576 : i32, 4096 : i32]}> : (tensor<8x197x4096xbf16>, tensor<1576x4096xbf16>) -> tensor<1576x4096xbf16> loc(#loc1756)
    %3140 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1757)
    %3141 = "ttir.matmul"(%3139, %arg566, %3140) <{transpose_a = false, transpose_b = false}> : (tensor<1576x4096xbf16>, tensor<4096x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1757)
    %3142 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1758)
    %3143 = "ttir.reshape"(%3141, %3142) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1758)
    %3144 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc162)
    %3145 = "ttir.add"(%3143, %arg567, %3144) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc162)
    %3146 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1061)
    %3147 = "ttir.add"(%3145, %3103, %3146) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1061)
    %3148 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc1062)
    %3149 = "ttir.sum"(%3147, %3148) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc1062)
    %3150 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1063)
    %3151 = "ttir.multiply"(%arg185, %3149, %3150) : (tensor<8x197x1024xf32>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1063)
    %3152 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1064)
    %3153 = "ttir.subtract"(%3147, %3151, %3152) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1064)
    %3154 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1065)
    %3155 = "ttir.multiply"(%3153, %3153, %3154) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1065)
    %3156 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc1066)
    %3157 = "ttir.sum"(%3155, %3156) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc1066)
    %3158 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc1067)
    %3159 = "ttir.multiply"(%arg186, %3157, %3158) : (tensor<8x197x1xf32>, tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc1067)
    %3160 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc1068)
    %3161 = "ttir.add"(%3159, %arg187, %3160) : (tensor<8x197x1xbf16>, tensor<8x197x1xf32>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc1068)
    %3162 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc1069)
    %3163 = "ttir.sqrt"(%3161, %3162) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc1069)
    %3164 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc1070)
    %3165 = "ttir.reciprocal"(%3163, %3164) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc1070)
    %3166 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1071)
    %3167 = "ttir.multiply"(%3153, %3165, %3166) : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1071)
    %3168 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1072)
    %3169 = "ttir.multiply"(%3167, %arg568, %3168) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1072)
    %3170 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1073)
    %3171 = "ttir.add"(%3169, %arg569, %3170) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1073)
    %3172 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc2018)
    %3173 = "ttir.reshape"(%3171, %3172) <{shape = [1576 : i32, 1024 : i32]}> : (tensor<8x197x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc2018)
    %3174 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc2019)
    %3175 = "ttir.matmul"(%3173, %arg570, %3174) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc2019)
    %3176 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc2020)
    %3177 = "ttir.reshape"(%3175, %3176) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc2020)
    %3178 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc163)
    %3179 = "ttir.add"(%3177, %arg571, %3178) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc163)
    %3180 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1760)
    %3181 = "ttir.reshape"(%3179, %3180) <{shape = [8 : i32, 197 : i32, 16 : i32, 64 : i32]}> : (tensor<8x197x1024xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1760)
    %3182 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1761)
    %3183 = "ttir.transpose"(%3181, %3182) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x197x16x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1761)
    %3184 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1762)
    %3185 = "ttir.reshape"(%3183, %3184) <{shape = [128 : i32, 197 : i32, 64 : i32]}> : (tensor<8x16x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1762)
    %3186 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc2021)
    %3187 = "ttir.matmul"(%3173, %arg572, %3186) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc2021)
    %3188 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc2022)
    %3189 = "ttir.reshape"(%3187, %3188) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc2022)
    %3190 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc164)
    %3191 = "ttir.add"(%3189, %arg573, %3190) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc164)
    %3192 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1764)
    %3193 = "ttir.reshape"(%3191, %3192) <{shape = [8 : i32, 197 : i32, 16 : i32, 64 : i32]}> : (tensor<8x197x1024xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1764)
    %3194 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1765)
    %3195 = "ttir.transpose"(%3193, %3194) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x197x16x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1765)
    %3196 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1766)
    %3197 = "ttir.reshape"(%3195, %3196) <{shape = [128 : i32, 197 : i32, 64 : i32]}> : (tensor<8x16x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1766)
    %3198 = ttir.empty() : tensor<128x64x197xbf16> loc(#loc165)
    %3199 = "ttir.transpose"(%3197, %3198) <{dim0 = -2 : si32, dim1 = -1 : si32}> : (tensor<128x197x64xbf16>, tensor<128x64x197xbf16>) -> tensor<128x64x197xbf16> loc(#loc165)
    %3200 = ttir.empty() : tensor<128x197x197xbf16> loc(#loc1767)
    %3201 = "ttir.matmul"(%3185, %3199, %3200) <{transpose_a = false, transpose_b = false}> : (tensor<128x197x64xbf16>, tensor<128x64x197xbf16>, tensor<128x197x197xbf16>) -> tensor<128x197x197xbf16> loc(#loc1767)
    %3202 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1768)
    %3203 = "ttir.reshape"(%3201, %3202) <{shape = [8 : i32, 16 : i32, 197 : i32, 197 : i32]}> : (tensor<128x197x197xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1768)
    %3204 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1769)
    %3205 = "ttir.multiply"(%3203, %arg188, %3204) : (tensor<8x16x197x197xbf16>, tensor<1xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1769)
    %3206 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1770)
    %3207 = "ttir.add"(%3205, %arg189, %3206) : (tensor<8x16x197x197xbf16>, tensor<197x197xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1770)
    %3208 = ttir.empty() : tensor<8x16x197x197xbf16> loc(#loc1771)
    %3209 = "ttir.softmax"(%3207, %3208) <{dimension = -1 : si32}> : (tensor<8x16x197x197xbf16>, tensor<8x16x197x197xbf16>) -> tensor<8x16x197x197xbf16> loc(#loc1771)
    %3210 = ttir.empty() : tensor<128x197x197xbf16> loc(#loc1772)
    %3211 = "ttir.reshape"(%3209, %3210) <{shape = [128 : i32, 197 : i32, 197 : i32]}> : (tensor<8x16x197x197xbf16>, tensor<128x197x197xbf16>) -> tensor<128x197x197xbf16> loc(#loc1772)
    %3212 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc2023)
    %3213 = "ttir.matmul"(%3173, %arg574, %3212) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc2023)
    %3214 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc2024)
    %3215 = "ttir.reshape"(%3213, %3214) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc2024)
    %3216 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc166)
    %3217 = "ttir.add"(%3215, %arg575, %3216) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc166)
    %3218 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1774)
    %3219 = "ttir.reshape"(%3217, %3218) <{shape = [8 : i32, 197 : i32, 16 : i32, 64 : i32]}> : (tensor<8x197x1024xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1774)
    %3220 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1775)
    %3221 = "ttir.transpose"(%3219, %3220) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x197x16x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1775)
    %3222 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1776)
    %3223 = "ttir.reshape"(%3221, %3222) <{shape = [128 : i32, 197 : i32, 64 : i32]}> : (tensor<8x16x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1776)
    %3224 = ttir.empty() : tensor<128x197x64xbf16> loc(#loc1777)
    %3225 = "ttir.matmul"(%3211, %3223, %3224) <{transpose_a = false, transpose_b = false}> : (tensor<128x197x197xbf16>, tensor<128x197x64xbf16>, tensor<128x197x64xbf16>) -> tensor<128x197x64xbf16> loc(#loc1777)
    %3226 = ttir.empty() : tensor<8x16x197x64xbf16> loc(#loc1778)
    %3227 = "ttir.reshape"(%3225, %3226) <{shape = [8 : i32, 16 : i32, 197 : i32, 64 : i32]}> : (tensor<128x197x64xbf16>, tensor<8x16x197x64xbf16>) -> tensor<8x16x197x64xbf16> loc(#loc1778)
    %3228 = ttir.empty() : tensor<8x197x16x64xbf16> loc(#loc1779)
    %3229 = "ttir.transpose"(%3227, %3228) <{dim0 = -3 : si32, dim1 = -2 : si32}> : (tensor<8x16x197x64xbf16>, tensor<8x197x16x64xbf16>) -> tensor<8x197x16x64xbf16> loc(#loc1779)
    %3230 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc2025)
    %3231 = "ttir.reshape"(%3229, %3230) <{shape = [1576 : i32, 1024 : i32]}> : (tensor<8x197x16x64xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc2025)
    %3232 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc2026)
    %3233 = "ttir.matmul"(%3231, %arg576, %3232) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc2026)
    %3234 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc2027)
    %3235 = "ttir.reshape"(%3233, %3234) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc2027)
    %3236 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc167)
    %3237 = "ttir.add"(%3235, %arg577, %3236) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc167)
    %3238 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc363)
    %3239 = "ttir.add"(%3237, %3147, %3238) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc363)
    %3240 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc1076)
    %3241 = "ttir.sum"(%3239, %3240) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc1076)
    %3242 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1077)
    %3243 = "ttir.multiply"(%arg190, %3241, %3242) : (tensor<8x197x1024xf32>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1077)
    %3244 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1078)
    %3245 = "ttir.subtract"(%3239, %3243, %3244) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1078)
    %3246 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1079)
    %3247 = "ttir.multiply"(%3245, %3245, %3246) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1079)
    %3248 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc1080)
    %3249 = "ttir.sum"(%3247, %3248) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc1080)
    %3250 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc1081)
    %3251 = "ttir.multiply"(%arg191, %3249, %3250) : (tensor<8x197x1xf32>, tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc1081)
    %3252 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc1082)
    %3253 = "ttir.add"(%3251, %arg192, %3252) : (tensor<8x197x1xbf16>, tensor<8x197x1xf32>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc1082)
    %3254 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc1083)
    %3255 = "ttir.sqrt"(%3253, %3254) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc1083)
    %3256 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc1084)
    %3257 = "ttir.reciprocal"(%3255, %3256) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc1084)
    %3258 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1085)
    %3259 = "ttir.multiply"(%3245, %3257, %3258) : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1085)
    %3260 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1086)
    %3261 = "ttir.multiply"(%3259, %arg578, %3260) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1086)
    %3262 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1087)
    %3263 = "ttir.add"(%3261, %arg579, %3262) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1087)
    %3264 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1781)
    %3265 = "ttir.reshape"(%3263, %3264) <{shape = [1576 : i32, 1024 : i32]}> : (tensor<8x197x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1781)
    %3266 = ttir.empty() : tensor<1576x4096xbf16> loc(#loc1782)
    %3267 = "ttir.matmul"(%3265, %arg580, %3266) <{transpose_a = false, transpose_b = false}> : (tensor<1576x1024xbf16>, tensor<1024x4096xbf16>, tensor<1576x4096xbf16>) -> tensor<1576x4096xbf16> loc(#loc1782)
    %3268 = ttir.empty() : tensor<8x197x4096xbf16> loc(#loc1783)
    %3269 = "ttir.reshape"(%3267, %3268) <{shape = [8 : i32, 197 : i32, 4096 : i32]}> : (tensor<1576x4096xbf16>, tensor<8x197x4096xbf16>) -> tensor<8x197x4096xbf16> loc(#loc1783)
    %3270 = ttir.empty() : tensor<8x197x4096xbf16> loc(#loc168)
    %3271 = "ttir.add"(%3269, %arg581, %3270) : (tensor<8x197x4096xbf16>, tensor<4096xbf16>, tensor<8x197x4096xbf16>) -> tensor<8x197x4096xbf16> loc(#loc168)
    %3272 = ttir.empty() : tensor<8x197x4096xbf16> loc(#loc1784)
    %3273 = "ttir.gelu"(%3271, %3272) : (tensor<8x197x4096xbf16>, tensor<8x197x4096xbf16>) -> tensor<8x197x4096xbf16> loc(#loc1784)
    %3274 = ttir.empty() : tensor<1576x4096xbf16> loc(#loc1785)
    %3275 = "ttir.reshape"(%3273, %3274) <{shape = [1576 : i32, 4096 : i32]}> : (tensor<8x197x4096xbf16>, tensor<1576x4096xbf16>) -> tensor<1576x4096xbf16> loc(#loc1785)
    %3276 = ttir.empty() : tensor<1576x1024xbf16> loc(#loc1786)
    %3277 = "ttir.matmul"(%3275, %arg582, %3276) <{transpose_a = false, transpose_b = false}> : (tensor<1576x4096xbf16>, tensor<4096x1024xbf16>, tensor<1576x1024xbf16>) -> tensor<1576x1024xbf16> loc(#loc1786)
    %3278 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1787)
    %3279 = "ttir.reshape"(%3277, %3278) <{shape = [8 : i32, 197 : i32, 1024 : i32]}> : (tensor<1576x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1787)
    %3280 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc169)
    %3281 = "ttir.add"(%3279, %arg583, %3280) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc169)
    %3282 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc1091)
    %3283 = "ttir.add"(%3281, %3239, %3282) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc1091)
    %3284 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc207)
    %3285 = "ttir.sum"(%3283, %3284) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc207)
    %3286 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc208)
    %3287 = "ttir.multiply"(%arg193, %3285, %3286) : (tensor<8x197x1024xf32>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc208)
    %3288 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc209)
    %3289 = "ttir.subtract"(%3283, %3287, %3288) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc209)
    %3290 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc210)
    %3291 = "ttir.multiply"(%3289, %3289, %3290) : (tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc210)
    %3292 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc211)
    %3293 = "ttir.sum"(%3291, %3292) <{dim_arg = [-1 : i32], keep_dim = true}> : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc211)
    %3294 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc212)
    %3295 = "ttir.multiply"(%arg194, %3293, %3294) : (tensor<8x197x1xf32>, tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc212)
    %3296 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc213)
    %3297 = "ttir.add"(%3295, %arg195, %3296) : (tensor<8x197x1xbf16>, tensor<8x197x1xf32>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc213)
    %3298 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc214)
    %3299 = "ttir.sqrt"(%3297, %3298) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc214)
    %3300 = ttir.empty() : tensor<8x197x1xbf16> loc(#loc215)
    %3301 = "ttir.reciprocal"(%3299, %3300) : (tensor<8x197x1xbf16>, tensor<8x197x1xbf16>) -> tensor<8x197x1xbf16> loc(#loc215)
    %3302 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc216)
    %3303 = "ttir.multiply"(%3289, %3301, %3302) : (tensor<8x197x1024xbf16>, tensor<8x197x1xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc216)
    %3304 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc217)
    %3305 = "ttir.multiply"(%3303, %arg584, %3304) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc217)
    %3306 = ttir.empty() : tensor<8x197x1024xbf16> loc(#loc218)
    %3307 = "ttir.add"(%3305, %arg585, %3306) : (tensor<8x197x1024xbf16>, tensor<1024xbf16>, tensor<8x197x1024xbf16>) -> tensor<8x197x1024xbf16> loc(#loc218)
    %3308 = ttir.empty() : tensor<8x1x1024xbf16> loc(#loc170)
    %3309 = "ttir.index"(%3307, %3308) <{begin = 0 : i32, dim = 1 : i32, end = 1 : i32, step = 1 : i32}> : (tensor<8x197x1024xbf16>, tensor<8x1x1024xbf16>) -> tensor<8x1x1024xbf16> loc(#loc170)
    %3310 = ttir.empty() : tensor<8x1024xbf16> loc(#loc174)
    %3311 = "ttir.reshape"(%3309, %3310) <{shape = [8 : i32, 1024 : i32]}> : (tensor<8x1x1024xbf16>, tensor<8x1024xbf16>) -> tensor<8x1024xbf16> loc(#loc174)
    %3312 = ttir.empty() : tensor<8x1000xbf16> loc(#loc179)
    %3313 = "ttir.matmul"(%3311, %arg586, %3312) <{transpose_a = false, transpose_b = false}> : (tensor<8x1024xbf16>, tensor<1024x1000xbf16>, tensor<8x1000xbf16>) -> tensor<8x1000xbf16> loc(#loc179)
    %3314 = ttir.empty() : tensor<8x1000xbf16> loc(#loc171)
    %3315 = "ttir.add"(%3313, %arg587, %3314) : (tensor<8x1000xbf16>, tensor<1000xbf16>, tensor<8x1000xbf16>) -> tensor<8x1000xbf16> loc(#loc171)
    return %3315 : tensor<8x1000xbf16> loc(#loc172)
  } loc(#loc)
} loc(#loc)
#loc1 = loc("transformers.models.vit.modeling_vit.ViTForImageClassification::")
#loc2 = loc("add_16")
#loc3 = loc("add_23")
#loc4 = loc("transpose_27")
#loc5 = loc("add_38")
#loc6 = loc("add_49")
#loc7 = loc("add_57")
#loc8 = loc("add_63")
#loc9 = loc("add_71")
#loc10 = loc("add_78")
#loc11 = loc("transpose_82")
#loc12 = loc("add_93")
#loc13 = loc("add_104")
#loc14 = loc("add_112")
#loc15 = loc("add_118")
#loc16 = loc("add_126")
#loc17 = loc("add_133")
#loc18 = loc("transpose_137")
#loc19 = loc("add_148")
#loc20 = loc("add_159")
#loc21 = loc("add_167")
#loc22 = loc("add_173")
#loc23 = loc("add_181")
#loc24 = loc("add_188")
#loc25 = loc("transpose_192")
#loc26 = loc("add_203")
#loc27 = loc("add_214")
#loc28 = loc("add_222")
#loc29 = loc("add_228")
#loc30 = loc("add_236")
#loc31 = loc("add_243")
#loc32 = loc("transpose_247")
#loc33 = loc("add_258")
#loc34 = loc("add_269")
#loc35 = loc("add_277")
#loc36 = loc("add_283")
#loc37 = loc("add_291")
#loc38 = loc("add_298")
#loc39 = loc("transpose_302")
#loc40 = loc("add_313")
#loc41 = loc("add_324")
#loc42 = loc("add_332")
#loc43 = loc("add_338")
#loc44 = loc("add_346")
#loc45 = loc("add_353")
#loc46 = loc("transpose_357")
#loc47 = loc("add_368")
#loc48 = loc("add_379")
#loc49 = loc("add_387")
#loc50 = loc("add_393")
#loc51 = loc("add_401")
#loc52 = loc("add_408")
#loc53 = loc("transpose_412")
#loc54 = loc("add_423")
#loc55 = loc("add_434")
#loc56 = loc("add_442")
#loc57 = loc("add_448")
#loc58 = loc("add_456")
#loc59 = loc("add_463")
#loc60 = loc("transpose_467")
#loc61 = loc("add_478")
#loc62 = loc("add_489")
#loc63 = loc("add_497")
#loc64 = loc("add_503")
#loc65 = loc("add_511")
#loc66 = loc("add_518")
#loc67 = loc("transpose_522")
#loc68 = loc("add_533")
#loc69 = loc("add_544")
#loc70 = loc("add_552")
#loc71 = loc("add_558")
#loc72 = loc("add_566")
#loc73 = loc("add_573")
#loc74 = loc("transpose_577")
#loc75 = loc("add_588")
#loc76 = loc("add_599")
#loc77 = loc("add_607")
#loc78 = loc("add_613")
#loc79 = loc("add_621")
#loc80 = loc("add_628")
#loc81 = loc("transpose_632")
#loc82 = loc("add_643")
#loc83 = loc("add_654")
#loc84 = loc("add_662")
#loc85 = loc("add_668")
#loc86 = loc("add_676")
#loc87 = loc("add_683")
#loc88 = loc("transpose_687")
#loc89 = loc("add_698")
#loc90 = loc("add_709")
#loc91 = loc("add_717")
#loc92 = loc("add_723")
#loc93 = loc("add_731")
#loc94 = loc("add_738")
#loc95 = loc("transpose_742")
#loc96 = loc("add_753")
#loc97 = loc("add_764")
#loc98 = loc("add_772")
#loc99 = loc("add_778")
#loc100 = loc("add_786")
#loc101 = loc("add_793")
#loc102 = loc("transpose_797")
#loc103 = loc("add_808")
#loc104 = loc("add_819")
#loc105 = loc("add_827")
#loc106 = loc("add_833")
#loc107 = loc("add_841")
#loc108 = loc("add_848")
#loc109 = loc("transpose_852")
#loc110 = loc("add_863")
#loc111 = loc("add_874")
#loc112 = loc("add_882")
#loc113 = loc("add_888")
#loc114 = loc("add_896")
#loc115 = loc("add_903")
#loc116 = loc("transpose_907")
#loc117 = loc("add_918")
#loc118 = loc("add_929")
#loc119 = loc("add_937")
#loc120 = loc("add_943")
#loc121 = loc("add_951")
#loc122 = loc("add_958")
#loc123 = loc("transpose_962")
#loc124 = loc("add_973")
#loc125 = loc("add_984")
#loc126 = loc("add_992")
#loc127 = loc("add_998")
#loc128 = loc("add_1006")
#loc129 = loc("add_1013")
#loc130 = loc("transpose_1017")
#loc131 = loc("add_1028")
#loc132 = loc("add_1039")
#loc133 = loc("add_1047")
#loc134 = loc("add_1053")
#loc135 = loc("add_1061")
#loc136 = loc("add_1068")
#loc137 = loc("transpose_1072")
#loc138 = loc("add_1083")
#loc139 = loc("add_1094")
#loc140 = loc("add_1102")
#loc141 = loc("add_1108")
#loc142 = loc("add_1116")
#loc143 = loc("add_1123")
#loc144 = loc("transpose_1127")
#loc145 = loc("add_1138")
#loc146 = loc("add_1149")
#loc147 = loc("add_1157")
#loc148 = loc("add_1163")
#loc149 = loc("add_1171")
#loc150 = loc("add_1178")
#loc151 = loc("transpose_1182")
#loc152 = loc("add_1193")
#loc153 = loc("add_1204")
#loc154 = loc("add_1212")
#loc155 = loc("add_1218")
#loc156 = loc("add_1226")
#loc157 = loc("add_1233")
#loc158 = loc("transpose_1237")
#loc159 = loc("add_1248")
#loc160 = loc("add_1259")
#loc161 = loc("add_1267")
#loc162 = loc("add_1273")
#loc163 = loc("add_1281")
#loc164 = loc("add_1288")
#loc165 = loc("transpose_1292")
#loc166 = loc("add_1303")
#loc167 = loc("add_1314")
#loc168 = loc("add_1322")
#loc169 = loc("add_1328")
#loc170 = loc("index_1332")
#loc171 = loc("add_1336")
#loc172 = loc(unknown)
#loc173 = loc("transformers.models.vit.modeling_vit.ViTModel::vit"(#loc1))
#loc174 = loc("reshape_1333"(#loc1))
#loc175 = loc("torch.nn.modules.linear.Linear::classifier"(#loc1))
#loc176 = loc("transformers.models.vit.modeling_vit.ViTEmbeddings::embeddings"(#loc173))
#loc177 = loc("transformers.models.vit.modeling_vit.ViTEncoder::encoder"(#loc173))
#loc178 = loc("torch.nn.modules.normalization.LayerNorm::layernorm"(#loc173))
#loc179 = loc("matmul_1335"(#loc175))
#loc180 = loc("transformers.models.vit.modeling_vit.ViTPatchEmbeddings::patch_embeddings"(#loc176))
#loc181 = loc("concatenate_8"(#loc176))
#loc182 = loc("add_9"(#loc176))
#loc183 = loc("transformers.models.vit.modeling_vit.ViTLayer::layer.0"(#loc177))
#loc184 = loc("transformers.models.vit.modeling_vit.ViTLayer::layer.1"(#loc177))
#loc185 = loc("transformers.models.vit.modeling_vit.ViTLayer::layer.2"(#loc177))
#loc186 = loc("transformers.models.vit.modeling_vit.ViTLayer::layer.3"(#loc177))
#loc187 = loc("transformers.models.vit.modeling_vit.ViTLayer::layer.4"(#loc177))
#loc188 = loc("transformers.models.vit.modeling_vit.ViTLayer::layer.5"(#loc177))
#loc189 = loc("transformers.models.vit.modeling_vit.ViTLayer::layer.6"(#loc177))
#loc190 = loc("transformers.models.vit.modeling_vit.ViTLayer::layer.7"(#loc177))
#loc191 = loc("transformers.models.vit.modeling_vit.ViTLayer::layer.8"(#loc177))
#loc192 = loc("transformers.models.vit.modeling_vit.ViTLayer::layer.9"(#loc177))
#loc193 = loc("transformers.models.vit.modeling_vit.ViTLayer::layer.10"(#loc177))
#loc194 = loc("transformers.models.vit.modeling_vit.ViTLayer::layer.11"(#loc177))
#loc195 = loc("transformers.models.vit.modeling_vit.ViTLayer::layer.12"(#loc177))
#loc196 = loc("transformers.models.vit.modeling_vit.ViTLayer::layer.13"(#loc177))
#loc197 = loc("transformers.models.vit.modeling_vit.ViTLayer::layer.14"(#loc177))
#loc198 = loc("transformers.models.vit.modeling_vit.ViTLayer::layer.15"(#loc177))
#loc199 = loc("transformers.models.vit.modeling_vit.ViTLayer::layer.16"(#loc177))
#loc200 = loc("transformers.models.vit.modeling_vit.ViTLayer::layer.17"(#loc177))
#loc201 = loc("transformers.models.vit.modeling_vit.ViTLayer::layer.18"(#loc177))
#loc202 = loc("transformers.models.vit.modeling_vit.ViTLayer::layer.19"(#loc177))
#loc203 = loc("transformers.models.vit.modeling_vit.ViTLayer::layer.20"(#loc177))
#loc204 = loc("transformers.models.vit.modeling_vit.ViTLayer::layer.21"(#loc177))
#loc205 = loc("transformers.models.vit.modeling_vit.ViTLayer::layer.22"(#loc177))
#loc206 = loc("transformers.models.vit.modeling_vit.ViTLayer::layer.23"(#loc177))
#loc207 = loc("layernorm_1331.dc.reduce_sum.0"(#loc178))
#loc208 = loc("layernorm_1331.dc.multiply.2"(#loc178))
#loc209 = loc("layernorm_1331.dc.subtract.3"(#loc178))
#loc210 = loc("layernorm_1331.dc.multiply.4"(#loc178))
#loc211 = loc("layernorm_1331.dc.reduce_sum.5"(#loc178))
#loc212 = loc("layernorm_1331.dc.multiply.7"(#loc178))
#loc213 = loc("layernorm_1331.dc.add.9"(#loc178))
#loc214 = loc("layernorm_1331.dc.sqrt.10"(#loc178))
#loc215 = loc("layernorm_1331.dc.reciprocal.11"(#loc178))
#loc216 = loc("layernorm_1331.dc.multiply.12"(#loc178))
#loc217 = loc("layernorm_1331.dc.multiply.13"(#loc178))
#loc218 = loc("layernorm_1331.dc.add.14"(#loc178))
#loc219 = loc("torch.nn.modules.conv.Conv2d::projection"(#loc180))
#loc220 = loc("reshape_5"(#loc180))
#loc221 = loc("squeeze_6"(#loc180))
#loc222 = loc("transpose_7"(#loc180))
#loc223 = loc("torch.nn.modules.normalization.LayerNorm::layernorm_before"(#loc183))
#loc224 = loc("transformers.models.vit.modeling_vit.ViTSdpaAttention::attention"(#loc183))
#loc225 = loc("add_51"(#loc183))
#loc226 = loc("torch.nn.modules.normalization.LayerNorm::layernorm_after"(#loc183))
#loc227 = loc("transformers.models.vit.modeling_vit.ViTIntermediate::intermediate"(#loc183))
#loc228 = loc("transformers.models.vit.modeling_vit.ViTOutput::output"(#loc183))
#loc229 = loc("torch.nn.modules.normalization.LayerNorm::layernorm_before"(#loc184))
#loc230 = loc("transformers.models.vit.modeling_vit.ViTSdpaAttention::attention"(#loc184))
#loc231 = loc("add_106"(#loc184))
#loc232 = loc("torch.nn.modules.normalization.LayerNorm::layernorm_after"(#loc184))
#loc233 = loc("transformers.models.vit.modeling_vit.ViTIntermediate::intermediate"(#loc184))
#loc234 = loc("transformers.models.vit.modeling_vit.ViTOutput::output"(#loc184))
#loc235 = loc("torch.nn.modules.normalization.LayerNorm::layernorm_before"(#loc185))
#loc236 = loc("transformers.models.vit.modeling_vit.ViTSdpaAttention::attention"(#loc185))
#loc237 = loc("add_161"(#loc185))
#loc238 = loc("torch.nn.modules.normalization.LayerNorm::layernorm_after"(#loc185))
#loc239 = loc("transformers.models.vit.modeling_vit.ViTIntermediate::intermediate"(#loc185))
#loc240 = loc("transformers.models.vit.modeling_vit.ViTOutput::output"(#loc185))
#loc241 = loc("torch.nn.modules.normalization.LayerNorm::layernorm_before"(#loc186))
#loc242 = loc("transformers.models.vit.modeling_vit.ViTSdpaAttention::attention"(#loc186))
#loc243 = loc("add_216"(#loc186))
#loc244 = loc("torch.nn.modules.normalization.LayerNorm::layernorm_after"(#loc186))
#loc245 = loc("transformers.models.vit.modeling_vit.ViTIntermediate::intermediate"(#loc186))
#loc246 = loc("transformers.models.vit.modeling_vit.ViTOutput::output"(#loc186))
#loc247 = loc("torch.nn.modules.normalization.LayerNorm::layernorm_before"(#loc187))
#loc248 = loc("transformers.models.vit.modeling_vit.ViTSdpaAttention::attention"(#loc187))
#loc249 = loc("add_271"(#loc187))
#loc250 = loc("torch.nn.modules.normalization.LayerNorm::layernorm_after"(#loc187))
#loc251 = loc("transformers.models.vit.modeling_vit.ViTIntermediate::intermediate"(#loc187))
#loc252 = loc("transformers.models.vit.modeling_vit.ViTOutput::output"(#loc187))
#loc253 = loc("torch.nn.modules.normalization.LayerNorm::layernorm_before"(#loc188))
#loc254 = loc("transformers.models.vit.modeling_vit.ViTSdpaAttention::attention"(#loc188))
#loc255 = loc("add_326"(#loc188))
#loc256 = loc("torch.nn.modules.normalization.LayerNorm::layernorm_after"(#loc188))
#loc257 = loc("transformers.models.vit.modeling_vit.ViTIntermediate::intermediate"(#loc188))
#loc258 = loc("transformers.models.vit.modeling_vit.ViTOutput::output"(#loc188))
#loc259 = loc("torch.nn.modules.normalization.LayerNorm::layernorm_before"(#loc189))
#loc260 = loc("transformers.models.vit.modeling_vit.ViTSdpaAttention::attention"(#loc189))
#loc261 = loc("add_381"(#loc189))
#loc262 = loc("torch.nn.modules.normalization.LayerNorm::layernorm_after"(#loc189))
#loc263 = loc("transformers.models.vit.modeling_vit.ViTIntermediate::intermediate"(#loc189))
#loc264 = loc("transformers.models.vit.modeling_vit.ViTOutput::output"(#loc189))
#loc265 = loc("torch.nn.modules.normalization.LayerNorm::layernorm_before"(#loc190))
#loc266 = loc("transformers.models.vit.modeling_vit.ViTSdpaAttention::attention"(#loc190))
#loc267 = loc("add_436"(#loc190))
#loc268 = loc("torch.nn.modules.normalization.LayerNorm::layernorm_after"(#loc190))
#loc269 = loc("transformers.models.vit.modeling_vit.ViTIntermediate::intermediate"(#loc190))
#loc270 = loc("transformers.models.vit.modeling_vit.ViTOutput::output"(#loc190))
#loc271 = loc("torch.nn.modules.normalization.LayerNorm::layernorm_before"(#loc191))
#loc272 = loc("transformers.models.vit.modeling_vit.ViTSdpaAttention::attention"(#loc191))
#loc273 = loc("add_491"(#loc191))
#loc274 = loc("torch.nn.modules.normalization.LayerNorm::layernorm_after"(#loc191))
#loc275 = loc("transformers.models.vit.modeling_vit.ViTIntermediate::intermediate"(#loc191))
#loc276 = loc("transformers.models.vit.modeling_vit.ViTOutput::output"(#loc191))
#loc277 = loc("torch.nn.modules.normalization.LayerNorm::layernorm_before"(#loc192))
#loc278 = loc("transformers.models.vit.modeling_vit.ViTSdpaAttention::attention"(#loc192))
#loc279 = loc("add_546"(#loc192))
#loc280 = loc("torch.nn.modules.normalization.LayerNorm::layernorm_after"(#loc192))
#loc281 = loc("transformers.models.vit.modeling_vit.ViTIntermediate::intermediate"(#loc192))
#loc282 = loc("transformers.models.vit.modeling_vit.ViTOutput::output"(#loc192))
#loc283 = loc("torch.nn.modules.normalization.LayerNorm::layernorm_before"(#loc193))
#loc284 = loc("transformers.models.vit.modeling_vit.ViTSdpaAttention::attention"(#loc193))
#loc285 = loc("add_601"(#loc193))
#loc286 = loc("torch.nn.modules.normalization.LayerNorm::layernorm_after"(#loc193))
#loc287 = loc("transformers.models.vit.modeling_vit.ViTIntermediate::intermediate"(#loc193))
#loc288 = loc("transformers.models.vit.modeling_vit.ViTOutput::output"(#loc193))
#loc289 = loc("torch.nn.modules.normalization.LayerNorm::layernorm_before"(#loc194))
#loc290 = loc("transformers.models.vit.modeling_vit.ViTSdpaAttention::attention"(#loc194))
#loc291 = loc("add_656"(#loc194))
#loc292 = loc("torch.nn.modules.normalization.LayerNorm::layernorm_after"(#loc194))
#loc293 = loc("transformers.models.vit.modeling_vit.ViTIntermediate::intermediate"(#loc194))
#loc294 = loc("transformers.models.vit.modeling_vit.ViTOutput::output"(#loc194))
#loc295 = loc("torch.nn.modules.normalization.LayerNorm::layernorm_before"(#loc195))
#loc296 = loc("transformers.models.vit.modeling_vit.ViTSdpaAttention::attention"(#loc195))
#loc297 = loc("add_711"(#loc195))
#loc298 = loc("torch.nn.modules.normalization.LayerNorm::layernorm_after"(#loc195))
#loc299 = loc("transformers.models.vit.modeling_vit.ViTIntermediate::intermediate"(#loc195))
#loc300 = loc("transformers.models.vit.modeling_vit.ViTOutput::output"(#loc195))
#loc301 = loc("torch.nn.modules.normalization.LayerNorm::layernorm_before"(#loc196))
#loc302 = loc("transformers.models.vit.modeling_vit.ViTSdpaAttention::attention"(#loc196))
#loc303 = loc("add_766"(#loc196))
#loc304 = loc("torch.nn.modules.normalization.LayerNorm::layernorm_after"(#loc196))
#loc305 = loc("transformers.models.vit.modeling_vit.ViTIntermediate::intermediate"(#loc196))
#loc306 = loc("transformers.models.vit.modeling_vit.ViTOutput::output"(#loc196))
#loc307 = loc("torch.nn.modules.normalization.LayerNorm::layernorm_before"(#loc197))
#loc308 = loc("transformers.models.vit.modeling_vit.ViTSdpaAttention::attention"(#loc197))
#loc309 = loc("add_821"(#loc197))
#loc310 = loc("torch.nn.modules.normalization.LayerNorm::layernorm_after"(#loc197))
#loc311 = loc("transformers.models.vit.modeling_vit.ViTIntermediate::intermediate"(#loc197))
#loc312 = loc("transformers.models.vit.modeling_vit.ViTOutput::output"(#loc197))
#loc313 = loc("torch.nn.modules.normalization.LayerNorm::layernorm_before"(#loc198))
#loc314 = loc("transformers.models.vit.modeling_vit.ViTSdpaAttention::attention"(#loc198))
#loc315 = loc("add_876"(#loc198))
#loc316 = loc("torch.nn.modules.normalization.LayerNorm::layernorm_after"(#loc198))
#loc317 = loc("transformers.models.vit.modeling_vit.ViTIntermediate::intermediate"(#loc198))
#loc318 = loc("transformers.models.vit.modeling_vit.ViTOutput::output"(#loc198))
#loc319 = loc("torch.nn.modules.normalization.LayerNorm::layernorm_before"(#loc199))
#loc320 = loc("transformers.models.vit.modeling_vit.ViTSdpaAttention::attention"(#loc199))
#loc321 = loc("add_931"(#loc199))
#loc322 = loc("torch.nn.modules.normalization.LayerNorm::layernorm_after"(#loc199))
#loc323 = loc("transformers.models.vit.modeling_vit.ViTIntermediate::intermediate"(#loc199))
#loc324 = loc("transformers.models.vit.modeling_vit.ViTOutput::output"(#loc199))
#loc325 = loc("torch.nn.modules.normalization.LayerNorm::layernorm_before"(#loc200))
#loc326 = loc("transformers.models.vit.modeling_vit.ViTSdpaAttention::attention"(#loc200))
#loc327 = loc("add_986"(#loc200))
#loc328 = loc("torch.nn.modules.normalization.LayerNorm::layernorm_after"(#loc200))
#loc329 = loc("transformers.models.vit.modeling_vit.ViTIntermediate::intermediate"(#loc200))
#loc330 = loc("transformers.models.vit.modeling_vit.ViTOutput::output"(#loc200))
#loc331 = loc("torch.nn.modules.normalization.LayerNorm::layernorm_before"(#loc201))
#loc332 = loc("transformers.models.vit.modeling_vit.ViTSdpaAttention::attention"(#loc201))
#loc333 = loc("add_1041"(#loc201))
#loc334 = loc("torch.nn.modules.normalization.LayerNorm::layernorm_after"(#loc201))
#loc335 = loc("transformers.models.vit.modeling_vit.ViTIntermediate::intermediate"(#loc201))
#loc336 = loc("transformers.models.vit.modeling_vit.ViTOutput::output"(#loc201))
#loc337 = loc("torch.nn.modules.normalization.LayerNorm::layernorm_before"(#loc202))
#loc338 = loc("transformers.models.vit.modeling_vit.ViTSdpaAttention::attention"(#loc202))
#loc339 = loc("add_1096"(#loc202))
#loc340 = loc("torch.nn.modules.normalization.LayerNorm::layernorm_after"(#loc202))
#loc341 = loc("transformers.models.vit.modeling_vit.ViTIntermediate::intermediate"(#loc202))
#loc342 = loc("transformers.models.vit.modeling_vit.ViTOutput::output"(#loc202))
#loc343 = loc("torch.nn.modules.normalization.LayerNorm::layernorm_before"(#loc203))
#loc344 = loc("transformers.models.vit.modeling_vit.ViTSdpaAttention::attention"(#loc203))
#loc345 = loc("add_1151"(#loc203))
#loc346 = loc("torch.nn.modules.normalization.LayerNorm::layernorm_after"(#loc203))
#loc347 = loc("transformers.models.vit.modeling_vit.ViTIntermediate::intermediate"(#loc203))
#loc348 = loc("transformers.models.vit.modeling_vit.ViTOutput::output"(#loc203))
#loc349 = loc("torch.nn.modules.normalization.LayerNorm::layernorm_before"(#loc204))
#loc350 = loc("transformers.models.vit.modeling_vit.ViTSdpaAttention::attention"(#loc204))
#loc351 = loc("add_1206"(#loc204))
#loc352 = loc("torch.nn.modules.normalization.LayerNorm::layernorm_after"(#loc204))
#loc353 = loc("transformers.models.vit.modeling_vit.ViTIntermediate::intermediate"(#loc204))
#loc354 = loc("transformers.models.vit.modeling_vit.ViTOutput::output"(#loc204))
#loc355 = loc("torch.nn.modules.normalization.LayerNorm::layernorm_before"(#loc205))
#loc356 = loc("transformers.models.vit.modeling_vit.ViTSdpaAttention::attention"(#loc205))
#loc357 = loc("add_1261"(#loc205))
#loc358 = loc("torch.nn.modules.normalization.LayerNorm::layernorm_after"(#loc205))
#loc359 = loc("transformers.models.vit.modeling_vit.ViTIntermediate::intermediate"(#loc205))
#loc360 = loc("transformers.models.vit.modeling_vit.ViTOutput::output"(#loc205))
#loc361 = loc("torch.nn.modules.normalization.LayerNorm::layernorm_before"(#loc206))
#loc362 = loc("transformers.models.vit.modeling_vit.ViTSdpaAttention::attention"(#loc206))
#loc363 = loc("add_1316"(#loc206))
#loc364 = loc("torch.nn.modules.normalization.LayerNorm::layernorm_after"(#loc206))
#loc365 = loc("transformers.models.vit.modeling_vit.ViTIntermediate::intermediate"(#loc206))
#loc366 = loc("transformers.models.vit.modeling_vit.ViTOutput::output"(#loc206))
#loc367 = loc("conv2d_1.dc.transpose.0"(#loc219))
#loc368 = loc("conv2d_1.dc.transpose.1"(#loc219))
#loc369 = loc("conv2d_1.dc.conv2d.4"(#loc219))
#loc370 = loc("conv2d_1.dc.transpose.5"(#loc219))
#loc371 = loc("conv2d_1.dc.transpose.6"(#loc219))
#loc372 = loc("layernorm_11.dc.reduce_sum.0"(#loc223))
#loc373 = loc("layernorm_11.dc.multiply.2"(#loc223))
#loc374 = loc("layernorm_11.dc.subtract.3"(#loc223))
#loc375 = loc("layernorm_11.dc.multiply.4"(#loc223))
#loc376 = loc("layernorm_11.dc.reduce_sum.5"(#loc223))
#loc377 = loc("layernorm_11.dc.multiply.7"(#loc223))
#loc378 = loc("layernorm_11.dc.add.9"(#loc223))
#loc379 = loc("layernorm_11.dc.sqrt.10"(#loc223))
#loc380 = loc("layernorm_11.dc.reciprocal.11"(#loc223))
#loc381 = loc("layernorm_11.dc.multiply.12"(#loc223))
#loc382 = loc("layernorm_11.dc.multiply.13"(#loc223))
#loc383 = loc("layernorm_11.dc.add.14"(#loc223))
#loc384 = loc("transformers.models.vit.modeling_vit.ViTSdpaSelfAttention::attention"(#loc224))
#loc385 = loc("transformers.models.vit.modeling_vit.ViTSelfOutput::output"(#loc224))
#loc386 = loc("layernorm_52.dc.reduce_sum.0"(#loc226))
#loc387 = loc("layernorm_52.dc.multiply.2"(#loc226))
#loc388 = loc("layernorm_52.dc.subtract.3"(#loc226))
#loc389 = loc("layernorm_52.dc.multiply.4"(#loc226))
#loc390 = loc("layernorm_52.dc.reduce_sum.5"(#loc226))
#loc391 = loc("layernorm_52.dc.multiply.7"(#loc226))
#loc392 = loc("layernorm_52.dc.add.9"(#loc226))
#loc393 = loc("layernorm_52.dc.sqrt.10"(#loc226))
#loc394 = loc("layernorm_52.dc.reciprocal.11"(#loc226))
#loc395 = loc("layernorm_52.dc.multiply.12"(#loc226))
#loc396 = loc("layernorm_52.dc.multiply.13"(#loc226))
#loc397 = loc("layernorm_52.dc.add.14"(#loc226))
#loc398 = loc("torch.nn.modules.linear.Linear::dense"(#loc227))
#loc399 = loc("transformers.activations.GELUActivation::intermediate_act_fn"(#loc227))
#loc400 = loc("torch.nn.modules.linear.Linear::dense"(#loc228))
#loc401 = loc("add_65"(#loc228))
#loc402 = loc("layernorm_66.dc.reduce_sum.0"(#loc229))
#loc403 = loc("layernorm_66.dc.multiply.2"(#loc229))
#loc404 = loc("layernorm_66.dc.subtract.3"(#loc229))
#loc405 = loc("layernorm_66.dc.multiply.4"(#loc229))
#loc406 = loc("layernorm_66.dc.reduce_sum.5"(#loc229))
#loc407 = loc("layernorm_66.dc.multiply.7"(#loc229))
#loc408 = loc("layernorm_66.dc.add.9"(#loc229))
#loc409 = loc("layernorm_66.dc.sqrt.10"(#loc229))
#loc410 = loc("layernorm_66.dc.reciprocal.11"(#loc229))
#loc411 = loc("layernorm_66.dc.multiply.12"(#loc229))
#loc412 = loc("layernorm_66.dc.multiply.13"(#loc229))
#loc413 = loc("layernorm_66.dc.add.14"(#loc229))
#loc414 = loc("transformers.models.vit.modeling_vit.ViTSdpaSelfAttention::attention"(#loc230))
#loc415 = loc("transformers.models.vit.modeling_vit.ViTSelfOutput::output"(#loc230))
#loc416 = loc("layernorm_107.dc.reduce_sum.0"(#loc232))
#loc417 = loc("layernorm_107.dc.multiply.2"(#loc232))
#loc418 = loc("layernorm_107.dc.subtract.3"(#loc232))
#loc419 = loc("layernorm_107.dc.multiply.4"(#loc232))
#loc420 = loc("layernorm_107.dc.reduce_sum.5"(#loc232))
#loc421 = loc("layernorm_107.dc.multiply.7"(#loc232))
#loc422 = loc("layernorm_107.dc.add.9"(#loc232))
#loc423 = loc("layernorm_107.dc.sqrt.10"(#loc232))
#loc424 = loc("layernorm_107.dc.reciprocal.11"(#loc232))
#loc425 = loc("layernorm_107.dc.multiply.12"(#loc232))
#loc426 = loc("layernorm_107.dc.multiply.13"(#loc232))
#loc427 = loc("layernorm_107.dc.add.14"(#loc232))
#loc428 = loc("torch.nn.modules.linear.Linear::dense"(#loc233))
#loc429 = loc("transformers.activations.GELUActivation::intermediate_act_fn"(#loc233))
#loc430 = loc("torch.nn.modules.linear.Linear::dense"(#loc234))
#loc431 = loc("add_120"(#loc234))
#loc432 = loc("layernorm_121.dc.reduce_sum.0"(#loc235))
#loc433 = loc("layernorm_121.dc.multiply.2"(#loc235))
#loc434 = loc("layernorm_121.dc.subtract.3"(#loc235))
#loc435 = loc("layernorm_121.dc.multiply.4"(#loc235))
#loc436 = loc("layernorm_121.dc.reduce_sum.5"(#loc235))
#loc437 = loc("layernorm_121.dc.multiply.7"(#loc235))
#loc438 = loc("layernorm_121.dc.add.9"(#loc235))
#loc439 = loc("layernorm_121.dc.sqrt.10"(#loc235))
#loc440 = loc("layernorm_121.dc.reciprocal.11"(#loc235))
#loc441 = loc("layernorm_121.dc.multiply.12"(#loc235))
#loc442 = loc("layernorm_121.dc.multiply.13"(#loc235))
#loc443 = loc("layernorm_121.dc.add.14"(#loc235))
#loc444 = loc("transformers.models.vit.modeling_vit.ViTSdpaSelfAttention::attention"(#loc236))
#loc445 = loc("transformers.models.vit.modeling_vit.ViTSelfOutput::output"(#loc236))
#loc446 = loc("layernorm_162.dc.reduce_sum.0"(#loc238))
#loc447 = loc("layernorm_162.dc.multiply.2"(#loc238))
#loc448 = loc("layernorm_162.dc.subtract.3"(#loc238))
#loc449 = loc("layernorm_162.dc.multiply.4"(#loc238))
#loc450 = loc("layernorm_162.dc.reduce_sum.5"(#loc238))
#loc451 = loc("layernorm_162.dc.multiply.7"(#loc238))
#loc452 = loc("layernorm_162.dc.add.9"(#loc238))
#loc453 = loc("layernorm_162.dc.sqrt.10"(#loc238))
#loc454 = loc("layernorm_162.dc.reciprocal.11"(#loc238))
#loc455 = loc("layernorm_162.dc.multiply.12"(#loc238))
#loc456 = loc("layernorm_162.dc.multiply.13"(#loc238))
#loc457 = loc("layernorm_162.dc.add.14"(#loc238))
#loc458 = loc("torch.nn.modules.linear.Linear::dense"(#loc239))
#loc459 = loc("transformers.activations.GELUActivation::intermediate_act_fn"(#loc239))
#loc460 = loc("torch.nn.modules.linear.Linear::dense"(#loc240))
#loc461 = loc("add_175"(#loc240))
#loc462 = loc("layernorm_176.dc.reduce_sum.0"(#loc241))
#loc463 = loc("layernorm_176.dc.multiply.2"(#loc241))
#loc464 = loc("layernorm_176.dc.subtract.3"(#loc241))
#loc465 = loc("layernorm_176.dc.multiply.4"(#loc241))
#loc466 = loc("layernorm_176.dc.reduce_sum.5"(#loc241))
#loc467 = loc("layernorm_176.dc.multiply.7"(#loc241))
#loc468 = loc("layernorm_176.dc.add.9"(#loc241))
#loc469 = loc("layernorm_176.dc.sqrt.10"(#loc241))
#loc470 = loc("layernorm_176.dc.reciprocal.11"(#loc241))
#loc471 = loc("layernorm_176.dc.multiply.12"(#loc241))
#loc472 = loc("layernorm_176.dc.multiply.13"(#loc241))
#loc473 = loc("layernorm_176.dc.add.14"(#loc241))
#loc474 = loc("transformers.models.vit.modeling_vit.ViTSdpaSelfAttention::attention"(#loc242))
#loc475 = loc("transformers.models.vit.modeling_vit.ViTSelfOutput::output"(#loc242))
#loc476 = loc("layernorm_217.dc.reduce_sum.0"(#loc244))
#loc477 = loc("layernorm_217.dc.multiply.2"(#loc244))
#loc478 = loc("layernorm_217.dc.subtract.3"(#loc244))
#loc479 = loc("layernorm_217.dc.multiply.4"(#loc244))
#loc480 = loc("layernorm_217.dc.reduce_sum.5"(#loc244))
#loc481 = loc("layernorm_217.dc.multiply.7"(#loc244))
#loc482 = loc("layernorm_217.dc.add.9"(#loc244))
#loc483 = loc("layernorm_217.dc.sqrt.10"(#loc244))
#loc484 = loc("layernorm_217.dc.reciprocal.11"(#loc244))
#loc485 = loc("layernorm_217.dc.multiply.12"(#loc244))
#loc486 = loc("layernorm_217.dc.multiply.13"(#loc244))
#loc487 = loc("layernorm_217.dc.add.14"(#loc244))
#loc488 = loc("torch.nn.modules.linear.Linear::dense"(#loc245))
#loc489 = loc("transformers.activations.GELUActivation::intermediate_act_fn"(#loc245))
#loc490 = loc("torch.nn.modules.linear.Linear::dense"(#loc246))
#loc491 = loc("add_230"(#loc246))
#loc492 = loc("layernorm_231.dc.reduce_sum.0"(#loc247))
#loc493 = loc("layernorm_231.dc.multiply.2"(#loc247))
#loc494 = loc("layernorm_231.dc.subtract.3"(#loc247))
#loc495 = loc("layernorm_231.dc.multiply.4"(#loc247))
#loc496 = loc("layernorm_231.dc.reduce_sum.5"(#loc247))
#loc497 = loc("layernorm_231.dc.multiply.7"(#loc247))
#loc498 = loc("layernorm_231.dc.add.9"(#loc247))
#loc499 = loc("layernorm_231.dc.sqrt.10"(#loc247))
#loc500 = loc("layernorm_231.dc.reciprocal.11"(#loc247))
#loc501 = loc("layernorm_231.dc.multiply.12"(#loc247))
#loc502 = loc("layernorm_231.dc.multiply.13"(#loc247))
#loc503 = loc("layernorm_231.dc.add.14"(#loc247))
#loc504 = loc("transformers.models.vit.modeling_vit.ViTSdpaSelfAttention::attention"(#loc248))
#loc505 = loc("transformers.models.vit.modeling_vit.ViTSelfOutput::output"(#loc248))
#loc506 = loc("layernorm_272.dc.reduce_sum.0"(#loc250))
#loc507 = loc("layernorm_272.dc.multiply.2"(#loc250))
#loc508 = loc("layernorm_272.dc.subtract.3"(#loc250))
#loc509 = loc("layernorm_272.dc.multiply.4"(#loc250))
#loc510 = loc("layernorm_272.dc.reduce_sum.5"(#loc250))
#loc511 = loc("layernorm_272.dc.multiply.7"(#loc250))
#loc512 = loc("layernorm_272.dc.add.9"(#loc250))
#loc513 = loc("layernorm_272.dc.sqrt.10"(#loc250))
#loc514 = loc("layernorm_272.dc.reciprocal.11"(#loc250))
#loc515 = loc("layernorm_272.dc.multiply.12"(#loc250))
#loc516 = loc("layernorm_272.dc.multiply.13"(#loc250))
#loc517 = loc("layernorm_272.dc.add.14"(#loc250))
#loc518 = loc("torch.nn.modules.linear.Linear::dense"(#loc251))
#loc519 = loc("transformers.activations.GELUActivation::intermediate_act_fn"(#loc251))
#loc520 = loc("torch.nn.modules.linear.Linear::dense"(#loc252))
#loc521 = loc("add_285"(#loc252))
#loc522 = loc("layernorm_286.dc.reduce_sum.0"(#loc253))
#loc523 = loc("layernorm_286.dc.multiply.2"(#loc253))
#loc524 = loc("layernorm_286.dc.subtract.3"(#loc253))
#loc525 = loc("layernorm_286.dc.multiply.4"(#loc253))
#loc526 = loc("layernorm_286.dc.reduce_sum.5"(#loc253))
#loc527 = loc("layernorm_286.dc.multiply.7"(#loc253))
#loc528 = loc("layernorm_286.dc.add.9"(#loc253))
#loc529 = loc("layernorm_286.dc.sqrt.10"(#loc253))
#loc530 = loc("layernorm_286.dc.reciprocal.11"(#loc253))
#loc531 = loc("layernorm_286.dc.multiply.12"(#loc253))
#loc532 = loc("layernorm_286.dc.multiply.13"(#loc253))
#loc533 = loc("layernorm_286.dc.add.14"(#loc253))
#loc534 = loc("transformers.models.vit.modeling_vit.ViTSdpaSelfAttention::attention"(#loc254))
#loc535 = loc("transformers.models.vit.modeling_vit.ViTSelfOutput::output"(#loc254))
#loc536 = loc("layernorm_327.dc.reduce_sum.0"(#loc256))
#loc537 = loc("layernorm_327.dc.multiply.2"(#loc256))
#loc538 = loc("layernorm_327.dc.subtract.3"(#loc256))
#loc539 = loc("layernorm_327.dc.multiply.4"(#loc256))
#loc540 = loc("layernorm_327.dc.reduce_sum.5"(#loc256))
#loc541 = loc("layernorm_327.dc.multiply.7"(#loc256))
#loc542 = loc("layernorm_327.dc.add.9"(#loc256))
#loc543 = loc("layernorm_327.dc.sqrt.10"(#loc256))
#loc544 = loc("layernorm_327.dc.reciprocal.11"(#loc256))
#loc545 = loc("layernorm_327.dc.multiply.12"(#loc256))
#loc546 = loc("layernorm_327.dc.multiply.13"(#loc256))
#loc547 = loc("layernorm_327.dc.add.14"(#loc256))
#loc548 = loc("torch.nn.modules.linear.Linear::dense"(#loc257))
#loc549 = loc("transformers.activations.GELUActivation::intermediate_act_fn"(#loc257))
#loc550 = loc("torch.nn.modules.linear.Linear::dense"(#loc258))
#loc551 = loc("add_340"(#loc258))
#loc552 = loc("layernorm_341.dc.reduce_sum.0"(#loc259))
#loc553 = loc("layernorm_341.dc.multiply.2"(#loc259))
#loc554 = loc("layernorm_341.dc.subtract.3"(#loc259))
#loc555 = loc("layernorm_341.dc.multiply.4"(#loc259))
#loc556 = loc("layernorm_341.dc.reduce_sum.5"(#loc259))
#loc557 = loc("layernorm_341.dc.multiply.7"(#loc259))
#loc558 = loc("layernorm_341.dc.add.9"(#loc259))
#loc559 = loc("layernorm_341.dc.sqrt.10"(#loc259))
#loc560 = loc("layernorm_341.dc.reciprocal.11"(#loc259))
#loc561 = loc("layernorm_341.dc.multiply.12"(#loc259))
#loc562 = loc("layernorm_341.dc.multiply.13"(#loc259))
#loc563 = loc("layernorm_341.dc.add.14"(#loc259))
#loc564 = loc("transformers.models.vit.modeling_vit.ViTSdpaSelfAttention::attention"(#loc260))
#loc565 = loc("transformers.models.vit.modeling_vit.ViTSelfOutput::output"(#loc260))
#loc566 = loc("layernorm_382.dc.reduce_sum.0"(#loc262))
#loc567 = loc("layernorm_382.dc.multiply.2"(#loc262))
#loc568 = loc("layernorm_382.dc.subtract.3"(#loc262))
#loc569 = loc("layernorm_382.dc.multiply.4"(#loc262))
#loc570 = loc("layernorm_382.dc.reduce_sum.5"(#loc262))
#loc571 = loc("layernorm_382.dc.multiply.7"(#loc262))
#loc572 = loc("layernorm_382.dc.add.9"(#loc262))
#loc573 = loc("layernorm_382.dc.sqrt.10"(#loc262))
#loc574 = loc("layernorm_382.dc.reciprocal.11"(#loc262))
#loc575 = loc("layernorm_382.dc.multiply.12"(#loc262))
#loc576 = loc("layernorm_382.dc.multiply.13"(#loc262))
#loc577 = loc("layernorm_382.dc.add.14"(#loc262))
#loc578 = loc("torch.nn.modules.linear.Linear::dense"(#loc263))
#loc579 = loc("transformers.activations.GELUActivation::intermediate_act_fn"(#loc263))
#loc580 = loc("torch.nn.modules.linear.Linear::dense"(#loc264))
#loc581 = loc("add_395"(#loc264))
#loc582 = loc("layernorm_396.dc.reduce_sum.0"(#loc265))
#loc583 = loc("layernorm_396.dc.multiply.2"(#loc265))
#loc584 = loc("layernorm_396.dc.subtract.3"(#loc265))
#loc585 = loc("layernorm_396.dc.multiply.4"(#loc265))
#loc586 = loc("layernorm_396.dc.reduce_sum.5"(#loc265))
#loc587 = loc("layernorm_396.dc.multiply.7"(#loc265))
#loc588 = loc("layernorm_396.dc.add.9"(#loc265))
#loc589 = loc("layernorm_396.dc.sqrt.10"(#loc265))
#loc590 = loc("layernorm_396.dc.reciprocal.11"(#loc265))
#loc591 = loc("layernorm_396.dc.multiply.12"(#loc265))
#loc592 = loc("layernorm_396.dc.multiply.13"(#loc265))
#loc593 = loc("layernorm_396.dc.add.14"(#loc265))
#loc594 = loc("transformers.models.vit.modeling_vit.ViTSdpaSelfAttention::attention"(#loc266))
#loc595 = loc("transformers.models.vit.modeling_vit.ViTSelfOutput::output"(#loc266))
#loc596 = loc("layernorm_437.dc.reduce_sum.0"(#loc268))
#loc597 = loc("layernorm_437.dc.multiply.2"(#loc268))
#loc598 = loc("layernorm_437.dc.subtract.3"(#loc268))
#loc599 = loc("layernorm_437.dc.multiply.4"(#loc268))
#loc600 = loc("layernorm_437.dc.reduce_sum.5"(#loc268))
#loc601 = loc("layernorm_437.dc.multiply.7"(#loc268))
#loc602 = loc("layernorm_437.dc.add.9"(#loc268))
#loc603 = loc("layernorm_437.dc.sqrt.10"(#loc268))
#loc604 = loc("layernorm_437.dc.reciprocal.11"(#loc268))
#loc605 = loc("layernorm_437.dc.multiply.12"(#loc268))
#loc606 = loc("layernorm_437.dc.multiply.13"(#loc268))
#loc607 = loc("layernorm_437.dc.add.14"(#loc268))
#loc608 = loc("torch.nn.modules.linear.Linear::dense"(#loc269))
#loc609 = loc("transformers.activations.GELUActivation::intermediate_act_fn"(#loc269))
#loc610 = loc("torch.nn.modules.linear.Linear::dense"(#loc270))
#loc611 = loc("add_450"(#loc270))
#loc612 = loc("layernorm_451.dc.reduce_sum.0"(#loc271))
#loc613 = loc("layernorm_451.dc.multiply.2"(#loc271))
#loc614 = loc("layernorm_451.dc.subtract.3"(#loc271))
#loc615 = loc("layernorm_451.dc.multiply.4"(#loc271))
#loc616 = loc("layernorm_451.dc.reduce_sum.5"(#loc271))
#loc617 = loc("layernorm_451.dc.multiply.7"(#loc271))
#loc618 = loc("layernorm_451.dc.add.9"(#loc271))
#loc619 = loc("layernorm_451.dc.sqrt.10"(#loc271))
#loc620 = loc("layernorm_451.dc.reciprocal.11"(#loc271))
#loc621 = loc("layernorm_451.dc.multiply.12"(#loc271))
#loc622 = loc("layernorm_451.dc.multiply.13"(#loc271))
#loc623 = loc("layernorm_451.dc.add.14"(#loc271))
#loc624 = loc("transformers.models.vit.modeling_vit.ViTSdpaSelfAttention::attention"(#loc272))
#loc625 = loc("transformers.models.vit.modeling_vit.ViTSelfOutput::output"(#loc272))
#loc626 = loc("layernorm_492.dc.reduce_sum.0"(#loc274))
#loc627 = loc("layernorm_492.dc.multiply.2"(#loc274))
#loc628 = loc("layernorm_492.dc.subtract.3"(#loc274))
#loc629 = loc("layernorm_492.dc.multiply.4"(#loc274))
#loc630 = loc("layernorm_492.dc.reduce_sum.5"(#loc274))
#loc631 = loc("layernorm_492.dc.multiply.7"(#loc274))
#loc632 = loc("layernorm_492.dc.add.9"(#loc274))
#loc633 = loc("layernorm_492.dc.sqrt.10"(#loc274))
#loc634 = loc("layernorm_492.dc.reciprocal.11"(#loc274))
#loc635 = loc("layernorm_492.dc.multiply.12"(#loc274))
#loc636 = loc("layernorm_492.dc.multiply.13"(#loc274))
#loc637 = loc("layernorm_492.dc.add.14"(#loc274))
#loc638 = loc("torch.nn.modules.linear.Linear::dense"(#loc275))
#loc639 = loc("transformers.activations.GELUActivation::intermediate_act_fn"(#loc275))
#loc640 = loc("torch.nn.modules.linear.Linear::dense"(#loc276))
#loc641 = loc("add_505"(#loc276))
#loc642 = loc("layernorm_506.dc.reduce_sum.0"(#loc277))
#loc643 = loc("layernorm_506.dc.multiply.2"(#loc277))
#loc644 = loc("layernorm_506.dc.subtract.3"(#loc277))
#loc645 = loc("layernorm_506.dc.multiply.4"(#loc277))
#loc646 = loc("layernorm_506.dc.reduce_sum.5"(#loc277))
#loc647 = loc("layernorm_506.dc.multiply.7"(#loc277))
#loc648 = loc("layernorm_506.dc.add.9"(#loc277))
#loc649 = loc("layernorm_506.dc.sqrt.10"(#loc277))
#loc650 = loc("layernorm_506.dc.reciprocal.11"(#loc277))
#loc651 = loc("layernorm_506.dc.multiply.12"(#loc277))
#loc652 = loc("layernorm_506.dc.multiply.13"(#loc277))
#loc653 = loc("layernorm_506.dc.add.14"(#loc277))
#loc654 = loc("transformers.models.vit.modeling_vit.ViTSdpaSelfAttention::attention"(#loc278))
#loc655 = loc("transformers.models.vit.modeling_vit.ViTSelfOutput::output"(#loc278))
#loc656 = loc("layernorm_547.dc.reduce_sum.0"(#loc280))
#loc657 = loc("layernorm_547.dc.multiply.2"(#loc280))
#loc658 = loc("layernorm_547.dc.subtract.3"(#loc280))
#loc659 = loc("layernorm_547.dc.multiply.4"(#loc280))
#loc660 = loc("layernorm_547.dc.reduce_sum.5"(#loc280))
#loc661 = loc("layernorm_547.dc.multiply.7"(#loc280))
#loc662 = loc("layernorm_547.dc.add.9"(#loc280))
#loc663 = loc("layernorm_547.dc.sqrt.10"(#loc280))
#loc664 = loc("layernorm_547.dc.reciprocal.11"(#loc280))
#loc665 = loc("layernorm_547.dc.multiply.12"(#loc280))
#loc666 = loc("layernorm_547.dc.multiply.13"(#loc280))
#loc667 = loc("layernorm_547.dc.add.14"(#loc280))
#loc668 = loc("torch.nn.modules.linear.Linear::dense"(#loc281))
#loc669 = loc("transformers.activations.GELUActivation::intermediate_act_fn"(#loc281))
#loc670 = loc("torch.nn.modules.linear.Linear::dense"(#loc282))
#loc671 = loc("add_560"(#loc282))
#loc672 = loc("layernorm_561.dc.reduce_sum.0"(#loc283))
#loc673 = loc("layernorm_561.dc.multiply.2"(#loc283))
#loc674 = loc("layernorm_561.dc.subtract.3"(#loc283))
#loc675 = loc("layernorm_561.dc.multiply.4"(#loc283))
#loc676 = loc("layernorm_561.dc.reduce_sum.5"(#loc283))
#loc677 = loc("layernorm_561.dc.multiply.7"(#loc283))
#loc678 = loc("layernorm_561.dc.add.9"(#loc283))
#loc679 = loc("layernorm_561.dc.sqrt.10"(#loc283))
#loc680 = loc("layernorm_561.dc.reciprocal.11"(#loc283))
#loc681 = loc("layernorm_561.dc.multiply.12"(#loc283))
#loc682 = loc("layernorm_561.dc.multiply.13"(#loc283))
#loc683 = loc("layernorm_561.dc.add.14"(#loc283))
#loc684 = loc("transformers.models.vit.modeling_vit.ViTSdpaSelfAttention::attention"(#loc284))
#loc685 = loc("transformers.models.vit.modeling_vit.ViTSelfOutput::output"(#loc284))
#loc686 = loc("layernorm_602.dc.reduce_sum.0"(#loc286))
#loc687 = loc("layernorm_602.dc.multiply.2"(#loc286))
#loc688 = loc("layernorm_602.dc.subtract.3"(#loc286))
#loc689 = loc("layernorm_602.dc.multiply.4"(#loc286))
#loc690 = loc("layernorm_602.dc.reduce_sum.5"(#loc286))
#loc691 = loc("layernorm_602.dc.multiply.7"(#loc286))
#loc692 = loc("layernorm_602.dc.add.9"(#loc286))
#loc693 = loc("layernorm_602.dc.sqrt.10"(#loc286))
#loc694 = loc("layernorm_602.dc.reciprocal.11"(#loc286))
#loc695 = loc("layernorm_602.dc.multiply.12"(#loc286))
#loc696 = loc("layernorm_602.dc.multiply.13"(#loc286))
#loc697 = loc("layernorm_602.dc.add.14"(#loc286))
#loc698 = loc("torch.nn.modules.linear.Linear::dense"(#loc287))
#loc699 = loc("transformers.activations.GELUActivation::intermediate_act_fn"(#loc287))
#loc700 = loc("torch.nn.modules.linear.Linear::dense"(#loc288))
#loc701 = loc("add_615"(#loc288))
#loc702 = loc("layernorm_616.dc.reduce_sum.0"(#loc289))
#loc703 = loc("layernorm_616.dc.multiply.2"(#loc289))
#loc704 = loc("layernorm_616.dc.subtract.3"(#loc289))
#loc705 = loc("layernorm_616.dc.multiply.4"(#loc289))
#loc706 = loc("layernorm_616.dc.reduce_sum.5"(#loc289))
#loc707 = loc("layernorm_616.dc.multiply.7"(#loc289))
#loc708 = loc("layernorm_616.dc.add.9"(#loc289))
#loc709 = loc("layernorm_616.dc.sqrt.10"(#loc289))
#loc710 = loc("layernorm_616.dc.reciprocal.11"(#loc289))
#loc711 = loc("layernorm_616.dc.multiply.12"(#loc289))
#loc712 = loc("layernorm_616.dc.multiply.13"(#loc289))
#loc713 = loc("layernorm_616.dc.add.14"(#loc289))
#loc714 = loc("transformers.models.vit.modeling_vit.ViTSdpaSelfAttention::attention"(#loc290))
#loc715 = loc("transformers.models.vit.modeling_vit.ViTSelfOutput::output"(#loc290))
#loc716 = loc("layernorm_657.dc.reduce_sum.0"(#loc292))
#loc717 = loc("layernorm_657.dc.multiply.2"(#loc292))
#loc718 = loc("layernorm_657.dc.subtract.3"(#loc292))
#loc719 = loc("layernorm_657.dc.multiply.4"(#loc292))
#loc720 = loc("layernorm_657.dc.reduce_sum.5"(#loc292))
#loc721 = loc("layernorm_657.dc.multiply.7"(#loc292))
#loc722 = loc("layernorm_657.dc.add.9"(#loc292))
#loc723 = loc("layernorm_657.dc.sqrt.10"(#loc292))
#loc724 = loc("layernorm_657.dc.reciprocal.11"(#loc292))
#loc725 = loc("layernorm_657.dc.multiply.12"(#loc292))
#loc726 = loc("layernorm_657.dc.multiply.13"(#loc292))
#loc727 = loc("layernorm_657.dc.add.14"(#loc292))
#loc728 = loc("torch.nn.modules.linear.Linear::dense"(#loc293))
#loc729 = loc("transformers.activations.GELUActivation::intermediate_act_fn"(#loc293))
#loc730 = loc("torch.nn.modules.linear.Linear::dense"(#loc294))
#loc731 = loc("add_670"(#loc294))
#loc732 = loc("layernorm_671.dc.reduce_sum.0"(#loc295))
#loc733 = loc("layernorm_671.dc.multiply.2"(#loc295))
#loc734 = loc("layernorm_671.dc.subtract.3"(#loc295))
#loc735 = loc("layernorm_671.dc.multiply.4"(#loc295))
#loc736 = loc("layernorm_671.dc.reduce_sum.5"(#loc295))
#loc737 = loc("layernorm_671.dc.multiply.7"(#loc295))
#loc738 = loc("layernorm_671.dc.add.9"(#loc295))
#loc739 = loc("layernorm_671.dc.sqrt.10"(#loc295))
#loc740 = loc("layernorm_671.dc.reciprocal.11"(#loc295))
#loc741 = loc("layernorm_671.dc.multiply.12"(#loc295))
#loc742 = loc("layernorm_671.dc.multiply.13"(#loc295))
#loc743 = loc("layernorm_671.dc.add.14"(#loc295))
#loc744 = loc("transformers.models.vit.modeling_vit.ViTSdpaSelfAttention::attention"(#loc296))
#loc745 = loc("transformers.models.vit.modeling_vit.ViTSelfOutput::output"(#loc296))
#loc746 = loc("layernorm_712.dc.reduce_sum.0"(#loc298))
#loc747 = loc("layernorm_712.dc.multiply.2"(#loc298))
#loc748 = loc("layernorm_712.dc.subtract.3"(#loc298))
#loc749 = loc("layernorm_712.dc.multiply.4"(#loc298))
#loc750 = loc("layernorm_712.dc.reduce_sum.5"(#loc298))
#loc751 = loc("layernorm_712.dc.multiply.7"(#loc298))
#loc752 = loc("layernorm_712.dc.add.9"(#loc298))
#loc753 = loc("layernorm_712.dc.sqrt.10"(#loc298))
#loc754 = loc("layernorm_712.dc.reciprocal.11"(#loc298))
#loc755 = loc("layernorm_712.dc.multiply.12"(#loc298))
#loc756 = loc("layernorm_712.dc.multiply.13"(#loc298))
#loc757 = loc("layernorm_712.dc.add.14"(#loc298))
#loc758 = loc("torch.nn.modules.linear.Linear::dense"(#loc299))
#loc759 = loc("transformers.activations.GELUActivation::intermediate_act_fn"(#loc299))
#loc760 = loc("torch.nn.modules.linear.Linear::dense"(#loc300))
#loc761 = loc("add_725"(#loc300))
#loc762 = loc("layernorm_726.dc.reduce_sum.0"(#loc301))
#loc763 = loc("layernorm_726.dc.multiply.2"(#loc301))
#loc764 = loc("layernorm_726.dc.subtract.3"(#loc301))
#loc765 = loc("layernorm_726.dc.multiply.4"(#loc301))
#loc766 = loc("layernorm_726.dc.reduce_sum.5"(#loc301))
#loc767 = loc("layernorm_726.dc.multiply.7"(#loc301))
#loc768 = loc("layernorm_726.dc.add.9"(#loc301))
#loc769 = loc("layernorm_726.dc.sqrt.10"(#loc301))
#loc770 = loc("layernorm_726.dc.reciprocal.11"(#loc301))
#loc771 = loc("layernorm_726.dc.multiply.12"(#loc301))
#loc772 = loc("layernorm_726.dc.multiply.13"(#loc301))
#loc773 = loc("layernorm_726.dc.add.14"(#loc301))
#loc774 = loc("transformers.models.vit.modeling_vit.ViTSdpaSelfAttention::attention"(#loc302))
#loc775 = loc("transformers.models.vit.modeling_vit.ViTSelfOutput::output"(#loc302))
#loc776 = loc("layernorm_767.dc.reduce_sum.0"(#loc304))
#loc777 = loc("layernorm_767.dc.multiply.2"(#loc304))
#loc778 = loc("layernorm_767.dc.subtract.3"(#loc304))
#loc779 = loc("layernorm_767.dc.multiply.4"(#loc304))
#loc780 = loc("layernorm_767.dc.reduce_sum.5"(#loc304))
#loc781 = loc("layernorm_767.dc.multiply.7"(#loc304))
#loc782 = loc("layernorm_767.dc.add.9"(#loc304))
#loc783 = loc("layernorm_767.dc.sqrt.10"(#loc304))
#loc784 = loc("layernorm_767.dc.reciprocal.11"(#loc304))
#loc785 = loc("layernorm_767.dc.multiply.12"(#loc304))
#loc786 = loc("layernorm_767.dc.multiply.13"(#loc304))
#loc787 = loc("layernorm_767.dc.add.14"(#loc304))
#loc788 = loc("torch.nn.modules.linear.Linear::dense"(#loc305))
#loc789 = loc("transformers.activations.GELUActivation::intermediate_act_fn"(#loc305))
#loc790 = loc("torch.nn.modules.linear.Linear::dense"(#loc306))
#loc791 = loc("add_780"(#loc306))
#loc792 = loc("layernorm_781.dc.reduce_sum.0"(#loc307))
#loc793 = loc("layernorm_781.dc.multiply.2"(#loc307))
#loc794 = loc("layernorm_781.dc.subtract.3"(#loc307))
#loc795 = loc("layernorm_781.dc.multiply.4"(#loc307))
#loc796 = loc("layernorm_781.dc.reduce_sum.5"(#loc307))
#loc797 = loc("layernorm_781.dc.multiply.7"(#loc307))
#loc798 = loc("layernorm_781.dc.add.9"(#loc307))
#loc799 = loc("layernorm_781.dc.sqrt.10"(#loc307))
#loc800 = loc("layernorm_781.dc.reciprocal.11"(#loc307))
#loc801 = loc("layernorm_781.dc.multiply.12"(#loc307))
#loc802 = loc("layernorm_781.dc.multiply.13"(#loc307))
#loc803 = loc("layernorm_781.dc.add.14"(#loc307))
#loc804 = loc("transformers.models.vit.modeling_vit.ViTSdpaSelfAttention::attention"(#loc308))
#loc805 = loc("transformers.models.vit.modeling_vit.ViTSelfOutput::output"(#loc308))
#loc806 = loc("layernorm_822.dc.reduce_sum.0"(#loc310))
#loc807 = loc("layernorm_822.dc.multiply.2"(#loc310))
#loc808 = loc("layernorm_822.dc.subtract.3"(#loc310))
#loc809 = loc("layernorm_822.dc.multiply.4"(#loc310))
#loc810 = loc("layernorm_822.dc.reduce_sum.5"(#loc310))
#loc811 = loc("layernorm_822.dc.multiply.7"(#loc310))
#loc812 = loc("layernorm_822.dc.add.9"(#loc310))
#loc813 = loc("layernorm_822.dc.sqrt.10"(#loc310))
#loc814 = loc("layernorm_822.dc.reciprocal.11"(#loc310))
#loc815 = loc("layernorm_822.dc.multiply.12"(#loc310))
#loc816 = loc("layernorm_822.dc.multiply.13"(#loc310))
#loc817 = loc("layernorm_822.dc.add.14"(#loc310))
#loc818 = loc("torch.nn.modules.linear.Linear::dense"(#loc311))
#loc819 = loc("transformers.activations.GELUActivation::intermediate_act_fn"(#loc311))
#loc820 = loc("torch.nn.modules.linear.Linear::dense"(#loc312))
#loc821 = loc("add_835"(#loc312))
#loc822 = loc("layernorm_836.dc.reduce_sum.0"(#loc313))
#loc823 = loc("layernorm_836.dc.multiply.2"(#loc313))
#loc824 = loc("layernorm_836.dc.subtract.3"(#loc313))
#loc825 = loc("layernorm_836.dc.multiply.4"(#loc313))
#loc826 = loc("layernorm_836.dc.reduce_sum.5"(#loc313))
#loc827 = loc("layernorm_836.dc.multiply.7"(#loc313))
#loc828 = loc("layernorm_836.dc.add.9"(#loc313))
#loc829 = loc("layernorm_836.dc.sqrt.10"(#loc313))
#loc830 = loc("layernorm_836.dc.reciprocal.11"(#loc313))
#loc831 = loc("layernorm_836.dc.multiply.12"(#loc313))
#loc832 = loc("layernorm_836.dc.multiply.13"(#loc313))
#loc833 = loc("layernorm_836.dc.add.14"(#loc313))
#loc834 = loc("transformers.models.vit.modeling_vit.ViTSdpaSelfAttention::attention"(#loc314))
#loc835 = loc("transformers.models.vit.modeling_vit.ViTSelfOutput::output"(#loc314))
#loc836 = loc("layernorm_877.dc.reduce_sum.0"(#loc316))
#loc837 = loc("layernorm_877.dc.multiply.2"(#loc316))
#loc838 = loc("layernorm_877.dc.subtract.3"(#loc316))
#loc839 = loc("layernorm_877.dc.multiply.4"(#loc316))
#loc840 = loc("layernorm_877.dc.reduce_sum.5"(#loc316))
#loc841 = loc("layernorm_877.dc.multiply.7"(#loc316))
#loc842 = loc("layernorm_877.dc.add.9"(#loc316))
#loc843 = loc("layernorm_877.dc.sqrt.10"(#loc316))
#loc844 = loc("layernorm_877.dc.reciprocal.11"(#loc316))
#loc845 = loc("layernorm_877.dc.multiply.12"(#loc316))
#loc846 = loc("layernorm_877.dc.multiply.13"(#loc316))
#loc847 = loc("layernorm_877.dc.add.14"(#loc316))
#loc848 = loc("torch.nn.modules.linear.Linear::dense"(#loc317))
#loc849 = loc("transformers.activations.GELUActivation::intermediate_act_fn"(#loc317))
#loc850 = loc("torch.nn.modules.linear.Linear::dense"(#loc318))
#loc851 = loc("add_890"(#loc318))
#loc852 = loc("layernorm_891.dc.reduce_sum.0"(#loc319))
#loc853 = loc("layernorm_891.dc.multiply.2"(#loc319))
#loc854 = loc("layernorm_891.dc.subtract.3"(#loc319))
#loc855 = loc("layernorm_891.dc.multiply.4"(#loc319))
#loc856 = loc("layernorm_891.dc.reduce_sum.5"(#loc319))
#loc857 = loc("layernorm_891.dc.multiply.7"(#loc319))
#loc858 = loc("layernorm_891.dc.add.9"(#loc319))
#loc859 = loc("layernorm_891.dc.sqrt.10"(#loc319))
#loc860 = loc("layernorm_891.dc.reciprocal.11"(#loc319))
#loc861 = loc("layernorm_891.dc.multiply.12"(#loc319))
#loc862 = loc("layernorm_891.dc.multiply.13"(#loc319))
#loc863 = loc("layernorm_891.dc.add.14"(#loc319))
#loc864 = loc("transformers.models.vit.modeling_vit.ViTSdpaSelfAttention::attention"(#loc320))
#loc865 = loc("transformers.models.vit.modeling_vit.ViTSelfOutput::output"(#loc320))
#loc866 = loc("layernorm_932.dc.reduce_sum.0"(#loc322))
#loc867 = loc("layernorm_932.dc.multiply.2"(#loc322))
#loc868 = loc("layernorm_932.dc.subtract.3"(#loc322))
#loc869 = loc("layernorm_932.dc.multiply.4"(#loc322))
#loc870 = loc("layernorm_932.dc.reduce_sum.5"(#loc322))
#loc871 = loc("layernorm_932.dc.multiply.7"(#loc322))
#loc872 = loc("layernorm_932.dc.add.9"(#loc322))
#loc873 = loc("layernorm_932.dc.sqrt.10"(#loc322))
#loc874 = loc("layernorm_932.dc.reciprocal.11"(#loc322))
#loc875 = loc("layernorm_932.dc.multiply.12"(#loc322))
#loc876 = loc("layernorm_932.dc.multiply.13"(#loc322))
#loc877 = loc("layernorm_932.dc.add.14"(#loc322))
#loc878 = loc("torch.nn.modules.linear.Linear::dense"(#loc323))
#loc879 = loc("transformers.activations.GELUActivation::intermediate_act_fn"(#loc323))
#loc880 = loc("torch.nn.modules.linear.Linear::dense"(#loc324))
#loc881 = loc("add_945"(#loc324))
#loc882 = loc("layernorm_946.dc.reduce_sum.0"(#loc325))
#loc883 = loc("layernorm_946.dc.multiply.2"(#loc325))
#loc884 = loc("layernorm_946.dc.subtract.3"(#loc325))
#loc885 = loc("layernorm_946.dc.multiply.4"(#loc325))
#loc886 = loc("layernorm_946.dc.reduce_sum.5"(#loc325))
#loc887 = loc("layernorm_946.dc.multiply.7"(#loc325))
#loc888 = loc("layernorm_946.dc.add.9"(#loc325))
#loc889 = loc("layernorm_946.dc.sqrt.10"(#loc325))
#loc890 = loc("layernorm_946.dc.reciprocal.11"(#loc325))
#loc891 = loc("layernorm_946.dc.multiply.12"(#loc325))
#loc892 = loc("layernorm_946.dc.multiply.13"(#loc325))
#loc893 = loc("layernorm_946.dc.add.14"(#loc325))
#loc894 = loc("transformers.models.vit.modeling_vit.ViTSdpaSelfAttention::attention"(#loc326))
#loc895 = loc("transformers.models.vit.modeling_vit.ViTSelfOutput::output"(#loc326))
#loc896 = loc("layernorm_987.dc.reduce_sum.0"(#loc328))
#loc897 = loc("layernorm_987.dc.multiply.2"(#loc328))
#loc898 = loc("layernorm_987.dc.subtract.3"(#loc328))
#loc899 = loc("layernorm_987.dc.multiply.4"(#loc328))
#loc900 = loc("layernorm_987.dc.reduce_sum.5"(#loc328))
#loc901 = loc("layernorm_987.dc.multiply.7"(#loc328))
#loc902 = loc("layernorm_987.dc.add.9"(#loc328))
#loc903 = loc("layernorm_987.dc.sqrt.10"(#loc328))
#loc904 = loc("layernorm_987.dc.reciprocal.11"(#loc328))
#loc905 = loc("layernorm_987.dc.multiply.12"(#loc328))
#loc906 = loc("layernorm_987.dc.multiply.13"(#loc328))
#loc907 = loc("layernorm_987.dc.add.14"(#loc328))
#loc908 = loc("torch.nn.modules.linear.Linear::dense"(#loc329))
#loc909 = loc("transformers.activations.GELUActivation::intermediate_act_fn"(#loc329))
#loc910 = loc("torch.nn.modules.linear.Linear::dense"(#loc330))
#loc911 = loc("add_1000"(#loc330))
#loc912 = loc("layernorm_1001.dc.reduce_sum.0"(#loc331))
#loc913 = loc("layernorm_1001.dc.multiply.2"(#loc331))
#loc914 = loc("layernorm_1001.dc.subtract.3"(#loc331))
#loc915 = loc("layernorm_1001.dc.multiply.4"(#loc331))
#loc916 = loc("layernorm_1001.dc.reduce_sum.5"(#loc331))
#loc917 = loc("layernorm_1001.dc.multiply.7"(#loc331))
#loc918 = loc("layernorm_1001.dc.add.9"(#loc331))
#loc919 = loc("layernorm_1001.dc.sqrt.10"(#loc331))
#loc920 = loc("layernorm_1001.dc.reciprocal.11"(#loc331))
#loc921 = loc("layernorm_1001.dc.multiply.12"(#loc331))
#loc922 = loc("layernorm_1001.dc.multiply.13"(#loc331))
#loc923 = loc("layernorm_1001.dc.add.14"(#loc331))
#loc924 = loc("transformers.models.vit.modeling_vit.ViTSdpaSelfAttention::attention"(#loc332))
#loc925 = loc("transformers.models.vit.modeling_vit.ViTSelfOutput::output"(#loc332))
#loc926 = loc("layernorm_1042.dc.reduce_sum.0"(#loc334))
#loc927 = loc("layernorm_1042.dc.multiply.2"(#loc334))
#loc928 = loc("layernorm_1042.dc.subtract.3"(#loc334))
#loc929 = loc("layernorm_1042.dc.multiply.4"(#loc334))
#loc930 = loc("layernorm_1042.dc.reduce_sum.5"(#loc334))
#loc931 = loc("layernorm_1042.dc.multiply.7"(#loc334))
#loc932 = loc("layernorm_1042.dc.add.9"(#loc334))
#loc933 = loc("layernorm_1042.dc.sqrt.10"(#loc334))
#loc934 = loc("layernorm_1042.dc.reciprocal.11"(#loc334))
#loc935 = loc("layernorm_1042.dc.multiply.12"(#loc334))
#loc936 = loc("layernorm_1042.dc.multiply.13"(#loc334))
#loc937 = loc("layernorm_1042.dc.add.14"(#loc334))
#loc938 = loc("torch.nn.modules.linear.Linear::dense"(#loc335))
#loc939 = loc("transformers.activations.GELUActivation::intermediate_act_fn"(#loc335))
#loc940 = loc("torch.nn.modules.linear.Linear::dense"(#loc336))
#loc941 = loc("add_1055"(#loc336))
#loc942 = loc("layernorm_1056.dc.reduce_sum.0"(#loc337))
#loc943 = loc("layernorm_1056.dc.multiply.2"(#loc337))
#loc944 = loc("layernorm_1056.dc.subtract.3"(#loc337))
#loc945 = loc("layernorm_1056.dc.multiply.4"(#loc337))
#loc946 = loc("layernorm_1056.dc.reduce_sum.5"(#loc337))
#loc947 = loc("layernorm_1056.dc.multiply.7"(#loc337))
#loc948 = loc("layernorm_1056.dc.add.9"(#loc337))
#loc949 = loc("layernorm_1056.dc.sqrt.10"(#loc337))
#loc950 = loc("layernorm_1056.dc.reciprocal.11"(#loc337))
#loc951 = loc("layernorm_1056.dc.multiply.12"(#loc337))
#loc952 = loc("layernorm_1056.dc.multiply.13"(#loc337))
#loc953 = loc("layernorm_1056.dc.add.14"(#loc337))
#loc954 = loc("transformers.models.vit.modeling_vit.ViTSdpaSelfAttention::attention"(#loc338))
#loc955 = loc("transformers.models.vit.modeling_vit.ViTSelfOutput::output"(#loc338))
#loc956 = loc("layernorm_1097.dc.reduce_sum.0"(#loc340))
#loc957 = loc("layernorm_1097.dc.multiply.2"(#loc340))
#loc958 = loc("layernorm_1097.dc.subtract.3"(#loc340))
#loc959 = loc("layernorm_1097.dc.multiply.4"(#loc340))
#loc960 = loc("layernorm_1097.dc.reduce_sum.5"(#loc340))
#loc961 = loc("layernorm_1097.dc.multiply.7"(#loc340))
#loc962 = loc("layernorm_1097.dc.add.9"(#loc340))
#loc963 = loc("layernorm_1097.dc.sqrt.10"(#loc340))
#loc964 = loc("layernorm_1097.dc.reciprocal.11"(#loc340))
#loc965 = loc("layernorm_1097.dc.multiply.12"(#loc340))
#loc966 = loc("layernorm_1097.dc.multiply.13"(#loc340))
#loc967 = loc("layernorm_1097.dc.add.14"(#loc340))
#loc968 = loc("torch.nn.modules.linear.Linear::dense"(#loc341))
#loc969 = loc("transformers.activations.GELUActivation::intermediate_act_fn"(#loc341))
#loc970 = loc("torch.nn.modules.linear.Linear::dense"(#loc342))
#loc971 = loc("add_1110"(#loc342))
#loc972 = loc("layernorm_1111.dc.reduce_sum.0"(#loc343))
#loc973 = loc("layernorm_1111.dc.multiply.2"(#loc343))
#loc974 = loc("layernorm_1111.dc.subtract.3"(#loc343))
#loc975 = loc("layernorm_1111.dc.multiply.4"(#loc343))
#loc976 = loc("layernorm_1111.dc.reduce_sum.5"(#loc343))
#loc977 = loc("layernorm_1111.dc.multiply.7"(#loc343))
#loc978 = loc("layernorm_1111.dc.add.9"(#loc343))
#loc979 = loc("layernorm_1111.dc.sqrt.10"(#loc343))
#loc980 = loc("layernorm_1111.dc.reciprocal.11"(#loc343))
#loc981 = loc("layernorm_1111.dc.multiply.12"(#loc343))
#loc982 = loc("layernorm_1111.dc.multiply.13"(#loc343))
#loc983 = loc("layernorm_1111.dc.add.14"(#loc343))
#loc984 = loc("transformers.models.vit.modeling_vit.ViTSdpaSelfAttention::attention"(#loc344))
#loc985 = loc("transformers.models.vit.modeling_vit.ViTSelfOutput::output"(#loc344))
#loc986 = loc("layernorm_1152.dc.reduce_sum.0"(#loc346))
#loc987 = loc("layernorm_1152.dc.multiply.2"(#loc346))
#loc988 = loc("layernorm_1152.dc.subtract.3"(#loc346))
#loc989 = loc("layernorm_1152.dc.multiply.4"(#loc346))
#loc990 = loc("layernorm_1152.dc.reduce_sum.5"(#loc346))
#loc991 = loc("layernorm_1152.dc.multiply.7"(#loc346))
#loc992 = loc("layernorm_1152.dc.add.9"(#loc346))
#loc993 = loc("layernorm_1152.dc.sqrt.10"(#loc346))
#loc994 = loc("layernorm_1152.dc.reciprocal.11"(#loc346))
#loc995 = loc("layernorm_1152.dc.multiply.12"(#loc346))
#loc996 = loc("layernorm_1152.dc.multiply.13"(#loc346))
#loc997 = loc("layernorm_1152.dc.add.14"(#loc346))
#loc998 = loc("torch.nn.modules.linear.Linear::dense"(#loc347))
#loc999 = loc("transformers.activations.GELUActivation::intermediate_act_fn"(#loc347))
#loc1000 = loc("torch.nn.modules.linear.Linear::dense"(#loc348))
#loc1001 = loc("add_1165"(#loc348))
#loc1002 = loc("layernorm_1166.dc.reduce_sum.0"(#loc349))
#loc1003 = loc("layernorm_1166.dc.multiply.2"(#loc349))
#loc1004 = loc("layernorm_1166.dc.subtract.3"(#loc349))
#loc1005 = loc("layernorm_1166.dc.multiply.4"(#loc349))
#loc1006 = loc("layernorm_1166.dc.reduce_sum.5"(#loc349))
#loc1007 = loc("layernorm_1166.dc.multiply.7"(#loc349))
#loc1008 = loc("layernorm_1166.dc.add.9"(#loc349))
#loc1009 = loc("layernorm_1166.dc.sqrt.10"(#loc349))
#loc1010 = loc("layernorm_1166.dc.reciprocal.11"(#loc349))
#loc1011 = loc("layernorm_1166.dc.multiply.12"(#loc349))
#loc1012 = loc("layernorm_1166.dc.multiply.13"(#loc349))
#loc1013 = loc("layernorm_1166.dc.add.14"(#loc349))
#loc1014 = loc("transformers.models.vit.modeling_vit.ViTSdpaSelfAttention::attention"(#loc350))
#loc1015 = loc("transformers.models.vit.modeling_vit.ViTSelfOutput::output"(#loc350))
#loc1016 = loc("layernorm_1207.dc.reduce_sum.0"(#loc352))
#loc1017 = loc("layernorm_1207.dc.multiply.2"(#loc352))
#loc1018 = loc("layernorm_1207.dc.subtract.3"(#loc352))
#loc1019 = loc("layernorm_1207.dc.multiply.4"(#loc352))
#loc1020 = loc("layernorm_1207.dc.reduce_sum.5"(#loc352))
#loc1021 = loc("layernorm_1207.dc.multiply.7"(#loc352))
#loc1022 = loc("layernorm_1207.dc.add.9"(#loc352))
#loc1023 = loc("layernorm_1207.dc.sqrt.10"(#loc352))
#loc1024 = loc("layernorm_1207.dc.reciprocal.11"(#loc352))
#loc1025 = loc("layernorm_1207.dc.multiply.12"(#loc352))
#loc1026 = loc("layernorm_1207.dc.multiply.13"(#loc352))
#loc1027 = loc("layernorm_1207.dc.add.14"(#loc352))
#loc1028 = loc("torch.nn.modules.linear.Linear::dense"(#loc353))
#loc1029 = loc("transformers.activations.GELUActivation::intermediate_act_fn"(#loc353))
#loc1030 = loc("torch.nn.modules.linear.Linear::dense"(#loc354))
#loc1031 = loc("add_1220"(#loc354))
#loc1032 = loc("layernorm_1221.dc.reduce_sum.0"(#loc355))
#loc1033 = loc("layernorm_1221.dc.multiply.2"(#loc355))
#loc1034 = loc("layernorm_1221.dc.subtract.3"(#loc355))
#loc1035 = loc("layernorm_1221.dc.multiply.4"(#loc355))
#loc1036 = loc("layernorm_1221.dc.reduce_sum.5"(#loc355))
#loc1037 = loc("layernorm_1221.dc.multiply.7"(#loc355))
#loc1038 = loc("layernorm_1221.dc.add.9"(#loc355))
#loc1039 = loc("layernorm_1221.dc.sqrt.10"(#loc355))
#loc1040 = loc("layernorm_1221.dc.reciprocal.11"(#loc355))
#loc1041 = loc("layernorm_1221.dc.multiply.12"(#loc355))
#loc1042 = loc("layernorm_1221.dc.multiply.13"(#loc355))
#loc1043 = loc("layernorm_1221.dc.add.14"(#loc355))
#loc1044 = loc("transformers.models.vit.modeling_vit.ViTSdpaSelfAttention::attention"(#loc356))
#loc1045 = loc("transformers.models.vit.modeling_vit.ViTSelfOutput::output"(#loc356))
#loc1046 = loc("layernorm_1262.dc.reduce_sum.0"(#loc358))
#loc1047 = loc("layernorm_1262.dc.multiply.2"(#loc358))
#loc1048 = loc("layernorm_1262.dc.subtract.3"(#loc358))
#loc1049 = loc("layernorm_1262.dc.multiply.4"(#loc358))
#loc1050 = loc("layernorm_1262.dc.reduce_sum.5"(#loc358))
#loc1051 = loc("layernorm_1262.dc.multiply.7"(#loc358))
#loc1052 = loc("layernorm_1262.dc.add.9"(#loc358))
#loc1053 = loc("layernorm_1262.dc.sqrt.10"(#loc358))
#loc1054 = loc("layernorm_1262.dc.reciprocal.11"(#loc358))
#loc1055 = loc("layernorm_1262.dc.multiply.12"(#loc358))
#loc1056 = loc("layernorm_1262.dc.multiply.13"(#loc358))
#loc1057 = loc("layernorm_1262.dc.add.14"(#loc358))
#loc1058 = loc("torch.nn.modules.linear.Linear::dense"(#loc359))
#loc1059 = loc("transformers.activations.GELUActivation::intermediate_act_fn"(#loc359))
#loc1060 = loc("torch.nn.modules.linear.Linear::dense"(#loc360))
#loc1061 = loc("add_1275"(#loc360))
#loc1062 = loc("layernorm_1276.dc.reduce_sum.0"(#loc361))
#loc1063 = loc("layernorm_1276.dc.multiply.2"(#loc361))
#loc1064 = loc("layernorm_1276.dc.subtract.3"(#loc361))
#loc1065 = loc("layernorm_1276.dc.multiply.4"(#loc361))
#loc1066 = loc("layernorm_1276.dc.reduce_sum.5"(#loc361))
#loc1067 = loc("layernorm_1276.dc.multiply.7"(#loc361))
#loc1068 = loc("layernorm_1276.dc.add.9"(#loc361))
#loc1069 = loc("layernorm_1276.dc.sqrt.10"(#loc361))
#loc1070 = loc("layernorm_1276.dc.reciprocal.11"(#loc361))
#loc1071 = loc("layernorm_1276.dc.multiply.12"(#loc361))
#loc1072 = loc("layernorm_1276.dc.multiply.13"(#loc361))
#loc1073 = loc("layernorm_1276.dc.add.14"(#loc361))
#loc1074 = loc("transformers.models.vit.modeling_vit.ViTSdpaSelfAttention::attention"(#loc362))
#loc1075 = loc("transformers.models.vit.modeling_vit.ViTSelfOutput::output"(#loc362))
#loc1076 = loc("layernorm_1317.dc.reduce_sum.0"(#loc364))
#loc1077 = loc("layernorm_1317.dc.multiply.2"(#loc364))
#loc1078 = loc("layernorm_1317.dc.subtract.3"(#loc364))
#loc1079 = loc("layernorm_1317.dc.multiply.4"(#loc364))
#loc1080 = loc("layernorm_1317.dc.reduce_sum.5"(#loc364))
#loc1081 = loc("layernorm_1317.dc.multiply.7"(#loc364))
#loc1082 = loc("layernorm_1317.dc.add.9"(#loc364))
#loc1083 = loc("layernorm_1317.dc.sqrt.10"(#loc364))
#loc1084 = loc("layernorm_1317.dc.reciprocal.11"(#loc364))
#loc1085 = loc("layernorm_1317.dc.multiply.12"(#loc364))
#loc1086 = loc("layernorm_1317.dc.multiply.13"(#loc364))
#loc1087 = loc("layernorm_1317.dc.add.14"(#loc364))
#loc1088 = loc("torch.nn.modules.linear.Linear::dense"(#loc365))
#loc1089 = loc("transformers.activations.GELUActivation::intermediate_act_fn"(#loc365))
#loc1090 = loc("torch.nn.modules.linear.Linear::dense"(#loc366))
#loc1091 = loc("add_1330"(#loc366))
#loc1092 = loc("torch.nn.modules.linear.Linear::query"(#loc384))
#loc1093 = loc("reshape_17"(#loc384))
#loc1094 = loc("transpose_18"(#loc384))
#loc1095 = loc("reshape_19"(#loc384))
#loc1096 = loc("torch.nn.modules.linear.Linear::key"(#loc384))
#loc1097 = loc("reshape_24"(#loc384))
#loc1098 = loc("transpose_25"(#loc384))
#loc1099 = loc("reshape_26"(#loc384))
#loc1100 = loc("matmul_28"(#loc384))
#loc1101 = loc("reshape_29"(#loc384))
#loc1102 = loc("multiply_30"(#loc384))
#loc1103 = loc("add_31"(#loc384))
#loc1104 = loc("softmax_32"(#loc384))
#loc1105 = loc("reshape_34"(#loc384))
#loc1106 = loc("torch.nn.modules.linear.Linear::value"(#loc384))
#loc1107 = loc("reshape_39"(#loc384))
#loc1108 = loc("transpose_40"(#loc384))
#loc1109 = loc("reshape_41"(#loc384))
#loc1110 = loc("matmul_42"(#loc384))
#loc1111 = loc("reshape_43"(#loc384))
#loc1112 = loc("transpose_44"(#loc384))
#loc1113 = loc("torch.nn.modules.linear.Linear::dense"(#loc385))
#loc1114 = loc("reshape_53"(#loc398))
#loc1115 = loc("matmul_55"(#loc398))
#loc1116 = loc("reshape_56"(#loc398))
#loc1117 = loc("gelu_58"(#loc399))
#loc1118 = loc("reshape_59"(#loc400))
#loc1119 = loc("matmul_61"(#loc400))
#loc1120 = loc("reshape_62"(#loc400))
#loc1121 = loc("torch.nn.modules.linear.Linear::query"(#loc414))
#loc1122 = loc("reshape_72"(#loc414))
#loc1123 = loc("transpose_73"(#loc414))
#loc1124 = loc("reshape_74"(#loc414))
#loc1125 = loc("torch.nn.modules.linear.Linear::key"(#loc414))
#loc1126 = loc("reshape_79"(#loc414))
#loc1127 = loc("transpose_80"(#loc414))
#loc1128 = loc("reshape_81"(#loc414))
#loc1129 = loc("matmul_83"(#loc414))
#loc1130 = loc("reshape_84"(#loc414))
#loc1131 = loc("multiply_85"(#loc414))
#loc1132 = loc("add_86"(#loc414))
#loc1133 = loc("softmax_87"(#loc414))
#loc1134 = loc("reshape_89"(#loc414))
#loc1135 = loc("torch.nn.modules.linear.Linear::value"(#loc414))
#loc1136 = loc("reshape_94"(#loc414))
#loc1137 = loc("transpose_95"(#loc414))
#loc1138 = loc("reshape_96"(#loc414))
#loc1139 = loc("matmul_97"(#loc414))
#loc1140 = loc("reshape_98"(#loc414))
#loc1141 = loc("transpose_99"(#loc414))
#loc1142 = loc("torch.nn.modules.linear.Linear::dense"(#loc415))
#loc1143 = loc("reshape_108"(#loc428))
#loc1144 = loc("matmul_110"(#loc428))
#loc1145 = loc("reshape_111"(#loc428))
#loc1146 = loc("gelu_113"(#loc429))
#loc1147 = loc("reshape_114"(#loc430))
#loc1148 = loc("matmul_116"(#loc430))
#loc1149 = loc("reshape_117"(#loc430))
#loc1150 = loc("torch.nn.modules.linear.Linear::query"(#loc444))
#loc1151 = loc("reshape_127"(#loc444))
#loc1152 = loc("transpose_128"(#loc444))
#loc1153 = loc("reshape_129"(#loc444))
#loc1154 = loc("torch.nn.modules.linear.Linear::key"(#loc444))
#loc1155 = loc("reshape_134"(#loc444))
#loc1156 = loc("transpose_135"(#loc444))
#loc1157 = loc("reshape_136"(#loc444))
#loc1158 = loc("matmul_138"(#loc444))
#loc1159 = loc("reshape_139"(#loc444))
#loc1160 = loc("multiply_140"(#loc444))
#loc1161 = loc("add_141"(#loc444))
#loc1162 = loc("softmax_142"(#loc444))
#loc1163 = loc("reshape_144"(#loc444))
#loc1164 = loc("torch.nn.modules.linear.Linear::value"(#loc444))
#loc1165 = loc("reshape_149"(#loc444))
#loc1166 = loc("transpose_150"(#loc444))
#loc1167 = loc("reshape_151"(#loc444))
#loc1168 = loc("matmul_152"(#loc444))
#loc1169 = loc("reshape_153"(#loc444))
#loc1170 = loc("transpose_154"(#loc444))
#loc1171 = loc("torch.nn.modules.linear.Linear::dense"(#loc445))
#loc1172 = loc("reshape_163"(#loc458))
#loc1173 = loc("matmul_165"(#loc458))
#loc1174 = loc("reshape_166"(#loc458))
#loc1175 = loc("gelu_168"(#loc459))
#loc1176 = loc("reshape_169"(#loc460))
#loc1177 = loc("matmul_171"(#loc460))
#loc1178 = loc("reshape_172"(#loc460))
#loc1179 = loc("torch.nn.modules.linear.Linear::query"(#loc474))
#loc1180 = loc("reshape_182"(#loc474))
#loc1181 = loc("transpose_183"(#loc474))
#loc1182 = loc("reshape_184"(#loc474))
#loc1183 = loc("torch.nn.modules.linear.Linear::key"(#loc474))
#loc1184 = loc("reshape_189"(#loc474))
#loc1185 = loc("transpose_190"(#loc474))
#loc1186 = loc("reshape_191"(#loc474))
#loc1187 = loc("matmul_193"(#loc474))
#loc1188 = loc("reshape_194"(#loc474))
#loc1189 = loc("multiply_195"(#loc474))
#loc1190 = loc("add_196"(#loc474))
#loc1191 = loc("softmax_197"(#loc474))
#loc1192 = loc("reshape_199"(#loc474))
#loc1193 = loc("torch.nn.modules.linear.Linear::value"(#loc474))
#loc1194 = loc("reshape_204"(#loc474))
#loc1195 = loc("transpose_205"(#loc474))
#loc1196 = loc("reshape_206"(#loc474))
#loc1197 = loc("matmul_207"(#loc474))
#loc1198 = loc("reshape_208"(#loc474))
#loc1199 = loc("transpose_209"(#loc474))
#loc1200 = loc("torch.nn.modules.linear.Linear::dense"(#loc475))
#loc1201 = loc("reshape_218"(#loc488))
#loc1202 = loc("matmul_220"(#loc488))
#loc1203 = loc("reshape_221"(#loc488))
#loc1204 = loc("gelu_223"(#loc489))
#loc1205 = loc("reshape_224"(#loc490))
#loc1206 = loc("matmul_226"(#loc490))
#loc1207 = loc("reshape_227"(#loc490))
#loc1208 = loc("torch.nn.modules.linear.Linear::query"(#loc504))
#loc1209 = loc("reshape_237"(#loc504))
#loc1210 = loc("transpose_238"(#loc504))
#loc1211 = loc("reshape_239"(#loc504))
#loc1212 = loc("torch.nn.modules.linear.Linear::key"(#loc504))
#loc1213 = loc("reshape_244"(#loc504))
#loc1214 = loc("transpose_245"(#loc504))
#loc1215 = loc("reshape_246"(#loc504))
#loc1216 = loc("matmul_248"(#loc504))
#loc1217 = loc("reshape_249"(#loc504))
#loc1218 = loc("multiply_250"(#loc504))
#loc1219 = loc("add_251"(#loc504))
#loc1220 = loc("softmax_252"(#loc504))
#loc1221 = loc("reshape_254"(#loc504))
#loc1222 = loc("torch.nn.modules.linear.Linear::value"(#loc504))
#loc1223 = loc("reshape_259"(#loc504))
#loc1224 = loc("transpose_260"(#loc504))
#loc1225 = loc("reshape_261"(#loc504))
#loc1226 = loc("matmul_262"(#loc504))
#loc1227 = loc("reshape_263"(#loc504))
#loc1228 = loc("transpose_264"(#loc504))
#loc1229 = loc("torch.nn.modules.linear.Linear::dense"(#loc505))
#loc1230 = loc("reshape_273"(#loc518))
#loc1231 = loc("matmul_275"(#loc518))
#loc1232 = loc("reshape_276"(#loc518))
#loc1233 = loc("gelu_278"(#loc519))
#loc1234 = loc("reshape_279"(#loc520))
#loc1235 = loc("matmul_281"(#loc520))
#loc1236 = loc("reshape_282"(#loc520))
#loc1237 = loc("torch.nn.modules.linear.Linear::query"(#loc534))
#loc1238 = loc("reshape_292"(#loc534))
#loc1239 = loc("transpose_293"(#loc534))
#loc1240 = loc("reshape_294"(#loc534))
#loc1241 = loc("torch.nn.modules.linear.Linear::key"(#loc534))
#loc1242 = loc("reshape_299"(#loc534))
#loc1243 = loc("transpose_300"(#loc534))
#loc1244 = loc("reshape_301"(#loc534))
#loc1245 = loc("matmul_303"(#loc534))
#loc1246 = loc("reshape_304"(#loc534))
#loc1247 = loc("multiply_305"(#loc534))
#loc1248 = loc("add_306"(#loc534))
#loc1249 = loc("softmax_307"(#loc534))
#loc1250 = loc("reshape_309"(#loc534))
#loc1251 = loc("torch.nn.modules.linear.Linear::value"(#loc534))
#loc1252 = loc("reshape_314"(#loc534))
#loc1253 = loc("transpose_315"(#loc534))
#loc1254 = loc("reshape_316"(#loc534))
#loc1255 = loc("matmul_317"(#loc534))
#loc1256 = loc("reshape_318"(#loc534))
#loc1257 = loc("transpose_319"(#loc534))
#loc1258 = loc("torch.nn.modules.linear.Linear::dense"(#loc535))
#loc1259 = loc("reshape_328"(#loc548))
#loc1260 = loc("matmul_330"(#loc548))
#loc1261 = loc("reshape_331"(#loc548))
#loc1262 = loc("gelu_333"(#loc549))
#loc1263 = loc("reshape_334"(#loc550))
#loc1264 = loc("matmul_336"(#loc550))
#loc1265 = loc("reshape_337"(#loc550))
#loc1266 = loc("torch.nn.modules.linear.Linear::query"(#loc564))
#loc1267 = loc("reshape_347"(#loc564))
#loc1268 = loc("transpose_348"(#loc564))
#loc1269 = loc("reshape_349"(#loc564))
#loc1270 = loc("torch.nn.modules.linear.Linear::key"(#loc564))
#loc1271 = loc("reshape_354"(#loc564))
#loc1272 = loc("transpose_355"(#loc564))
#loc1273 = loc("reshape_356"(#loc564))
#loc1274 = loc("matmul_358"(#loc564))
#loc1275 = loc("reshape_359"(#loc564))
#loc1276 = loc("multiply_360"(#loc564))
#loc1277 = loc("add_361"(#loc564))
#loc1278 = loc("softmax_362"(#loc564))
#loc1279 = loc("reshape_364"(#loc564))
#loc1280 = loc("torch.nn.modules.linear.Linear::value"(#loc564))
#loc1281 = loc("reshape_369"(#loc564))
#loc1282 = loc("transpose_370"(#loc564))
#loc1283 = loc("reshape_371"(#loc564))
#loc1284 = loc("matmul_372"(#loc564))
#loc1285 = loc("reshape_373"(#loc564))
#loc1286 = loc("transpose_374"(#loc564))
#loc1287 = loc("torch.nn.modules.linear.Linear::dense"(#loc565))
#loc1288 = loc("reshape_383"(#loc578))
#loc1289 = loc("matmul_385"(#loc578))
#loc1290 = loc("reshape_386"(#loc578))
#loc1291 = loc("gelu_388"(#loc579))
#loc1292 = loc("reshape_389"(#loc580))
#loc1293 = loc("matmul_391"(#loc580))
#loc1294 = loc("reshape_392"(#loc580))
#loc1295 = loc("torch.nn.modules.linear.Linear::query"(#loc594))
#loc1296 = loc("reshape_402"(#loc594))
#loc1297 = loc("transpose_403"(#loc594))
#loc1298 = loc("reshape_404"(#loc594))
#loc1299 = loc("torch.nn.modules.linear.Linear::key"(#loc594))
#loc1300 = loc("reshape_409"(#loc594))
#loc1301 = loc("transpose_410"(#loc594))
#loc1302 = loc("reshape_411"(#loc594))
#loc1303 = loc("matmul_413"(#loc594))
#loc1304 = loc("reshape_414"(#loc594))
#loc1305 = loc("multiply_415"(#loc594))
#loc1306 = loc("add_416"(#loc594))
#loc1307 = loc("softmax_417"(#loc594))
#loc1308 = loc("reshape_419"(#loc594))
#loc1309 = loc("torch.nn.modules.linear.Linear::value"(#loc594))
#loc1310 = loc("reshape_424"(#loc594))
#loc1311 = loc("transpose_425"(#loc594))
#loc1312 = loc("reshape_426"(#loc594))
#loc1313 = loc("matmul_427"(#loc594))
#loc1314 = loc("reshape_428"(#loc594))
#loc1315 = loc("transpose_429"(#loc594))
#loc1316 = loc("torch.nn.modules.linear.Linear::dense"(#loc595))
#loc1317 = loc("reshape_438"(#loc608))
#loc1318 = loc("matmul_440"(#loc608))
#loc1319 = loc("reshape_441"(#loc608))
#loc1320 = loc("gelu_443"(#loc609))
#loc1321 = loc("reshape_444"(#loc610))
#loc1322 = loc("matmul_446"(#loc610))
#loc1323 = loc("reshape_447"(#loc610))
#loc1324 = loc("torch.nn.modules.linear.Linear::query"(#loc624))
#loc1325 = loc("reshape_457"(#loc624))
#loc1326 = loc("transpose_458"(#loc624))
#loc1327 = loc("reshape_459"(#loc624))
#loc1328 = loc("torch.nn.modules.linear.Linear::key"(#loc624))
#loc1329 = loc("reshape_464"(#loc624))
#loc1330 = loc("transpose_465"(#loc624))
#loc1331 = loc("reshape_466"(#loc624))
#loc1332 = loc("matmul_468"(#loc624))
#loc1333 = loc("reshape_469"(#loc624))
#loc1334 = loc("multiply_470"(#loc624))
#loc1335 = loc("add_471"(#loc624))
#loc1336 = loc("softmax_472"(#loc624))
#loc1337 = loc("reshape_474"(#loc624))
#loc1338 = loc("torch.nn.modules.linear.Linear::value"(#loc624))
#loc1339 = loc("reshape_479"(#loc624))
#loc1340 = loc("transpose_480"(#loc624))
#loc1341 = loc("reshape_481"(#loc624))
#loc1342 = loc("matmul_482"(#loc624))
#loc1343 = loc("reshape_483"(#loc624))
#loc1344 = loc("transpose_484"(#loc624))
#loc1345 = loc("torch.nn.modules.linear.Linear::dense"(#loc625))
#loc1346 = loc("reshape_493"(#loc638))
#loc1347 = loc("matmul_495"(#loc638))
#loc1348 = loc("reshape_496"(#loc638))
#loc1349 = loc("gelu_498"(#loc639))
#loc1350 = loc("reshape_499"(#loc640))
#loc1351 = loc("matmul_501"(#loc640))
#loc1352 = loc("reshape_502"(#loc640))
#loc1353 = loc("torch.nn.modules.linear.Linear::query"(#loc654))
#loc1354 = loc("reshape_512"(#loc654))
#loc1355 = loc("transpose_513"(#loc654))
#loc1356 = loc("reshape_514"(#loc654))
#loc1357 = loc("torch.nn.modules.linear.Linear::key"(#loc654))
#loc1358 = loc("reshape_519"(#loc654))
#loc1359 = loc("transpose_520"(#loc654))
#loc1360 = loc("reshape_521"(#loc654))
#loc1361 = loc("matmul_523"(#loc654))
#loc1362 = loc("reshape_524"(#loc654))
#loc1363 = loc("multiply_525"(#loc654))
#loc1364 = loc("add_526"(#loc654))
#loc1365 = loc("softmax_527"(#loc654))
#loc1366 = loc("reshape_529"(#loc654))
#loc1367 = loc("torch.nn.modules.linear.Linear::value"(#loc654))
#loc1368 = loc("reshape_534"(#loc654))
#loc1369 = loc("transpose_535"(#loc654))
#loc1370 = loc("reshape_536"(#loc654))
#loc1371 = loc("matmul_537"(#loc654))
#loc1372 = loc("reshape_538"(#loc654))
#loc1373 = loc("transpose_539"(#loc654))
#loc1374 = loc("torch.nn.modules.linear.Linear::dense"(#loc655))
#loc1375 = loc("reshape_548"(#loc668))
#loc1376 = loc("matmul_550"(#loc668))
#loc1377 = loc("reshape_551"(#loc668))
#loc1378 = loc("gelu_553"(#loc669))
#loc1379 = loc("reshape_554"(#loc670))
#loc1380 = loc("matmul_556"(#loc670))
#loc1381 = loc("reshape_557"(#loc670))
#loc1382 = loc("torch.nn.modules.linear.Linear::query"(#loc684))
#loc1383 = loc("reshape_567"(#loc684))
#loc1384 = loc("transpose_568"(#loc684))
#loc1385 = loc("reshape_569"(#loc684))
#loc1386 = loc("torch.nn.modules.linear.Linear::key"(#loc684))
#loc1387 = loc("reshape_574"(#loc684))
#loc1388 = loc("transpose_575"(#loc684))
#loc1389 = loc("reshape_576"(#loc684))
#loc1390 = loc("matmul_578"(#loc684))
#loc1391 = loc("reshape_579"(#loc684))
#loc1392 = loc("multiply_580"(#loc684))
#loc1393 = loc("add_581"(#loc684))
#loc1394 = loc("softmax_582"(#loc684))
#loc1395 = loc("reshape_584"(#loc684))
#loc1396 = loc("torch.nn.modules.linear.Linear::value"(#loc684))
#loc1397 = loc("reshape_589"(#loc684))
#loc1398 = loc("transpose_590"(#loc684))
#loc1399 = loc("reshape_591"(#loc684))
#loc1400 = loc("matmul_592"(#loc684))
#loc1401 = loc("reshape_593"(#loc684))
#loc1402 = loc("transpose_594"(#loc684))
#loc1403 = loc("torch.nn.modules.linear.Linear::dense"(#loc685))
#loc1404 = loc("reshape_603"(#loc698))
#loc1405 = loc("matmul_605"(#loc698))
#loc1406 = loc("reshape_606"(#loc698))
#loc1407 = loc("gelu_608"(#loc699))
#loc1408 = loc("reshape_609"(#loc700))
#loc1409 = loc("matmul_611"(#loc700))
#loc1410 = loc("reshape_612"(#loc700))
#loc1411 = loc("torch.nn.modules.linear.Linear::query"(#loc714))
#loc1412 = loc("reshape_622"(#loc714))
#loc1413 = loc("transpose_623"(#loc714))
#loc1414 = loc("reshape_624"(#loc714))
#loc1415 = loc("torch.nn.modules.linear.Linear::key"(#loc714))
#loc1416 = loc("reshape_629"(#loc714))
#loc1417 = loc("transpose_630"(#loc714))
#loc1418 = loc("reshape_631"(#loc714))
#loc1419 = loc("matmul_633"(#loc714))
#loc1420 = loc("reshape_634"(#loc714))
#loc1421 = loc("multiply_635"(#loc714))
#loc1422 = loc("add_636"(#loc714))
#loc1423 = loc("softmax_637"(#loc714))
#loc1424 = loc("reshape_639"(#loc714))
#loc1425 = loc("torch.nn.modules.linear.Linear::value"(#loc714))
#loc1426 = loc("reshape_644"(#loc714))
#loc1427 = loc("transpose_645"(#loc714))
#loc1428 = loc("reshape_646"(#loc714))
#loc1429 = loc("matmul_647"(#loc714))
#loc1430 = loc("reshape_648"(#loc714))
#loc1431 = loc("transpose_649"(#loc714))
#loc1432 = loc("torch.nn.modules.linear.Linear::dense"(#loc715))
#loc1433 = loc("reshape_658"(#loc728))
#loc1434 = loc("matmul_660"(#loc728))
#loc1435 = loc("reshape_661"(#loc728))
#loc1436 = loc("gelu_663"(#loc729))
#loc1437 = loc("reshape_664"(#loc730))
#loc1438 = loc("matmul_666"(#loc730))
#loc1439 = loc("reshape_667"(#loc730))
#loc1440 = loc("torch.nn.modules.linear.Linear::query"(#loc744))
#loc1441 = loc("reshape_677"(#loc744))
#loc1442 = loc("transpose_678"(#loc744))
#loc1443 = loc("reshape_679"(#loc744))
#loc1444 = loc("torch.nn.modules.linear.Linear::key"(#loc744))
#loc1445 = loc("reshape_684"(#loc744))
#loc1446 = loc("transpose_685"(#loc744))
#loc1447 = loc("reshape_686"(#loc744))
#loc1448 = loc("matmul_688"(#loc744))
#loc1449 = loc("reshape_689"(#loc744))
#loc1450 = loc("multiply_690"(#loc744))
#loc1451 = loc("add_691"(#loc744))
#loc1452 = loc("softmax_692"(#loc744))
#loc1453 = loc("reshape_694"(#loc744))
#loc1454 = loc("torch.nn.modules.linear.Linear::value"(#loc744))
#loc1455 = loc("reshape_699"(#loc744))
#loc1456 = loc("transpose_700"(#loc744))
#loc1457 = loc("reshape_701"(#loc744))
#loc1458 = loc("matmul_702"(#loc744))
#loc1459 = loc("reshape_703"(#loc744))
#loc1460 = loc("transpose_704"(#loc744))
#loc1461 = loc("torch.nn.modules.linear.Linear::dense"(#loc745))
#loc1462 = loc("reshape_713"(#loc758))
#loc1463 = loc("matmul_715"(#loc758))
#loc1464 = loc("reshape_716"(#loc758))
#loc1465 = loc("gelu_718"(#loc759))
#loc1466 = loc("reshape_719"(#loc760))
#loc1467 = loc("matmul_721"(#loc760))
#loc1468 = loc("reshape_722"(#loc760))
#loc1469 = loc("torch.nn.modules.linear.Linear::query"(#loc774))
#loc1470 = loc("reshape_732"(#loc774))
#loc1471 = loc("transpose_733"(#loc774))
#loc1472 = loc("reshape_734"(#loc774))
#loc1473 = loc("torch.nn.modules.linear.Linear::key"(#loc774))
#loc1474 = loc("reshape_739"(#loc774))
#loc1475 = loc("transpose_740"(#loc774))
#loc1476 = loc("reshape_741"(#loc774))
#loc1477 = loc("matmul_743"(#loc774))
#loc1478 = loc("reshape_744"(#loc774))
#loc1479 = loc("multiply_745"(#loc774))
#loc1480 = loc("add_746"(#loc774))
#loc1481 = loc("softmax_747"(#loc774))
#loc1482 = loc("reshape_749"(#loc774))
#loc1483 = loc("torch.nn.modules.linear.Linear::value"(#loc774))
#loc1484 = loc("reshape_754"(#loc774))
#loc1485 = loc("transpose_755"(#loc774))
#loc1486 = loc("reshape_756"(#loc774))
#loc1487 = loc("matmul_757"(#loc774))
#loc1488 = loc("reshape_758"(#loc774))
#loc1489 = loc("transpose_759"(#loc774))
#loc1490 = loc("torch.nn.modules.linear.Linear::dense"(#loc775))
#loc1491 = loc("reshape_768"(#loc788))
#loc1492 = loc("matmul_770"(#loc788))
#loc1493 = loc("reshape_771"(#loc788))
#loc1494 = loc("gelu_773"(#loc789))
#loc1495 = loc("reshape_774"(#loc790))
#loc1496 = loc("matmul_776"(#loc790))
#loc1497 = loc("reshape_777"(#loc790))
#loc1498 = loc("torch.nn.modules.linear.Linear::query"(#loc804))
#loc1499 = loc("reshape_787"(#loc804))
#loc1500 = loc("transpose_788"(#loc804))
#loc1501 = loc("reshape_789"(#loc804))
#loc1502 = loc("torch.nn.modules.linear.Linear::key"(#loc804))
#loc1503 = loc("reshape_794"(#loc804))
#loc1504 = loc("transpose_795"(#loc804))
#loc1505 = loc("reshape_796"(#loc804))
#loc1506 = loc("matmul_798"(#loc804))
#loc1507 = loc("reshape_799"(#loc804))
#loc1508 = loc("multiply_800"(#loc804))
#loc1509 = loc("add_801"(#loc804))
#loc1510 = loc("softmax_802"(#loc804))
#loc1511 = loc("reshape_804"(#loc804))
#loc1512 = loc("torch.nn.modules.linear.Linear::value"(#loc804))
#loc1513 = loc("reshape_809"(#loc804))
#loc1514 = loc("transpose_810"(#loc804))
#loc1515 = loc("reshape_811"(#loc804))
#loc1516 = loc("matmul_812"(#loc804))
#loc1517 = loc("reshape_813"(#loc804))
#loc1518 = loc("transpose_814"(#loc804))
#loc1519 = loc("torch.nn.modules.linear.Linear::dense"(#loc805))
#loc1520 = loc("reshape_823"(#loc818))
#loc1521 = loc("matmul_825"(#loc818))
#loc1522 = loc("reshape_826"(#loc818))
#loc1523 = loc("gelu_828"(#loc819))
#loc1524 = loc("reshape_829"(#loc820))
#loc1525 = loc("matmul_831"(#loc820))
#loc1526 = loc("reshape_832"(#loc820))
#loc1527 = loc("torch.nn.modules.linear.Linear::query"(#loc834))
#loc1528 = loc("reshape_842"(#loc834))
#loc1529 = loc("transpose_843"(#loc834))
#loc1530 = loc("reshape_844"(#loc834))
#loc1531 = loc("torch.nn.modules.linear.Linear::key"(#loc834))
#loc1532 = loc("reshape_849"(#loc834))
#loc1533 = loc("transpose_850"(#loc834))
#loc1534 = loc("reshape_851"(#loc834))
#loc1535 = loc("matmul_853"(#loc834))
#loc1536 = loc("reshape_854"(#loc834))
#loc1537 = loc("multiply_855"(#loc834))
#loc1538 = loc("add_856"(#loc834))
#loc1539 = loc("softmax_857"(#loc834))
#loc1540 = loc("reshape_859"(#loc834))
#loc1541 = loc("torch.nn.modules.linear.Linear::value"(#loc834))
#loc1542 = loc("reshape_864"(#loc834))
#loc1543 = loc("transpose_865"(#loc834))
#loc1544 = loc("reshape_866"(#loc834))
#loc1545 = loc("matmul_867"(#loc834))
#loc1546 = loc("reshape_868"(#loc834))
#loc1547 = loc("transpose_869"(#loc834))
#loc1548 = loc("torch.nn.modules.linear.Linear::dense"(#loc835))
#loc1549 = loc("reshape_878"(#loc848))
#loc1550 = loc("matmul_880"(#loc848))
#loc1551 = loc("reshape_881"(#loc848))
#loc1552 = loc("gelu_883"(#loc849))
#loc1553 = loc("reshape_884"(#loc850))
#loc1554 = loc("matmul_886"(#loc850))
#loc1555 = loc("reshape_887"(#loc850))
#loc1556 = loc("torch.nn.modules.linear.Linear::query"(#loc864))
#loc1557 = loc("reshape_897"(#loc864))
#loc1558 = loc("transpose_898"(#loc864))
#loc1559 = loc("reshape_899"(#loc864))
#loc1560 = loc("torch.nn.modules.linear.Linear::key"(#loc864))
#loc1561 = loc("reshape_904"(#loc864))
#loc1562 = loc("transpose_905"(#loc864))
#loc1563 = loc("reshape_906"(#loc864))
#loc1564 = loc("matmul_908"(#loc864))
#loc1565 = loc("reshape_909"(#loc864))
#loc1566 = loc("multiply_910"(#loc864))
#loc1567 = loc("add_911"(#loc864))
#loc1568 = loc("softmax_912"(#loc864))
#loc1569 = loc("reshape_914"(#loc864))
#loc1570 = loc("torch.nn.modules.linear.Linear::value"(#loc864))
#loc1571 = loc("reshape_919"(#loc864))
#loc1572 = loc("transpose_920"(#loc864))
#loc1573 = loc("reshape_921"(#loc864))
#loc1574 = loc("matmul_922"(#loc864))
#loc1575 = loc("reshape_923"(#loc864))
#loc1576 = loc("transpose_924"(#loc864))
#loc1577 = loc("torch.nn.modules.linear.Linear::dense"(#loc865))
#loc1578 = loc("reshape_933"(#loc878))
#loc1579 = loc("matmul_935"(#loc878))
#loc1580 = loc("reshape_936"(#loc878))
#loc1581 = loc("gelu_938"(#loc879))
#loc1582 = loc("reshape_939"(#loc880))
#loc1583 = loc("matmul_941"(#loc880))
#loc1584 = loc("reshape_942"(#loc880))
#loc1585 = loc("torch.nn.modules.linear.Linear::query"(#loc894))
#loc1586 = loc("reshape_952"(#loc894))
#loc1587 = loc("transpose_953"(#loc894))
#loc1588 = loc("reshape_954"(#loc894))
#loc1589 = loc("torch.nn.modules.linear.Linear::key"(#loc894))
#loc1590 = loc("reshape_959"(#loc894))
#loc1591 = loc("transpose_960"(#loc894))
#loc1592 = loc("reshape_961"(#loc894))
#loc1593 = loc("matmul_963"(#loc894))
#loc1594 = loc("reshape_964"(#loc894))
#loc1595 = loc("multiply_965"(#loc894))
#loc1596 = loc("add_966"(#loc894))
#loc1597 = loc("softmax_967"(#loc894))
#loc1598 = loc("reshape_969"(#loc894))
#loc1599 = loc("torch.nn.modules.linear.Linear::value"(#loc894))
#loc1600 = loc("reshape_974"(#loc894))
#loc1601 = loc("transpose_975"(#loc894))
#loc1602 = loc("reshape_976"(#loc894))
#loc1603 = loc("matmul_977"(#loc894))
#loc1604 = loc("reshape_978"(#loc894))
#loc1605 = loc("transpose_979"(#loc894))
#loc1606 = loc("torch.nn.modules.linear.Linear::dense"(#loc895))
#loc1607 = loc("reshape_988"(#loc908))
#loc1608 = loc("matmul_990"(#loc908))
#loc1609 = loc("reshape_991"(#loc908))
#loc1610 = loc("gelu_993"(#loc909))
#loc1611 = loc("reshape_994"(#loc910))
#loc1612 = loc("matmul_996"(#loc910))
#loc1613 = loc("reshape_997"(#loc910))
#loc1614 = loc("torch.nn.modules.linear.Linear::query"(#loc924))
#loc1615 = loc("reshape_1007"(#loc924))
#loc1616 = loc("transpose_1008"(#loc924))
#loc1617 = loc("reshape_1009"(#loc924))
#loc1618 = loc("torch.nn.modules.linear.Linear::key"(#loc924))
#loc1619 = loc("reshape_1014"(#loc924))
#loc1620 = loc("transpose_1015"(#loc924))
#loc1621 = loc("reshape_1016"(#loc924))
#loc1622 = loc("matmul_1018"(#loc924))
#loc1623 = loc("reshape_1019"(#loc924))
#loc1624 = loc("multiply_1020"(#loc924))
#loc1625 = loc("add_1021"(#loc924))
#loc1626 = loc("softmax_1022"(#loc924))
#loc1627 = loc("reshape_1024"(#loc924))
#loc1628 = loc("torch.nn.modules.linear.Linear::value"(#loc924))
#loc1629 = loc("reshape_1029"(#loc924))
#loc1630 = loc("transpose_1030"(#loc924))
#loc1631 = loc("reshape_1031"(#loc924))
#loc1632 = loc("matmul_1032"(#loc924))
#loc1633 = loc("reshape_1033"(#loc924))
#loc1634 = loc("transpose_1034"(#loc924))
#loc1635 = loc("torch.nn.modules.linear.Linear::dense"(#loc925))
#loc1636 = loc("reshape_1043"(#loc938))
#loc1637 = loc("matmul_1045"(#loc938))
#loc1638 = loc("reshape_1046"(#loc938))
#loc1639 = loc("gelu_1048"(#loc939))
#loc1640 = loc("reshape_1049"(#loc940))
#loc1641 = loc("matmul_1051"(#loc940))
#loc1642 = loc("reshape_1052"(#loc940))
#loc1643 = loc("torch.nn.modules.linear.Linear::query"(#loc954))
#loc1644 = loc("reshape_1062"(#loc954))
#loc1645 = loc("transpose_1063"(#loc954))
#loc1646 = loc("reshape_1064"(#loc954))
#loc1647 = loc("torch.nn.modules.linear.Linear::key"(#loc954))
#loc1648 = loc("reshape_1069"(#loc954))
#loc1649 = loc("transpose_1070"(#loc954))
#loc1650 = loc("reshape_1071"(#loc954))
#loc1651 = loc("matmul_1073"(#loc954))
#loc1652 = loc("reshape_1074"(#loc954))
#loc1653 = loc("multiply_1075"(#loc954))
#loc1654 = loc("add_1076"(#loc954))
#loc1655 = loc("softmax_1077"(#loc954))
#loc1656 = loc("reshape_1079"(#loc954))
#loc1657 = loc("torch.nn.modules.linear.Linear::value"(#loc954))
#loc1658 = loc("reshape_1084"(#loc954))
#loc1659 = loc("transpose_1085"(#loc954))
#loc1660 = loc("reshape_1086"(#loc954))
#loc1661 = loc("matmul_1087"(#loc954))
#loc1662 = loc("reshape_1088"(#loc954))
#loc1663 = loc("transpose_1089"(#loc954))
#loc1664 = loc("torch.nn.modules.linear.Linear::dense"(#loc955))
#loc1665 = loc("reshape_1098"(#loc968))
#loc1666 = loc("matmul_1100"(#loc968))
#loc1667 = loc("reshape_1101"(#loc968))
#loc1668 = loc("gelu_1103"(#loc969))
#loc1669 = loc("reshape_1104"(#loc970))
#loc1670 = loc("matmul_1106"(#loc970))
#loc1671 = loc("reshape_1107"(#loc970))
#loc1672 = loc("torch.nn.modules.linear.Linear::query"(#loc984))
#loc1673 = loc("reshape_1117"(#loc984))
#loc1674 = loc("transpose_1118"(#loc984))
#loc1675 = loc("reshape_1119"(#loc984))
#loc1676 = loc("torch.nn.modules.linear.Linear::key"(#loc984))
#loc1677 = loc("reshape_1124"(#loc984))
#loc1678 = loc("transpose_1125"(#loc984))
#loc1679 = loc("reshape_1126"(#loc984))
#loc1680 = loc("matmul_1128"(#loc984))
#loc1681 = loc("reshape_1129"(#loc984))
#loc1682 = loc("multiply_1130"(#loc984))
#loc1683 = loc("add_1131"(#loc984))
#loc1684 = loc("softmax_1132"(#loc984))
#loc1685 = loc("reshape_1134"(#loc984))
#loc1686 = loc("torch.nn.modules.linear.Linear::value"(#loc984))
#loc1687 = loc("reshape_1139"(#loc984))
#loc1688 = loc("transpose_1140"(#loc984))
#loc1689 = loc("reshape_1141"(#loc984))
#loc1690 = loc("matmul_1142"(#loc984))
#loc1691 = loc("reshape_1143"(#loc984))
#loc1692 = loc("transpose_1144"(#loc984))
#loc1693 = loc("torch.nn.modules.linear.Linear::dense"(#loc985))
#loc1694 = loc("reshape_1153"(#loc998))
#loc1695 = loc("matmul_1155"(#loc998))
#loc1696 = loc("reshape_1156"(#loc998))
#loc1697 = loc("gelu_1158"(#loc999))
#loc1698 = loc("reshape_1159"(#loc1000))
#loc1699 = loc("matmul_1161"(#loc1000))
#loc1700 = loc("reshape_1162"(#loc1000))
#loc1701 = loc("torch.nn.modules.linear.Linear::query"(#loc1014))
#loc1702 = loc("reshape_1172"(#loc1014))
#loc1703 = loc("transpose_1173"(#loc1014))
#loc1704 = loc("reshape_1174"(#loc1014))
#loc1705 = loc("torch.nn.modules.linear.Linear::key"(#loc1014))
#loc1706 = loc("reshape_1179"(#loc1014))
#loc1707 = loc("transpose_1180"(#loc1014))
#loc1708 = loc("reshape_1181"(#loc1014))
#loc1709 = loc("matmul_1183"(#loc1014))
#loc1710 = loc("reshape_1184"(#loc1014))
#loc1711 = loc("multiply_1185"(#loc1014))
#loc1712 = loc("add_1186"(#loc1014))
#loc1713 = loc("softmax_1187"(#loc1014))
#loc1714 = loc("reshape_1189"(#loc1014))
#loc1715 = loc("torch.nn.modules.linear.Linear::value"(#loc1014))
#loc1716 = loc("reshape_1194"(#loc1014))
#loc1717 = loc("transpose_1195"(#loc1014))
#loc1718 = loc("reshape_1196"(#loc1014))
#loc1719 = loc("matmul_1197"(#loc1014))
#loc1720 = loc("reshape_1198"(#loc1014))
#loc1721 = loc("transpose_1199"(#loc1014))
#loc1722 = loc("torch.nn.modules.linear.Linear::dense"(#loc1015))
#loc1723 = loc("reshape_1208"(#loc1028))
#loc1724 = loc("matmul_1210"(#loc1028))
#loc1725 = loc("reshape_1211"(#loc1028))
#loc1726 = loc("gelu_1213"(#loc1029))
#loc1727 = loc("reshape_1214"(#loc1030))
#loc1728 = loc("matmul_1216"(#loc1030))
#loc1729 = loc("reshape_1217"(#loc1030))
#loc1730 = loc("torch.nn.modules.linear.Linear::query"(#loc1044))
#loc1731 = loc("reshape_1227"(#loc1044))
#loc1732 = loc("transpose_1228"(#loc1044))
#loc1733 = loc("reshape_1229"(#loc1044))
#loc1734 = loc("torch.nn.modules.linear.Linear::key"(#loc1044))
#loc1735 = loc("reshape_1234"(#loc1044))
#loc1736 = loc("transpose_1235"(#loc1044))
#loc1737 = loc("reshape_1236"(#loc1044))
#loc1738 = loc("matmul_1238"(#loc1044))
#loc1739 = loc("reshape_1239"(#loc1044))
#loc1740 = loc("multiply_1240"(#loc1044))
#loc1741 = loc("add_1241"(#loc1044))
#loc1742 = loc("softmax_1242"(#loc1044))
#loc1743 = loc("reshape_1244"(#loc1044))
#loc1744 = loc("torch.nn.modules.linear.Linear::value"(#loc1044))
#loc1745 = loc("reshape_1249"(#loc1044))
#loc1746 = loc("transpose_1250"(#loc1044))
#loc1747 = loc("reshape_1251"(#loc1044))
#loc1748 = loc("matmul_1252"(#loc1044))
#loc1749 = loc("reshape_1253"(#loc1044))
#loc1750 = loc("transpose_1254"(#loc1044))
#loc1751 = loc("torch.nn.modules.linear.Linear::dense"(#loc1045))
#loc1752 = loc("reshape_1263"(#loc1058))
#loc1753 = loc("matmul_1265"(#loc1058))
#loc1754 = loc("reshape_1266"(#loc1058))
#loc1755 = loc("gelu_1268"(#loc1059))
#loc1756 = loc("reshape_1269"(#loc1060))
#loc1757 = loc("matmul_1271"(#loc1060))
#loc1758 = loc("reshape_1272"(#loc1060))
#loc1759 = loc("torch.nn.modules.linear.Linear::query"(#loc1074))
#loc1760 = loc("reshape_1282"(#loc1074))
#loc1761 = loc("transpose_1283"(#loc1074))
#loc1762 = loc("reshape_1284"(#loc1074))
#loc1763 = loc("torch.nn.modules.linear.Linear::key"(#loc1074))
#loc1764 = loc("reshape_1289"(#loc1074))
#loc1765 = loc("transpose_1290"(#loc1074))
#loc1766 = loc("reshape_1291"(#loc1074))
#loc1767 = loc("matmul_1293"(#loc1074))
#loc1768 = loc("reshape_1294"(#loc1074))
#loc1769 = loc("multiply_1295"(#loc1074))
#loc1770 = loc("add_1296"(#loc1074))
#loc1771 = loc("softmax_1297"(#loc1074))
#loc1772 = loc("reshape_1299"(#loc1074))
#loc1773 = loc("torch.nn.modules.linear.Linear::value"(#loc1074))
#loc1774 = loc("reshape_1304"(#loc1074))
#loc1775 = loc("transpose_1305"(#loc1074))
#loc1776 = loc("reshape_1306"(#loc1074))
#loc1777 = loc("matmul_1307"(#loc1074))
#loc1778 = loc("reshape_1308"(#loc1074))
#loc1779 = loc("transpose_1309"(#loc1074))
#loc1780 = loc("torch.nn.modules.linear.Linear::dense"(#loc1075))
#loc1781 = loc("reshape_1318"(#loc1088))
#loc1782 = loc("matmul_1320"(#loc1088))
#loc1783 = loc("reshape_1321"(#loc1088))
#loc1784 = loc("gelu_1323"(#loc1089))
#loc1785 = loc("reshape_1324"(#loc1090))
#loc1786 = loc("matmul_1326"(#loc1090))
#loc1787 = loc("reshape_1327"(#loc1090))
#loc1788 = loc("reshape_12"(#loc1092))
#loc1789 = loc("matmul_14"(#loc1092))
#loc1790 = loc("reshape_15"(#loc1092))
#loc1791 = loc("matmul_21"(#loc1096))
#loc1792 = loc("reshape_22"(#loc1096))
#loc1793 = loc("matmul_36"(#loc1106))
#loc1794 = loc("reshape_37"(#loc1106))
#loc1795 = loc("reshape_45"(#loc1113))
#loc1796 = loc("matmul_47"(#loc1113))
#loc1797 = loc("reshape_48"(#loc1113))
#loc1798 = loc("reshape_67"(#loc1121))
#loc1799 = loc("matmul_69"(#loc1121))
#loc1800 = loc("reshape_70"(#loc1121))
#loc1801 = loc("matmul_76"(#loc1125))
#loc1802 = loc("reshape_77"(#loc1125))
#loc1803 = loc("matmul_91"(#loc1135))
#loc1804 = loc("reshape_92"(#loc1135))
#loc1805 = loc("reshape_100"(#loc1142))
#loc1806 = loc("matmul_102"(#loc1142))
#loc1807 = loc("reshape_103"(#loc1142))
#loc1808 = loc("reshape_122"(#loc1150))
#loc1809 = loc("matmul_124"(#loc1150))
#loc1810 = loc("reshape_125"(#loc1150))
#loc1811 = loc("matmul_131"(#loc1154))
#loc1812 = loc("reshape_132"(#loc1154))
#loc1813 = loc("matmul_146"(#loc1164))
#loc1814 = loc("reshape_147"(#loc1164))
#loc1815 = loc("reshape_155"(#loc1171))
#loc1816 = loc("matmul_157"(#loc1171))
#loc1817 = loc("reshape_158"(#loc1171))
#loc1818 = loc("reshape_177"(#loc1179))
#loc1819 = loc("matmul_179"(#loc1179))
#loc1820 = loc("reshape_180"(#loc1179))
#loc1821 = loc("matmul_186"(#loc1183))
#loc1822 = loc("reshape_187"(#loc1183))
#loc1823 = loc("matmul_201"(#loc1193))
#loc1824 = loc("reshape_202"(#loc1193))
#loc1825 = loc("reshape_210"(#loc1200))
#loc1826 = loc("matmul_212"(#loc1200))
#loc1827 = loc("reshape_213"(#loc1200))
#loc1828 = loc("reshape_232"(#loc1208))
#loc1829 = loc("matmul_234"(#loc1208))
#loc1830 = loc("reshape_235"(#loc1208))
#loc1831 = loc("matmul_241"(#loc1212))
#loc1832 = loc("reshape_242"(#loc1212))
#loc1833 = loc("matmul_256"(#loc1222))
#loc1834 = loc("reshape_257"(#loc1222))
#loc1835 = loc("reshape_265"(#loc1229))
#loc1836 = loc("matmul_267"(#loc1229))
#loc1837 = loc("reshape_268"(#loc1229))
#loc1838 = loc("reshape_287"(#loc1237))
#loc1839 = loc("matmul_289"(#loc1237))
#loc1840 = loc("reshape_290"(#loc1237))
#loc1841 = loc("matmul_296"(#loc1241))
#loc1842 = loc("reshape_297"(#loc1241))
#loc1843 = loc("matmul_311"(#loc1251))
#loc1844 = loc("reshape_312"(#loc1251))
#loc1845 = loc("reshape_320"(#loc1258))
#loc1846 = loc("matmul_322"(#loc1258))
#loc1847 = loc("reshape_323"(#loc1258))
#loc1848 = loc("reshape_342"(#loc1266))
#loc1849 = loc("matmul_344"(#loc1266))
#loc1850 = loc("reshape_345"(#loc1266))
#loc1851 = loc("matmul_351"(#loc1270))
#loc1852 = loc("reshape_352"(#loc1270))
#loc1853 = loc("matmul_366"(#loc1280))
#loc1854 = loc("reshape_367"(#loc1280))
#loc1855 = loc("reshape_375"(#loc1287))
#loc1856 = loc("matmul_377"(#loc1287))
#loc1857 = loc("reshape_378"(#loc1287))
#loc1858 = loc("reshape_397"(#loc1295))
#loc1859 = loc("matmul_399"(#loc1295))
#loc1860 = loc("reshape_400"(#loc1295))
#loc1861 = loc("matmul_406"(#loc1299))
#loc1862 = loc("reshape_407"(#loc1299))
#loc1863 = loc("matmul_421"(#loc1309))
#loc1864 = loc("reshape_422"(#loc1309))
#loc1865 = loc("reshape_430"(#loc1316))
#loc1866 = loc("matmul_432"(#loc1316))
#loc1867 = loc("reshape_433"(#loc1316))
#loc1868 = loc("reshape_452"(#loc1324))
#loc1869 = loc("matmul_454"(#loc1324))
#loc1870 = loc("reshape_455"(#loc1324))
#loc1871 = loc("matmul_461"(#loc1328))
#loc1872 = loc("reshape_462"(#loc1328))
#loc1873 = loc("matmul_476"(#loc1338))
#loc1874 = loc("reshape_477"(#loc1338))
#loc1875 = loc("reshape_485"(#loc1345))
#loc1876 = loc("matmul_487"(#loc1345))
#loc1877 = loc("reshape_488"(#loc1345))
#loc1878 = loc("reshape_507"(#loc1353))
#loc1879 = loc("matmul_509"(#loc1353))
#loc1880 = loc("reshape_510"(#loc1353))
#loc1881 = loc("matmul_516"(#loc1357))
#loc1882 = loc("reshape_517"(#loc1357))
#loc1883 = loc("matmul_531"(#loc1367))
#loc1884 = loc("reshape_532"(#loc1367))
#loc1885 = loc("reshape_540"(#loc1374))
#loc1886 = loc("matmul_542"(#loc1374))
#loc1887 = loc("reshape_543"(#loc1374))
#loc1888 = loc("reshape_562"(#loc1382))
#loc1889 = loc("matmul_564"(#loc1382))
#loc1890 = loc("reshape_565"(#loc1382))
#loc1891 = loc("matmul_571"(#loc1386))
#loc1892 = loc("reshape_572"(#loc1386))
#loc1893 = loc("matmul_586"(#loc1396))
#loc1894 = loc("reshape_587"(#loc1396))
#loc1895 = loc("reshape_595"(#loc1403))
#loc1896 = loc("matmul_597"(#loc1403))
#loc1897 = loc("reshape_598"(#loc1403))
#loc1898 = loc("reshape_617"(#loc1411))
#loc1899 = loc("matmul_619"(#loc1411))
#loc1900 = loc("reshape_620"(#loc1411))
#loc1901 = loc("matmul_626"(#loc1415))
#loc1902 = loc("reshape_627"(#loc1415))
#loc1903 = loc("matmul_641"(#loc1425))
#loc1904 = loc("reshape_642"(#loc1425))
#loc1905 = loc("reshape_650"(#loc1432))
#loc1906 = loc("matmul_652"(#loc1432))
#loc1907 = loc("reshape_653"(#loc1432))
#loc1908 = loc("reshape_672"(#loc1440))
#loc1909 = loc("matmul_674"(#loc1440))
#loc1910 = loc("reshape_675"(#loc1440))
#loc1911 = loc("matmul_681"(#loc1444))
#loc1912 = loc("reshape_682"(#loc1444))
#loc1913 = loc("matmul_696"(#loc1454))
#loc1914 = loc("reshape_697"(#loc1454))
#loc1915 = loc("reshape_705"(#loc1461))
#loc1916 = loc("matmul_707"(#loc1461))
#loc1917 = loc("reshape_708"(#loc1461))
#loc1918 = loc("reshape_727"(#loc1469))
#loc1919 = loc("matmul_729"(#loc1469))
#loc1920 = loc("reshape_730"(#loc1469))
#loc1921 = loc("matmul_736"(#loc1473))
#loc1922 = loc("reshape_737"(#loc1473))
#loc1923 = loc("matmul_751"(#loc1483))
#loc1924 = loc("reshape_752"(#loc1483))
#loc1925 = loc("reshape_760"(#loc1490))
#loc1926 = loc("matmul_762"(#loc1490))
#loc1927 = loc("reshape_763"(#loc1490))
#loc1928 = loc("reshape_782"(#loc1498))
#loc1929 = loc("matmul_784"(#loc1498))
#loc1930 = loc("reshape_785"(#loc1498))
#loc1931 = loc("matmul_791"(#loc1502))
#loc1932 = loc("reshape_792"(#loc1502))
#loc1933 = loc("matmul_806"(#loc1512))
#loc1934 = loc("reshape_807"(#loc1512))
#loc1935 = loc("reshape_815"(#loc1519))
#loc1936 = loc("matmul_817"(#loc1519))
#loc1937 = loc("reshape_818"(#loc1519))
#loc1938 = loc("reshape_837"(#loc1527))
#loc1939 = loc("matmul_839"(#loc1527))
#loc1940 = loc("reshape_840"(#loc1527))
#loc1941 = loc("matmul_846"(#loc1531))
#loc1942 = loc("reshape_847"(#loc1531))
#loc1943 = loc("matmul_861"(#loc1541))
#loc1944 = loc("reshape_862"(#loc1541))
#loc1945 = loc("reshape_870"(#loc1548))
#loc1946 = loc("matmul_872"(#loc1548))
#loc1947 = loc("reshape_873"(#loc1548))
#loc1948 = loc("reshape_892"(#loc1556))
#loc1949 = loc("matmul_894"(#loc1556))
#loc1950 = loc("reshape_895"(#loc1556))
#loc1951 = loc("matmul_901"(#loc1560))
#loc1952 = loc("reshape_902"(#loc1560))
#loc1953 = loc("matmul_916"(#loc1570))
#loc1954 = loc("reshape_917"(#loc1570))
#loc1955 = loc("reshape_925"(#loc1577))
#loc1956 = loc("matmul_927"(#loc1577))
#loc1957 = loc("reshape_928"(#loc1577))
#loc1958 = loc("reshape_947"(#loc1585))
#loc1959 = loc("matmul_949"(#loc1585))
#loc1960 = loc("reshape_950"(#loc1585))
#loc1961 = loc("matmul_956"(#loc1589))
#loc1962 = loc("reshape_957"(#loc1589))
#loc1963 = loc("matmul_971"(#loc1599))
#loc1964 = loc("reshape_972"(#loc1599))
#loc1965 = loc("reshape_980"(#loc1606))
#loc1966 = loc("matmul_982"(#loc1606))
#loc1967 = loc("reshape_983"(#loc1606))
#loc1968 = loc("reshape_1002"(#loc1614))
#loc1969 = loc("matmul_1004"(#loc1614))
#loc1970 = loc("reshape_1005"(#loc1614))
#loc1971 = loc("matmul_1011"(#loc1618))
#loc1972 = loc("reshape_1012"(#loc1618))
#loc1973 = loc("matmul_1026"(#loc1628))
#loc1974 = loc("reshape_1027"(#loc1628))
#loc1975 = loc("reshape_1035"(#loc1635))
#loc1976 = loc("matmul_1037"(#loc1635))
#loc1977 = loc("reshape_1038"(#loc1635))
#loc1978 = loc("reshape_1057"(#loc1643))
#loc1979 = loc("matmul_1059"(#loc1643))
#loc1980 = loc("reshape_1060"(#loc1643))
#loc1981 = loc("matmul_1066"(#loc1647))
#loc1982 = loc("reshape_1067"(#loc1647))
#loc1983 = loc("matmul_1081"(#loc1657))
#loc1984 = loc("reshape_1082"(#loc1657))
#loc1985 = loc("reshape_1090"(#loc1664))
#loc1986 = loc("matmul_1092"(#loc1664))
#loc1987 = loc("reshape_1093"(#loc1664))
#loc1988 = loc("reshape_1112"(#loc1672))
#loc1989 = loc("matmul_1114"(#loc1672))
#loc1990 = loc("reshape_1115"(#loc1672))
#loc1991 = loc("matmul_1121"(#loc1676))
#loc1992 = loc("reshape_1122"(#loc1676))
#loc1993 = loc("matmul_1136"(#loc1686))
#loc1994 = loc("reshape_1137"(#loc1686))
#loc1995 = loc("reshape_1145"(#loc1693))
#loc1996 = loc("matmul_1147"(#loc1693))
#loc1997 = loc("reshape_1148"(#loc1693))
#loc1998 = loc("reshape_1167"(#loc1701))
#loc1999 = loc("matmul_1169"(#loc1701))
#loc2000 = loc("reshape_1170"(#loc1701))
#loc2001 = loc("matmul_1176"(#loc1705))
#loc2002 = loc("reshape_1177"(#loc1705))
#loc2003 = loc("matmul_1191"(#loc1715))
#loc2004 = loc("reshape_1192"(#loc1715))
#loc2005 = loc("reshape_1200"(#loc1722))
#loc2006 = loc("matmul_1202"(#loc1722))
#loc2007 = loc("reshape_1203"(#loc1722))
#loc2008 = loc("reshape_1222"(#loc1730))
#loc2009 = loc("matmul_1224"(#loc1730))
#loc2010 = loc("reshape_1225"(#loc1730))
#loc2011 = loc("matmul_1231"(#loc1734))
#loc2012 = loc("reshape_1232"(#loc1734))
#loc2013 = loc("matmul_1246"(#loc1744))
#loc2014 = loc("reshape_1247"(#loc1744))
#loc2015 = loc("reshape_1255"(#loc1751))
#loc2016 = loc("matmul_1257"(#loc1751))
#loc2017 = loc("reshape_1258"(#loc1751))
#loc2018 = loc("reshape_1277"(#loc1759))
#loc2019 = loc("matmul_1279"(#loc1759))
#loc2020 = loc("reshape_1280"(#loc1759))
#loc2021 = loc("matmul_1286"(#loc1763))
#loc2022 = loc("reshape_1287"(#loc1763))
#loc2023 = loc("matmul_1301"(#loc1773))
#loc2024 = loc("reshape_1302"(#loc1773))
#loc2025 = loc("reshape_1310"(#loc1780))
#loc2026 = loc("matmul_1312"(#loc1780))
#loc2027 = loc("reshape_1313"(#loc1780))
